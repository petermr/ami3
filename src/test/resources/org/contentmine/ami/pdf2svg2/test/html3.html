<?xml version="1.0" encoding="UTF-8"?>
<div xmlns="http://www.w3.org/1999/xhtml">
 <!--======page 0 L=======-->
 <div>
  <span class="columnSpan">Marius Muja, Abstract— Index Terms— 1 INTRODUCTION ument retrieval, data compres data analysis. vision methods [2], [4], [7] ?M. Muja is with BitLit Media Inc, Vancouver, BC, Canada. E-mail: mariusm@cs.ubc.ca. ? Canada. E-mail: lowe@cs.ubc.ca. T
   <sup>HE</sup>
  </span>
 </div>
 <!--======page 0 R=======-->
 <div>
  <span class="columnSpan">and David G. Lowe,Member, IEEEthe performance of the algorithms employed quicklybecomes a key issue.When working with high dimensional features, as withmost of those encountered in computer vision applications(image patches, local descriptors, global image descriptors),there is often no known nearest-neighbor search algorithmthat is exact and has acceptable performance. To obtain aspeed improvement, many practical applications are forcedto settle for an approximate search, in which not all theneighbors returned are exact, meaning some are approxi-mate but typically still close to the exact neighbors. In prac-tice it is common for approximate nearest neighbor searchalgorithms to provide more than 95 percent of the correctneighbors and still be two or more orders of magnitudefaster than linear search. In many cases the nearest neighborsearch is just a part of a larger application containing otherapproximations and there is very little loss in performancefromusing approximate rather than exact neighbors.In this paper we evaluate the most promising nearest-neighbor search algorithms in the literature, propose newalgorithms and improvements to existing ones, present amethod for performing automatic algorithm selection andparameter optimization, and discuss the problem of scal-ing to very large data sets using compute clusters. Wehave released all this work as an open source librarynamed fast library for approximate nearest neighbors(FLANN).1.1 Deﬁnitions and NotationIn this paper we are concerned with the problem of efﬁcient Ç</span>
 </div>
 <!--======page 1 L=======-->
 <div>
  <span class="columnSpan">closest toqwith respect to a metric distanced:M?MNNðq; PÞ ¼argmindðq; xÞ:x2P Thenearest neighbor problem to pre-process the setPsuch that the operationNNð can be performed efﬁciently. the number of neighbors return query point:K-nearest neighbor (KNN) search is to ﬁnd the closestK radius nearest neighbor search (RNN) ﬁnd all the points located closer than some distanceR the query point. We deﬁne theK-nearest neighbor the following manner:KNNðq; P; KÞ ¼A; where A is a set that satisﬁes the following conditions:jAj ¼K; A?P8x2A; y2P?A; dðq; xÞ ?dðq; yÞ: will always return exactlyK Kpoints inP). Theradius nearest neighborRNNðq; P; RÞ ¼ fp2P; dðq; pÞ&lt; Rg: Depending on how the valueR the whole data set. In practice, passing a large value of points is often very inefﬁcient. number of points that the radius search should return:RKNNðq; P; K; RÞ ¼A; such thatjAj ?K; A?P8x2A; y2P?A; dðq; xÞ&lt; Randdðq; xÞ ?dðq; yÞ: 2 BACKGROUND tion presents a reviewof previous work in this area.</span>
 </div>
 <!--======page 1 R=======-->
 <div>
  <span class="columnSpan">The kd-tree [9], [10] is one of the best known nearest neigh-bor algorithms. While very effective in low dimensionalityspaces, its performance quickly decreases for high dimen-sional data. ðq; PÞArya et al. [11] propose a variation of the k-d tree tobe used for approximate search by consideringð1þ"Þ-approximatenearest neighbors, points for which??distðp; qÞ ? ð1þ"Þdistðp; qÞwherepis the true nearestneighbor. The authors also propose the use of a priorityqueue to speed up the search. This method of approxi-mating the nearest neighbor search is also referred to as“error bound” approximate search.Another way of approximating the nearest neighbor fromsearch is by limiting the time spent during the search, or“time bound” approximate search. This method is proposedin [12] where the k-d tree search is stopped early after exam-ining a ﬁxed number of leaf nodes. In practice the time-con-strained approximation criterion has been found to givebetter results than the error-constrained approximate search.Multiple randomized k-d trees are proposed in [13] as ameans to speed up approximate nearest-neighbor search.In [14] we perform a wide range of comparisons showingthat the multiple randomized trees are one of the mosteffective methods for matching high dimensional data.Variations of the k-d tree using non-axis-aligned parti-tioning hyperplanes have been proposed: the PCA-tree [15],the RP-tree [16], and the trinary projection tree [17]. Wehave not found such algorithms to be more efﬁcient than arandomized k-d tree decomposition, as the overhead ofevaluating multiple dimensions during search outweighedthe beneﬁt of the better space decomposition.Another class of partitioning trees decompose the spaceusing various clustering algorithms instead of using hyper-planes as in the case of the k-d tree and its variants. Example Rtoof such decompositions include the hierarchical k-meanstree [18], the GNAT [19], the anchors hierarchy [20], the vp-tree [21], the cover tree [22] and the spill-tree [23]. Nister andStewenius [24] propose the vocabulary tree, which issearched by accessing a single leaf of a hierarchical k-meanstree. Leibe et al. [25] propose a ball-tree data structure con-structed using a mixed partitional-agglomerative clusteringalgorithm. Schindler et al. [26] propose a newway of search-ing the hierarchical k-means tree. Philbin et al. [2] conductedexperiments showing that an approximate ﬂat vocabularyoutperforms a vocabulary tree in a recognition task. In thispaper we describe a modiﬁed k-means tree algorithm that Þ:we have found to give the best results for some data sets,while randomized k-d trees are best for others.?Jegou et al. [27] propose the product quantizationapproach in which they decompose the space into lowdimensional subspaces and represent the data sets pointsby compact codes computed as quantization indices in thesesubspaces. The compact codes are efﬁciently compared tothe query points using an asymmetric approximate dis-tance. Babenko and Lempitsky [28] propose the inverted !R:
   <sub>2.1.1 Partitioning Trees</sub>
  </span>
 </div>
 <!--======page 2 L=======-->
 <div>
  <span class="columnSpan">and possible incorporation into FLANN. the data without requiring hand tuning of parameters. nel hashing [38] and complementary hashing [39]. points are vertices and edges co graphas “seeds” andstart the gra in a best-ﬁrst fashion. Similarly, egy and pick the starting points a tionandpossible incorporationintoFLANN. approximate nearest neighbor graph. 2.1.3 Nearest Neighbor Graph Techniques 2.2 Automatic Conﬁguration of NN Algorithms</span>
 </div>
 <!--======page 2 R=======-->
 <div>
  <span class="columnSpan">In a previous paper [14] we have proposed an automatic nearest neighbor algorithm conﬁguration method by combining grid search with a ﬁner grained NelderMead downhill simplex optimization process [43]. There has been extensive research onalgorithm conﬁgurationmethods [44], [45], however we are not aware of papers that apply such techniques to ﬁnding optimum parameters for nearest neighbor algorithms. Bergstra and Bengio [46] show that, except for small parameter spaces, random search can be a more efﬁcient strategy for parameter optimization than grid search. 3 FASTAPPROXIMATENN MATCHING Exact search is too costly for many applications, so this has generated interest in approximate nearest-neighbor search algorithms which return non-optimal neighbors in some cases, but can be orders of magnitude faster than exact search. After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of dimensionality [14], [47], we have found that one of two algorithms gave the best performance: thepriority search k-means treeor themultiple randomized k-d trees. These algorithms are described in the remainder of this section. 3.1 The Randomized k-d Tree Algorithm The randomized k-d tree algorithm [13], is an approximate nearest neighbor search algorithm that builds multiple randomized k-d trees which are searched in parallel. The trees are built in a similar manner to the classic k-d tree [9], [10], with the difference that where the classic kd-tree algorithm splits data on the dimension with the highest variance, for the randomized k-d trees the split dimension is chosen randomly from the topNdimensions with the highestD variance. We used the ﬁxed valueN¼5in our implemen-D tation, as this performs well across all our data sets and does not beneﬁt signiﬁcantly fromfurther tuning. When searching the randomized k-d forest, a single priority queue is maintained across all the randomized trees. The priority queue is ordered by increasing distance to the decision boundary of each branch in the queue, so the search will explore ﬁrst the closest leaves from all the trees. Once a data point has been examined (compared to the query point) inside a tree, it is marked in order to not be reexamined in another tree. The degree of approximation is determined by the maximumnumber of leaves to be visited (across all trees), returning the best nearest neighbor candidates found up to that point. Fig. 1 shows the value of searching in many randomized kd-trees at the same time. It can be seen that the performance improves with the number of randomized trees up to a certain point (about 20 random trees in this case) and that increasing the number of random trees further leads to static or decreasing performance. The memory overhead of using multiple random trees increases linearly with the number of trees, so at some point the speedup may not justify the additional memory used.</span>
 </div>
 <!--======page 3 L=======-->
 <div>
  <span class="columnSpan">SIFT features data set). and its nearest neighbor will be in the same cell.</span>
 </div>
 <!--======page 3 R=======-->
 <div>
  <span class="columnSpan">We have found the randomized k-d forest to be very effective in many situations, however on other data sets a different algorithm, thepriority search k-means tree, has been more effective at ﬁnding approximate nearest neighbors, especially when a high precision is required. The priority search k-means tree tries to better exploit the natural structure existing in the data, by clustering the data points using the full distance across all dimensions, in contrast to the (randomized) k-d tree algorithm which only partitions the data based on one dimension at a time. Nearest-neighbor algorithms that use hierarchical partitioning schemes based on clustering the data points have been previously proposed in the literature [18], [19], [24]. These algorithms differ in the way they construct the partitioning tree (whether using k-means, agglomerative or some other form of clustering) and especially in the strategies used for exploring the hierarchical tree. We have developed an improved version that explores the k-means tree using abest-bin-ﬁrststrategy, by analogy to what has been found to signiﬁcantly improve the performance of the approximate kd-tree searches. The priority search k-means tree is constructed by partitioning the data points at each level intoKdistinct regions using k-means clustering, and then applying the same method recursively to the points in each region. The recursion is stopped when the number of points in a region is smaller thanK(see Algorithm1). 3.2 The Priority Search K-Means Tree Algorithm 3.2.1 AlgorithmDescription</span>
 </div>
 <!--======page 4 L=======-->
 <div>
  <span class="columnSpan">tree level, so that the darkest values (ratio? branch in the queue.</span>
 </div>
 <!--======page 4 R=======-->
 <div>
  <span class="columnSpan">ng good search performance. In Section 3.4 we propose an algorithm for ﬁnding the optimum algorithm parameters, including the optimum branching factor. Fig. 3 contains a visualisation of several hierarchical k-means decompositions with different branching factors. Another parameter of the priority search k-means tree isI
   <sub>max</sub>, themaximum number of iterationsto perform in the k-means clustering loop. Performing fewer iterations can substantially reduce the tree build time and results in a slightly less than optimal clustering (if we consider the sum of squared errors from the points to the cluster centres as the measure of optimality). However, we have observed that even when using a small number of iterations, the nearest neighbor search performance is similar to that of the tree constructed by running the clustering until convergence, as illustrated by Fig. 4. It can be seen that using as fewas seven iterations we get more than 90 percent of the nearest-neighbor performance of the tree constructed using full convergence, but requiring less than 10 percent of the build time. The algorithm to use when picking the initial centres in the k-means clustering can be controlled by theC
   <sub>alg</sub>parameter. In our experiments (and in the FLANN library) we have
  </span>
 </div>
 <!--======page 5 L=======-->
 <div>
  <span class="columnSpan">choice for the priority search k-means tree. time and the memory requirements for storing the tree. Construction time complexity performed for each inner node. Considering a nodev iterationsI the clustering operation isOðndKIÞ, wheredvP on a level, we haven¼nv ing a level in the tree isOðndKIÞ the height of the tree will beðlogn=logKÞ tree construction cost ofOðndKIðlogn=logKÞÞ. Search time complexity iningL means tree with branching factorK tree traversals required isL=K versal, the algorithm needs to checkOðlogn=logKÞ nodes andone leaf node. OðKdÞ priority queue, which can be accomplished inOðKÞ needs to be computed which takesOðKdÞ the overall search cost isOðLdðlogn=logKÞÞ. dimensions can be independently averaged).1 for counting the number of bits set in a word). 3.2.2 Analysis 3.3 The Hierarchical Clustering Tree</span>
 </div>
 <!--======page 5 R=======-->
 <div>
  <span class="columnSpan">found to be very effective at matching binary features. For amore detailed description of this algorithm the reader isencouraged to consult [47] and [52].The hierarchical clustering tree performs a decomposi-tion of the search space by recursively clustering the inputdata set using random data points as the cluster centers ofthe non-leaf nodes (see Algorithm3).nvIn contrast to the priority search k-means tree presentedabove, for which using more than one tree didnot bring signiﬁ-cant improvements, we have found that building multipleKhierarchical clustering trees and searching them in parallelusing a common priority queue(the same approach that has innerbeen foundto work well for randomizedkd-trees [13]) resultedin signiﬁcant improvements in the search performance.Our experiments have revealed that the optimal algorithmfor approximate nearest neighbor search is highly depenamor-dent on several factors such as the data dimensionality, sizeand structure of the data set (whether there is any correla-tion between the features in the data set) and the desiredsearch precision. Additionally, each algorithm has a set ofparameters that have signiﬁcant inﬂuence on the search per-formance (e.g., number of randomized trees, branching fac-tor, number of k-means iterations).As we already mention in Section 2.2, the optimumparameters for a nearest neighbor algorithm are typicallychosen manually, using various heuristics. In this sectionwe propose a method for automatic selection of the bestnearest neighbor algorithm to use for a particular data setand for choosing its optimumparameters.By considering the nearest neighbor algorithm itself as aparameter of a generic nearest neighbor search routineA, theproblem is reduced to determining the parametersu2Qthat give the best solution, whereQis also known as theparameter conﬁguration space. This can be formulated as anoptimization problemin the parameter conﬁguration space:mincðuÞ3.4 Automatic Selection of the Optimal Algorithm</span>
 </div>
 <!--======page 6 L=======-->
 <div>
  <span class="columnSpan">deﬁne the cost function as follows:sðuÞ þwbðuÞbcðuÞ ¼þwmðuÞ;mminðsðuÞ þwbðuÞÞbu2Q wheresðuÞ,bðuÞandmðuÞ and queried with parametersu the memory used by the data:mðuÞ ¼mðuÞ=m.td The weightswandwbm the overall cost. The build-time weight (wb importance of the tree build minðsðuÞ þwbðuÞÞu2Qb and applied to all future data sets of the same type. 4 EXPERIMENTS</span>
 </div>
 <!--======page 6 R=======-->
 <div>
  <span class="columnSpan">(1)Fig. 5. Search efﬁciency for data of varying dimensionality. We experi-mented on both random vectors and image patches, with data sets ofsize 100K. The random vectors (top ﬁgure) represent the hardest casein which dimensions have no correlations, while most real-world prob-lems behave more like the image patches (bottomﬁgure).the approximate algorithm which are exact nearest neigh-bors. We measure the search performance of an algorithmas the time required to perform a linear search divided bythe time required to perform the approximate search andwe refer to it as thesearch speedupor justspeedup.We present several experiments we have conducted inorder to analyse the performance of the two algorithmsdescribed in Section 3.Data dimensionality is one of the factors that has a greatimpact on the nearest neighbor matching performance. Thetop of Fig. 5 shows how the search performance degrades asthe dimensionality increases in the case of random vectors.5The data sets in this case each contain10vectors whose val-4.1 Fast Approximate Nearest Neighbor Search4.1.1 Data Dimensionality</span>
 </div>
 <!--======page 7 L=======-->
 <div>
  <span class="columnSpan">the number of dimensions is greater than 800).2 Brown image patches even for64?64 for overall patch similarity. set of patches for different patch sizes.3 cover images [24]. ture data set from the same source. (using the priority search k-means tree).4 rithms [29] 4.1.2 Search Precision</span>
 </div>
 <!--======page 7 R=======-->
 <div>
  <span class="columnSpan">Since the LSH implementation (the E
   <sup>2</sup>LSH package) solves theR-near neighbor problem (ﬁnds the neighbors within a radiusRof the query point, not the nearest neighbors), to ﬁnd the nearest neighbors we have used the approach suggested in the E
   <sup>2</sup>LSH’s user manual: we compute theR-near neighbors for increasing values ofR. The parameters for the LSH algorithm were chosen using the parameter estimation tool included in the E
   <sup>2</sup>LSH package. For each case we have computed the precision achieved as the percentage of the query points for which the nearest neighbors were correctly found. Fig. 8 shows that the priority search k-means algorithm outperforms both the ANN and LSH algorithms by about an order of magnitude. The results for ANN are consistent with the experiment in Fig. 1, as ANN uses only a single kd-tree and does not beneﬁt from the speedup due to using multiple randomized trees. Fig. 9 compares the performance of nearest neighbor matching when the data set contains true matches for each feature in the test set to the case when it contains false matches. A true match is a match in which the query and the nearest neighbor point represent the same entity, for example, in case of SIFT features, they represent image patches of the same object. In this experiment we used two 100K SIFT features data sets, one that has ground truth determined from global image matching and one that is randomly sampled from a 5 million SIFT features data set and it contains only false matches for each feature in the test set. Our experiments showed that the randomized kd-trees have a signiﬁcantly better performance for true matches, when the query features are
  </span>
 </div>
 <!--======page 8 L=======-->
 <div>
  <span class="columnSpan">algorithms. Similar results were reported in [54]. algorithmselection on each data set. eral combinations of the tradeoff factorswandwbm build time weight,wb 4.1.3 Automatic Selection of Optimal Algorithm</span>
 </div>
 <!--======page 8 R=======-->
 <div>
  <span class="columnSpan">Fig. 10. Search speedup for the Trevi Fountain patches data set. values: 0 representing the case where we don’t care about the tree build time, 1 for the case where the tree build time and search time have the same importance and 0.01 representing the case where we care mainly about the search time but we also want to avoid a large build time. Similarly, the memory weight was chosen to be 0 for the case where the memory usage is not a concern,1representing the case where the memory use is the dominant concern and 1 as a middle ground between the two cases. This section evaluates the performance of the hierarchical clustering tree described in Section 3.3. We use the Winder/Brown patches data set [53] to compare the nearest neighbor search performance of the hierarchical clustering tree to that of other well known nearest neighbor search algorithms. For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and binary features such as BRIEF and ORB. The image patches have been downscaled to16?16pixels and are matched using normalized cross correlation. Fig. 11 shows the nearest neighbor search times for the different feature types. Each point on the graph is computed using the best performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT, SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB). In each case the optimum choice of parameters that maximizes the speedup for a given precision is used. In Fig. 12 we compare the hierarchical clustering tree with a multi-probe locality sensitive hashing implementation [30]. For the comparison we used data sets of BRIEF and ORB features extracted fromthe recognition benchmark images data set of [24], containing close to 5 million features. It can be seen that the hierarchical clustering index outperforms the LSH implementation for this data set. The LSH implementation also requires signiﬁcantly more memory compared to the hierarchical clustering trees for when high precision is required, as it needs to allocate a large 4.2 Binary Features</span>
 </div>
 <!--======page 9 L=======-->
 <div>
  <span class="columnSpan">for the test set. 5 SCALINGNEARESTNEIGHBORSEARCH Many papers have shown that u used in [4], [8], [55]. bor search algorithm.</span>
 </div>
 <!--======page 9 R=======-->
 <div>
  <span class="columnSpan">TABLE 1the data in the memory of a single machine for very largedata sets. Storing the data on the disk involves signiﬁcantperformance penalties due to the performance gap betweenmemory and disk access times. In FLANN we used theapproach of performing distributed nearest neighbor searchacross multiple machines.In order to scale to very large data sets, we use the approachof distributing the data to multiple machines in a computecluster and perform the nearest neighbor search using allthe machines in parallel. The data is distributed equallybetween the machines, such that for a cluster ofNmachineseach of them will only have to index and search1=Nof thewhole data set (although the ratios can be changed to havemore data on some machines than others). The ﬁnal resultof the nearest neighbor search is obtained by merging thepartial results from all the machines in the cluster once theyhave completed the search.In order to distribute the nearest neighbor matching on acompute cluster we implemented a Map-Reduce like algo-rithmusing the message passing interface (MPI) speciﬁcation.Algorithm 4 describes the procedure for building a dis-tributed nearest neighbor matching index. Each process inthe cluster executes in parallel and reads from a distributedﬁlesystem a fraction of the data set. All processes build thenearest neighbor search index in parallel using their respec-tive data set fractions.5.1 Searching on a Compute Cluster</span>
 </div>
 <!--======page 10 L=======-->
 <div>
  <span class="columnSpan">about 5 million features. role of master server. the ﬁnal result is returned to the client. bor search operations on each server.</span>
 </div>
 <!--======page 10 R=======-->
 <div>
  <span class="columnSpan">Fig. 13. Scaling nearest neighbor search on a compute cluster using message passing interface standard. implementation where they place a root k-d tree on top of all the other trees (leaf trees) with the role of selecting a subset of trees to be searched and only send the query to those trees. They show the distributed k-d tree has higher throughput compared to using independent trees, due to the fact that only a portion of the trees need to be searched by each query. The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree, hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithmin FLANN. In the distributed k-d tree implementation of [58] the search does not backtrack in the root node, so it is possible that subsets of the data containing near points are not searched at all if the root k-d tree doesn’t select the corresponding leaf k-d trees at the beginning. In this section we present several experiments that demonstrate the effectiveness of the distributed nearest neighbor matching framework in FLANN. For these experiments we have used the 80 million patch data set of [7]. In an MPI distributed system it’s possible to run multiple parallel processes on the same machine, the recommended approach is to run as many processes as CPU cores on the machine. Fig. 14 presents the results of an experiment in which we run multiple MPI processes on a single machine with eight CPUcores. It can be seen that the overall performance improves when increasing the number of processes from 1 to 4, however there is a decrease in performance when moving from four to eight parallel processes. This can be explained by the fact that increasing the parallelismon the same machine also increases the number of requests to the main memory (since all processes share the same main memory), and at some point the bottleneck moves from the CPU to the memory. Increasing the parallelism past this point results in decreased performance. 5.2 Evaluation of Distributed Search</span>
 </div>
 <!--======page 11 L=======-->
 <!--======page 11 R=======-->
 <div>
  <span class="columnSpan">Fig. 16. Matching 80 million tiny images directly using a compute cluster. radomized k-d tree forest as itwas determined by the autotuning procedure to be the most efﬁcient in this case. It can be seen that the search performance scales well with the data set size and it beneﬁts from using multiple parallel processes. All the previous experiments have shown that distributing the nearest neighbor search to multiple machines results in an overall increase in performance in addition to the advantage of being able to use more memory. Ideally, when distributing the search toNmachines the speedup would beNtimes higher, however in practice for approximate nearest neighbor search the speedup is smaller due to the fact that the search on each of the machines has sub-linear complexity in the size of the input data set. 6 THEFLANN LIBRARY The work presented in this paper has been made publicly available as an open source library named Fast Library for5 Approximate Nearest Neighbors[59]. FLANN is used in a large number of both research and industry projects (e.g., [60], [61], [62], [63], [64]) and is widely used in the computer vision community, in part due to its inclusion in OpenCV[65], the popular open source computer vision library. FLANNalso is used by other well known open source projects, such as the point cloud library (PCL) and the robot operating system (ROS) [63]. FLANN has been packaged by most of the mainstream Linux distributions such as Debian, Ubuntu, Fedora, Arch, Gentoo and their derivatives. 7 CONCLUSIONS This paper addresses the problem of fast nearest neighbor search in high dimensional spaces, a core problem in many computer vision and machine learning algorithms and which is often the most computationally expensive part of these algorithms. We present and compare the algorithms we have found to work best at fast approximate search in high dimensional spaces: the randomized k-d trees and a</span>
 </div>
 <!--======page 12 L=======-->
 <div>
  <span class="columnSpan">pute clusters. REFERENCESpoints,”Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, 2004.Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2007, pp. 1–8.approach to object matching in videos,” inComput. Vis., 2003, pp. 1470–1477.tographs,”ACMTrans. Graph., vol. 26, p. 4, 2007.with parameter-sensitive hashing,” inComput. Vis., 2003, pp. 750–757.recognition using low distortion correspondences,” inCS Conf. Comput. Vis. Pattern Recog., 2005, vol. 1, pp. 26–33.nition,”IEEE Trans. Pattern Anal. Mach. Intell.pp. 1958–1970, Nov. 2008.“ImageNet: A large-scale hierarchical image database,” inIEEE Conf. Comput. Vis. Pattern Recog., 2009, pp. 248–255.ciative searching,”Commun. ACM1975.ﬁnding best matches in logarithmic expected time,”Math. Softw., vol. 3, no. 3, pp. 209–226, 1977.searching in ﬁxed dimensions,”J. ACM923, 1998.nearest-neighbour search in high-dimensional spaces,” inIEEE Conf. Comput. Vis. Pattern Recog., 1997, pp. 1000–1006.descriptor matching,” inRecog., 2008, pp. 1–8.with automatic algorithm conﬁguration,” inputer Vis. Theory Appl., 2009, pp. 331–340.dimensional trees,”Algorithmicadimensional manifolds,” inComput., 2008, pp. 537–546.trees for scalable visual descriptor indexing,” inComput. Vis. Pattern Recog., 2010, pp. 3392–3399.rithm for computing k-nearest neighbors,”vol. C-24, no. 7, pp. 750–753, Jul. 1975. [19] S. Brin, “Near neighbor search in large metric spaces,” in21th Int. Conf. Very Large Data Bases, 1995, pp. 574–584.ity to survive high dimensional data,” intainity Artif. Intell., 2000, pp. 397–405.bor search in general metric spaces,” inDiscrete Algorithms, 1993, pp. 311–321.est neighbor,” inProc. 23rd Int. Conf. Mach. Learning104.</span>
 </div>
 <!--======page 12 R=======-->
 <!--======page 13 L=======-->
 <!--======page 13 R=======-->
</div>
