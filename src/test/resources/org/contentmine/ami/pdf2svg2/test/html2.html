<?xml version="1.0" encoding="UTF-8"?>
<div xmlns="http://www.w3.org/1999/xhtml">
 <!--======page 0 L=======-->
 <div>
  <span class="columnSpan">{ on the ImageNettest on CIFAR-10 with 100 and 1000 layers.1 &amp; COCO 2015 competitions ization, COCO detection, and COCO segmentation.Abstract 1. Introduction</span>
 </div>
 <!--======page 0 R=======-->
 <div>
  <span class="columnSpan">2020)%)(56-layer%r(orrr20-layeroe1010rrgen56-layertisneitart20-layer0001234560123456iter. (1e4)iter. (1e4)Figure 1. Training error (left) and test error (right) on CIFAR-10with 20-layer and 56-layer “plain” networks. The deeper networkhas higher training error, and thus test error. Similar phenomenaon ImageNet is presented in Fig. 4.}@microsoft.com ×greatly beneﬁted from very deep models.Driven by the signiﬁcance of depth, a question arises:Islearning better networks as easy as stacking more layers?An obstacle to answering this question was the notoriousproblem of vanishing/exploding gradients [14, 1, 8], whichhamper convergence from the beginning. This problem,however, has been largely addressed by normalized initial-ization [23, 8, 36, 12] and intermediate normalization layers[16], which enable networks with tens of layers to start con-verging for stochastic gradient descent (SGD) with back-propagation [22].When deeper networks are able to start converging, adegradationproblem has been exposed: with the networkdepth increasing, accuracy gets saturated (which might beunsurprising) and then degrades rapidly. Unexpectedly,such degradation isnot caused by overﬁtting, and addingmore layers to a suitably deep model leads tohigher train-ing error, as reported in [10, 41] and thoroughly veriﬁed byour experiments. Fig. 1 shows a typical example.The degradation (of training accuracy) indicates that notall systems are similarly easy to optimize. Let us consider ashallower architecture and its deeper counterpart that addsmore layers onto it. There exists a solutionby constructionto the deeper model: the added layers areidentitymapping,and the other layers are copied from the learned shallowermodel. The existence of this constructed solution indicatesthat a deeper model should produce no higher training error</span>
 </div>
 <!--======page 1 L=======-->
 <div>
  <span class="columnSpan">xweight layerreluF(x)xweight layeridentityF(x)?+?xreluFigure 2. Residual learning: a building block. (or unable to do so in feasible time). introducing adeep residual learning underlying mapping asH(x) layers ﬁt another mapping ofF(x) :=H(x)−x inal mapping is recast intoF(x)+x of nonlinear layers. The formulation ofF(x) +x performidentity ily implemented using common libraries (e.g without modifying the solvers. sults substantially better than previous networks.</span>
 </div>
 <!--======page 1 R=======-->
 <div>
  <span class="columnSpan">ImageNettestset, andwon the 1st place in the ILSVRC 2015 classiﬁcation competition. The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to furtherwin the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentationin ILSVRC &amp; COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems. Residual Representations.In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image retrieval and classiﬁcation [4, 47]. For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a ﬁner scale. An alternative to Multigrid is hierarchical basis preconditioning [44, 45], which relies on variables that represent residual vectors between two scales. It has been shown [3, 44, 45] that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization. Shortcut Connections.Practices and theories that lead to shortcut connections [2, 33, 48] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [33, 48]. In [43, 24], a few intermediate layers are directly connected to auxiliary classiﬁers for addressing vanishing/exploding gradients. The papers of [38, 37, 31, 46] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In [43], an “inception” layer is composed of a shortcut branch and a few deeper branches. Concurrent with our work, “highway networks” [41, 42] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is “closed” (approaching zero), the layers in highway networks representnon-residualfunctions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, 2. Related Work</span>
 </div>
 <!--======page 2 L=======-->
 <div>
  <span class="columnSpan">extremely increased depth (e.g., over 100 layers). 3.1. Residual Learning Let us considerH(x) withx2 cally approximate complicated functions mate the residual functions,i.e.,H(x)−x rather than expect stacked layers to approximateH( F(x) :=H(x)−x F(x)+x the ease of learning might be different. ers toward zero to approach identity mappings. pings provide reasonable preconditioning. 3.2. Identity Mapping by Shortcuts we consider a building block deﬁned as:y=F(x,{W}) +x.i Herexandy ers considered. The functionF(x,{W})i that has two layers,F=Wσ(Wx)in whichσ21 3. Deep Residual Learning</span>
 </div>
 <!--======page 2 R=======-->
 <div>
  <span class="columnSpan">ReLU [29] and the biases are omitted for simplifying no-tations. The operationF+xis performed by a shortcutconnection and element-wise addition. We adopt the sec-ond nonlinearity after the addition (i.e.,σ(y), see Fig. 2).The shortcut connections in Eqn.(1) introduce neither ex-tra parameter nor computation complexity. This is not onlyattractive in practice but also important in our comparisonsbetween plain and residual networks. We can fairly com-pare plain/residual networks that simultaneously have thesame number of parameters, depth, width, and computa-tional cost (except for the negligible element-wise addition).The dimensions ofxandFmust be equal in Eqn.(1).If this is not the case (e.g., when changing the input/outputchannels), we can perform a linear projectionWby thesshortcut connections to match the dimensions: (x), wey=F(x,{W}) +Wx.(2)isWe can also use a square matrixWin Eqn.(1). But we willsshow by experiments that the identity mapping is sufﬁcientfor addressing the degradation problem and is economical,and thusWis only used when matching dimensions.sThe form of the residual functionFis ﬂexible. Exper-iments in this paper involve a functionFthat has two orthree layers (Fig. 5), while more layers are possible. But ifFhas only a single layer, Eqn.(1) is similar to a linear layer:y=Wx+x, for which we have not observed advantages.1We also note that although the above notations are aboutfully-connected layers for simplicity, they are applicable toconvolutional layers. The functionF(x,{W})can repre-isent multiple convolutional layers. The element-wise addi-tion is performed on two feature maps, channel by channel.We have tested various plain/residual nets, and have ob-served consistent phenomena. To provide instances for dis-cussion, we describe two models for ImageNet as follows.Plain Network.Our plain baselines (Fig. 3, middle) aremainly inspired by the philosophy of VGGnets [40] (Fig. 3,left). The convolutional layers mostly have 3×3 ﬁlters andfollow two simple design rules: (i) for the same outputfeature map size, the layers have the same number of ﬁl-ters; and (ii) if the feature map size is halved, the num-ber of ﬁlters is doubled so as to preserve the time com-plexity per layer. We perform downsampling directly byconvolutional layers that have a stride of 2. The network(1)ends with a global average pooling layer and a 1000-wayfully-connected layer with softmax. The total number ofweighted layers is 34 in Fig. 3 (middle).It is worth noticing that our model hasfewerﬁlters andlowercomplexity than VGG nets [40] (Fig. 3, left). Our 34denotes3.3. Network Architectures</span>
 </div>
 <!--======page 3 L=======-->
 <div>
  <span class="columnSpan">VGG-19 34-layer plainimageimageimage output 3x3 conv, 64 size: 2243x3 conv, 64pool, /2 output size: 1123x3 conv, 1283x3 conv, 1287x7 conv, 64, /27x7 conv, 64, /2pool, /2pool, /2pool, /2 output size: 563x3 conv, 2563x3 conv, 643x3 conv, 643x3 conv, 2563x3 conv, 643x3 conv, 643x3 conv, 2563x3 conv, 643x3 conv, 643x3 conv, 2563x3 conv, 643x3 conv, 643x3 conv, 643x3 conv, 643x3 conv, 643x3 conv, 64pool, /23x3 conv, 128, /23x3 conv, 128, /2 output size: 283x3 conv, 5123x3 conv, 1283x3 conv, 1283x3 conv, 5123x3 conv, 1283x3 conv, 1283x3 conv, 5123x3 conv, 1283x3 conv, 1283x3 conv, 5123x3 conv, 1283x3 conv, 1283x3 conv, 1283x3 conv, 1283x3 conv, 1283x3 conv, 1283x3 conv, 1283x3 conv, 128 output pool, /23x3 conv, 256, /23x3 conv, 256, /2 size: 143x3 conv, 5123x3 conv, 2563x3 conv, 2563x3 conv, 5123x3 conv, 2563x3 conv, 2563x3 conv, 5123x3 conv, 2563x3 conv, 2563x3 conv, 5123x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 2563x3 conv, 256 output pool, /23x3 conv, 512, /23x3 conv, 512, /2 size: 73x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 5123x3 conv, 512 output fc 4096avg poolavg pool size: 1fc 4096fc 1000fc 1000fc 1000 Figure 3. Example network architectures for ImageNet. dle Right FLOPs). The dotted shortcuts increase dimensions. more details and other variants.</span>
 </div>
 <!--======page 3 R=======-->
 <div>
  <span class="columnSpan">Residual Network.Based on the above plain network, weinsert shortcut connections (Fig. 3, right) which turn thenetwork into its counterpart residual version. The identityshortcuts (Eqn.(1)) can be directly used when the input andoutput are of the same dimensions (solid line shortcuts inFig. 3). When the dimensions increase (dotted line shortcutsin Fig. 3), we consider two options: (A) The shortcut stillperforms identity mapping, with extra zero entries paddedfor increasing dimensions. This option introduces no extraparameter; (B) The projection shortcut in Eqn.(2) is used tomatch dimensions (done by 1×1 convolutions). For bothoptions, when the shortcuts go across feature maps of twosizes, they are performed with a stride of 2.3.4. ImplementationOur implementation for ImageNet follows the practicein [21, 40]. The image is resized with its shorter side ran-domly sampled in[256,480]for scale augmentation [40].A 224×224 crop is randomly sampled from an image or itshorizontal ﬂip, with the per-pixel mean subtracted [21]. Thestandard color augmentation in [21] is used. We adopt batchnormalization (BN) [16] right after each convolution andbefore activation, following [16]. We initialize the weightsas in [12] and train all plain/residual nets from scratch. Weuse SGD with a mini-batch size of 256. The learning ratestarts from0.1 and is divided by 10 when the error plateaus,4and the models are trained for up to60×10iterations. Weuse a weight decay of 0.0001 and a momentum of 0.9. Wedo not use dropout [13], following the practice in [16].In testing, for comparison studies we adopt the standard10-crop testing [21]. For best results, we adopt the fully-convolutional form as in [40, 12], and average the scoresat multiple scales (images are resized such that the shorterside is in{224,256,384,480,640}).4.1. ImageNet ClassiﬁcationWe evaluate our method on the ImageNet 2012 classiﬁ-cation dataset [35] that consists of 1000 classes. The modelsare trained on the 1.28 million training images, and evalu-ated on the 50k validation images. We also obtain a ﬁnalresult on the 100k test images, reported by the test server.We evaluate both top-1 and top-5 error rates.Plain Networks.We ﬁrst evaluate 18-layer and 34-layerplain nets. The 34-layer plain net is in Fig. 3 (middle). The18-layer plain net is of a similar form. See Table 1 for deLeft: the Mid-tailed architectures.The results in Table 2 showthat the deeper 34-layer plainnet has higher validation error than the shallower 18-layer showsplain net. To reveal the reasons, in Fig. 4 (left) we com-4. Experiments</span>
 </div>
 <!--======page 4 L=======-->
 <div>
  <span class="columnSpan">layer nameoutput size18-layerconv1112×112???conv2x56×563×3, 643×23×3, 643???3×3, 1283×conv3x28×28×23×3, 1283×???3×3, 2563×conv4x14×14×23×3, 2563×???3×3, 5123×conv5x7×7×23×3, 5123×1×19FLOPs1.8×10 sampling is performed by conv31, conv41, and conv56050)%(ro40rre30plain-18plain-3420010203040iter. (1e4) Figure 4. Training onImageNet their plain counterparts.plainResNet18 layers27.9427.8834 layers28.5425.03 counterparts. Fig. 4 shows the training procedures. 34-layer plain net has highertraining 34-layer one. We argue that this optimization difﬁculty is</span>
 </div>
 <!--======page 4 R=======-->
 <div>
  <span class="columnSpan">34-layer50-layer101-layer152-layer7×7, 64, stride 23×3 max pool, stride 2⎡⎤⎡⎤⎡⎤?1×1, 641×1, 641×1, 64 3×3, 64⎣⎣⎣×3×3×3×33×3, 643×3, 643×3, 64 3×3, 641×1, 2561×1, 2561×1, 256⎡⎤⎡⎤⎡⎤?1×1, 1281×1, 1281×1, 128 3×3, 128⎣⎣⎣×4×4×4×83×3, 1283×3, 1283×3, 128 3×3, 1281×1, 5121×1, 5121×1, 512⎡⎤⎡⎤⎡⎤?1×1, 2561×1, 2561×1, 256 3×3, 256⎣⎣⎣×6×6×23×363×3, 2563×3, 2563×3, 256 3×3, 2561×1, 10241×1, 10241×1, 1024⎡⎤⎡⎤⎡⎤?1×1, 5121×1, 5121×1, 512 3×3, 512⎣⎣⎣×33×3, 512×33×3, 512×33×3, 512×3 3×3, 5121×1, 20481×1, 20481×1, 2048average pool, 1000-d fc, softmax9999 3.6×103.8×107.6×1011.3×10 1 with a stride of 2.6050)%(ro40rre 34-layer18-layer30 18-layerResNet-1834-layerResNet-34205001020304050iter. (1e4)3We have experimented with more training iterations (3×) and still ob-reducing of the training error
   <sup>3</sup>. The reason for such opti-mization difﬁculties will be studied in the future.Residual Networks.Next we evaluate 18-layer and 34-layer residual nets (ResNets). The baseline architecturesare the same as the above plain nets, expect that a shortcutconnection is added to each pair of 3×3 ﬁlters as in Fig. 3(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),we use identity mapping for all shortcuts and zero-paddingfor increasing dimensions (option A). So they haveno extraparametercompared to the plain counterparts.We have three major observations from Table 2 andFig. 4. First, the situation is reversed with residual learn-ing – the 34-layer ResNet is better than the 18-layer ResNetto(by 2.8%). More importantly, the 34-layer ResNet exhibitsconsiderably lower training error and is generalizable to thevalidation data. This indicates that the degradation problemis well addressed in this setting and we manage to obtainaccuracy gains from increased depth.Second, compared to its plain counterpart, the 34-layer
  </span>
 </div>
 <!--======page 5 L=======-->
 <div>
  <span class="columnSpan">modeltop-1 err. top-5 err.VGG-16 [40]28.07 9.33GoogLeNet [43]- 9.15PReLU-net [12]24.27 7.38plain-3428.54 10.02ResNet-34 A25.03 7.76ResNet-34 B24.52 7.46ResNet-34 C24.19 7.40ResNet-5022.85 6.71ResNet-10121.75 6.05ResNet-15221.43 5.71 Table 3. Error rates (%,10-crop that only uses projections for increasing dimensions.methodVGG [40] (ILSVRC’14)GoogLeNet [43] (ILSVRC’14)VGG [40](v5)PReLU-net [12]BN-inception [16]ResNet-34 BResNet-34 CResNet-50ResNet-101ResNet-152 Table 4. Error rates (%) ofsingle-model† validation set (exceptreported on the test set).methodtop-5 err. (VGG [40] (ILSVRC’14)7.32GoogLeNet [43] (ILSVRC’14)6.66VGG [40](v5)6.8PReLU-net [12]4.94BN-inception [16]4.82ResNet (ILSVRC’15)3.57 Table 5. Error rates (%) ofensembles test set of ImageNet and reported by the test server. learning on extremely deep systems. converges faster (Fig. 4 rightvs gence at the early stage.</span>
 </div>
 <!--======page 5 R=======-->
 <div>
  <span class="columnSpan">64-d 256-d1x1, 643x3, 64relurelu3x3, 64relu3x3, 641x1, 256relureluFigure 5. A deeper residual functionFfor ImageNet. Left: abuilding block (on 56×56 feature maps) as in Fig. 3 for ResNet-34. Right: a “bottleneck” building block for ResNet-50/101/152.4Deepernon-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracyfrom increased depth (as shown on CIFAR-10), but are not as economicalas the bottleneck ResNets. So the usage of bottleneck designs is mainly dueparameter-free, identity shortcuts help with training. Nextwe investigate projection shortcuts (Eqn.(2)). In Table 3 wecompare three options: (A) zero-padding shortcuts are usedfor increasing dimensions, and all shortcuts are parameter-free (the same as Table 2 and Fig. 4 right); (B) projec†
   <sub>tion shortcuts are used for increasing dimensions, and other</sub>shortcuts are identity; and (C) all shortcuts are projections.Table 3 shows that all three options are considerably bet-ter than the plain counterpart. Bis slightly better than A. Weargue that this is because the zero-padded dimensions in Aindeed have no residual learning. Cis marginally better thanB, and we attribute this to the extra parameters introducedby many (thirteen) projection shortcuts. But the small dif-ferences among A/B/C indicate that projection shortcuts arenot essential for addressing the degradation problem. So wedo not use option C in the rest of this paper, to reduce mem-ory/time complexity and model sizes. Identity shortcuts areparticularly important for not increasing the complexity of )
   <sup>the bottleneck architectures that are introduced below.</sup>Deeper Bottleneck Architectures.Next we describe ourdeeper nets for ImageNet. Because of concerns on the train-ing time that we can afford, we modify the building blockas abottleneckdesign
   <sup>4</sup>. For each residual functionF, weuse a stack of 3 layers instead of 2 (Fig. 5). The three layersare 1×1, 3×3, and 1×1 convolutions, where the 1×1 layersare responsible for reducing and then increasing (restoring)dimensions, leaving the 3×3 layer a bottleneck with smallerinput/output dimensions. Fig. 5 shows an example, whereboth designs have similar time complexity.The parameter-free identity shortcuts are particularly im-portant for the bottleneck architectures. If the identity short-vs.cut in Fig. 5 (right) is replaced with projection, one canshow that the time complexity and model size are doubled,as the shortcut is connected to the two high-dimensionalends. So identity shortcuts lead to more efﬁcient modelsfor the bottleneck designs.50-layer ResNet:We replace each 2-layer block in the
  </span>
 </div>
 <!--======page 6 L=======-->
 <div>
  <span class="columnSpan">dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: haslower complexity lion FLOPs). metrics (Table 3 and 4). Comparisons with State-of-the-art Methods. This leads to3.57% This entry won the 1st place in ILSVRC 2015. simple architectures as follows. (middle/right). The network inputs are 32× the per-pixel mean subtracted. The ﬁrst layer is 3× lutions. Then we use a stack of6nlayers with 3× lutions on the feature maps of sizes{32,16,8} with 2n ﬁlters are{16,32,64} layer, and softmax. There are totally 6noutput map size32×3216×168×8# layers1+2n2n2n# ﬁlters163264 4.2. CIFAR-10 and Analysis</span>
 </div>
 <!--======page 6 R=======-->
 <div>
  <span class="columnSpan">methoderror (%)Maxout [9]9.38NIN [25]8.81DSN [24]8.22# layers# paramsFitNet [34]192.5M8.39Highway [41, 42]192.3M7.54(7.72±0.16)Highway [41, 42]321.25M8.80ResNet200.27M8.75ResNet320.46M7.51ResNet440.66M7.17ResNet560.85M6.97ResNet1101.7M6.43(6.61±0.16)ResNet120219.4M7.93Table 6. Classiﬁcation error on theCIFAR-10test set. All meth-ods are with data augmentation. For ResNet-110, we run it 5 timesand show “best (mean±std)” as in [42].so our residual models have exactly the same depth, width,and number of parameters as the plain counterparts.We use a weight decay of 0.0001 and momentum of 0.9,and adopt the weight initialization in [12] and BN [16] butwith no dropout. These models are trained with a mini-batch size of 128 on two GPUs. We start with a learningrate of 0.1, divide it by 10 at 32k and 48k iterations, andterminate training at 64k iterations, which is determined ona 45k/5k train/val split. We follow the simple data augmen-tation in [24] for training: 4 pixels are padded on each side,and a 32×32 crop is randomly sampled from the paddedimage or its horizontal ﬂip. For testing, we only evaluatethe single view of the original 32×32 image.We comparen={3,5,7,9}, leading to 20, 32, 44, and56-layer networks. Fig. 6 (left) shows the behaviors of theplain nets. The deep plain nets suffer from increased depth,and exhibit higher training error when going deeper. Thisphenomenon is similar to that on ImageNet (Fig. 4, left) and 3 convo-on MNIST (see [41]), suggesting that such an optimization 3 convo-difﬁculty is a fundamental problem.Fig. 6 (middle) shows the behaviors of ResNets. Alsosimilar to the ImageNet cases (Fig. 4, right), our ResNetsmanage to overcome the optimization difﬁculty and demon-strate accuracy gains when the depth increases.We further exploren= 18that leads to a 110-layerResNet. In this case, we ﬁnd that the initial learning rateof 0.1 is slightly too large to start converging
   <sup>5</sup>. So we use0.01 to warmup the training until the training error is below80%(about 400 iterations), and then go back to 0.1 and con-tinue training. The rest of the learning schedule is as donepreviously. This 110-layer network converges well (Fig. 6,middle). It hasfewerparameters than other deep and thin
  </span>
 </div>
 <!--======page 7 L=======-->
 <div>
  <span class="columnSpan">2056-layer)%(r10or20-layerre5plain-20plain-32plain-44plain-5600123456iter. (1e4) Figure 6. Training onCIFAR-10 of plain-110 is higher than 60% and not displayed.plain-203plain-56ResNet-20dResNet-56t2s1020406080100layer index (original)plain-203plain-56ResNet-20dResNet-56t2s1020406080100layer index (sorted by magnitude) 10. The responses are the outputs of each 3× before nonlinearity.Top order.Bottom Analysis of Layer Responses. the outputs of each 3× signal less. Exploring Over 1000 layers. deep model of over 1000 layers. We setn above. Our method shows this10
   <sup>3</sup>-layer network is able to achieve &lt; (7.93%, Table 6).
  </span>
 </div>
 <!--======page 7 R=======-->
 <div>
  <span class="columnSpan">2020ResNet-20residual-110ResNet-32residual-1202ResNet-44ResNet-56ResNet-110)%(r101020-layerorre110-layer551000123456456iter. (1e4)iter. (1e4)Left: plain networks. The error : ResNets.Right: ResNets with 110 and 1202 layers.training data07+1207++12test dataVOC 07 testVOC 12 testVGG-1673.270.4ResNet-10176.473.8 100Table 7. Object detection mAP (%) on the PASCAL VOC2007/2012 test sets usingbaselineFaster R-CNN. See also ap-pendix for better results.metricmAP@.5mAP@[.5, .95] 100VGG-1641.521.2ResNet-10148.427.2Table 8. Object detection mAP (%) on the COCO validation setusingbaselineFaster R-CNN. See also appendix for better results.have similar training error. We argue that this is because ofoverﬁtting. The 1202-layer network may be unnecessarilylarge (19.4M) for this small dataset. Strong regularizationsuch as maxout [9] or dropout [13] is applied to obtain thebest results ([9, 25, 24, 34]) on this dataset. In this paper, weuse no maxout/dropout and just simply impose regulariza-tion via deep and thin architectures by design, without dis-tracting from the focus on the difﬁculties of optimization.But combining with stronger regularization may improveresults, which we will study in the future.Our method has good generalization performance onother recognition tasks. Table 7 and 8 show the object de-tection baseline results on PASCAL VOC 2007 and 2012[5] and COCO[26]. We adoptFaster R-CNN[32] as the de-tection method. Here we are interested in the improvementsof replacing VGG-16 [40] with ResNet-101. The detectionimplementation (see appendix) of using both models is thethatsame, so the gains can only be attributed to better networks.Most remarkably, on the challenging COCO dataset we ob, andtain a 6.0%increase in COCO’s standard metric (mAP@[.5,.95]), which is a 28% relative improvement. This gain issolely due to the learned representations.Based on deep residual nets, we won the 1st places inseveral tracks in ILSVRC &amp;COCO2015 competitions: Im-4.3. Object Detection on PASCAL and MS COCO</span>
 </div>
 <!--======page 8 L=======-->
 <div>
  <span class="columnSpan">cies with gradient descent is difﬁcult.Networks, 5(2):157–166, 1994. [2] C. M. Bishop.Neural networks for pattern recognitionuniversity press, 1995. [3] W. L. Briggs, S. F. McCormick, et al.2000.InBMVC, 2011.pages 303–338, 2010. [6] R. Girshick. Fast R-CNN. InICCV, 2015.CVPR, 2014.deep feedforward neural networks. InAISTATS, 2010.Y. Bengio. Maxout networks.arXiv:1302.4389, 2013.cost. InCVPR, 2015.convolutional networks for visual recognition. InECCVICCV, 2015.adaptation of feature detectors.arXiv:1207.0580, 2012.Diploma thesis, TU Munich, 1991.computation, 9(8):1735–1780, 1997.network training by reducing internal covariate shift. Inneighbor search.TPAMI, 33, 2011.TPAMI, 2012.fast feature embedding.arXiv:1408.5093, 2014.ages.Tech Report, 2009.with deep convolutional neural networks. InNIPS, 2012.written zip code recognition.Neural computation, 1989.¨ [23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. MInNeural Networks: Tricks of the Tradesupervised nets.arXiv:1409.5185, 2014. [25] M. Lin, Q. Chen, and S. Yan. Network in network.2013.P. Doll´context. InECCV. 2014. References</span>
 </div>
 <!--======page 8 R=======-->
</div>
