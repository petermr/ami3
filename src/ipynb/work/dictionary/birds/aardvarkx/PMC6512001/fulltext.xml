<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Vis Comput</journal-id><journal-id journal-id-type="iso-abbrev">Vis Comput</journal-id><journal-title-group><journal-title>The Visual Computer</journal-title></journal-title-group><issn pub-type="ppub">0178-2789</issn><issn pub-type="epub">1432-2315</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">1257</article-id><article-id pub-id-type="doi">10.1007/s00371-016-1257-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Visual analytics and rendering for tunnel crack analysis</article-title><subtitle>A methodological approach for integrating geometric and attribute data</subtitle></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ortner</surname><given-names>Thomas</given-names></name><address><email>ortner@vrvis.at</email></address><xref ref-type="aff" rid="Aff1">1</xref><bio><sec id="FPar1"><title>Thomas Ortner</title><p id="Par1">received the graduate degree in computer graphics programming from the Faculty of Science and Engineering at the University of Hull (UK), in 2008. After working for several years as a programmer, he started his Ph.D. at the VRVis Research Center for Virtual Reality and Visualization. His current research topics involve the integration of 3D real-time rendering and abstract visualization techniques.<graphic position="anchor" xlink:href="371_2016_1257_Figa_HTML" id="MO10"/>
</p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Sorger</surname><given-names>Johannes</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec id="FPar2"><title>Johannes Sorger</title><p id="Par2">received the graduate degree in Visual Computing from the Faculty of Informatics at the TU Wien in 2013. He is working towards the Ph.D. degree at the VRVis Research Center for Virtual Reality and Visualization. His current research interests involve the integration of spatial and non-spatial data visualization, and parameter-space exploration in dynamic simulations.<graphic position="anchor" xlink:href="371_2016_1257_Figb_HTML" id="MO11"/>
</p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Piringer</surname><given-names>Harald</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec id="FPar3"><title>Harald Piringer</title><p id="Par3">is the head of the Visual Analysis Group at the VRVis Research Center in Vienna, Austria. His research interests include visual analytics for simulation and statistical modeling with a focus on the integration in real-world settings. He authored and co-authored more than twenty internationally refereed scientific publications, among them two best papers. Harald received his Master degree and his Ph.D. from TU Wien.<graphic position="anchor" xlink:href="371_2016_1257_Figc_HTML" id="MO12"/>
</p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Hesina</surname><given-names>Gerd</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec id="FPar4"><title>Gerd Hesina</title><p id="Par4">has experience in software development in various programming languages for 23 years, experience in 3D visualization projects for 13 years and is senior researcher and project manager at the VRVis Research Center since 2002. He is author and co-author of several internationally refereed scientific papers. In addition to his work at VRVis he is also lecturing at the University of Applied Sciences Technikum Wien. Gerd received both his Master degree and his Ph.D. in computer science from TU Wien.<graphic position="anchor" xlink:href="371_2016_1257_Figd_HTML" id="MO13"/>
</p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Gr&#x000f6;ller</surname><given-names>Eduard</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><bio><sec id="FPar5"><title>Eduard Gr&#x000f6;ller</title><p id="Par5">is professor at the TU Wien and adjunct professor of computer science at the University of Bergen, Norway. His research interests include computer graphics, visualization and visual computing. He co-authored more than 240 scientific publications and acted as a co-chair, IPC member, and reviewer for numerous conferences and journals in the field. He became a fellow of the Eurographics association in 2009. He is recipient of the Eurographics 2015 Outstanding Technical Contributions Award.<graphic position="anchor" xlink:href="371_2016_1257_Fige_HTML" id="MO14"/>
</p></sec></bio></contrib><aff id="Aff1"><label>1</label>VRVis Research Center, Vienna, Austria </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5329.d</institution-id><institution-id institution-id-type="ISNI">0000000123484034</institution-id><institution>TU Wien, </institution></institution-wrap>Vienna, Austria </aff></contrib-group><pub-date pub-type="epub"><day>11</day><month>5</month><year>2016</year></pub-date><pub-date pub-type="pmc-release"><day>11</day><month>5</month><year>2016</year></pub-date><pub-date pub-type="ppub"><year>2016</year></pub-date><volume>32</volume><issue>6</issue><fpage>859</fpage><lpage>869</lpage><permissions><copyright-statement>&#x000a9; The Author(s) 2016</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p id="Par6">The visual analysis of surface cracks plays an essential role in tunnel maintenance when assessing the condition of a tunnel. To identify patterns of cracks, which endanger the structural integrity of its concrete surface, analysts need an integrated solution for visual analysis of geometric and multivariate data to decide if issuing a repair project is necessary. The primary contribution of this work is a design study, supporting tunnel crack analysis by tightly integrating geometric and attribute views to allow users a holistic visual analysis of geometric representations and multivariate attributes. Our secondary contribution is Visual Analytics and Rendering, a methodological approach which addresses challenges and recurring design questions in integrated systems. We evaluated the tunnel crack analysis solution in informal feedback sessions with experts from tunnel maintenance and surveying. We substantiated the derived methodology by providing guidelines and linking it to examples from the literature.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1007/s00371-016-1257-5) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>3D real-time rendering</kwd><kwd>Visual analytics</kwd><kwd>Integration of spatial and non-spatial data</kwd><kwd>Methodology</kwd></kwd-group><funding-group><award-group><funding-source><institution>TU Wien (TUW)</institution></funding-source></award-group><open-access><p>Open access funding provided by [OrgName].</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer-Verlag Berlin Heidelberg 2016</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par7">The detection and documentation of cracks in the concrete surface of a tunnel are essential for assessing its condition. These cracks comprise a 3D polyline and several multivariate attribute values, such as length, width, orientation, and moisture. Tasks of analysts are, for instance, to identify patterns which endanger the structural integrity of the tunnel surface or assess the density of cracks along the tunnel and identify critical sections. Accomplishing such tasks and evaluating if a repair project is necessary typically requires the visual analysis of detailed geometric data and multivariate attributes simultaneously.</p><p id="Par8">The historical workflow in tunnel maintenance involves an analyst inspecting the tunnel surface on-site. Meanwhile inspections are mostly performed virtually on detailed, digitally reconstructed 3D models of the tunnel surface. Tunnel cracks are traced in high-resolution images by semi-automatic crack-detection algorithms. However, the analysis of multivariate data is still mostly performed via spreadsheets and static plots. Since geometric and attribute data are evaluated separately, no integrated workflow is supported resulting in tedious work to relate both aspects of the data.</p><sec id="Sec2"><title>Contributions</title><p id="Par9">The primary contribution of this paper is a design study for visual analysis of tunnel cracks in the context of tunnel maintenance. Abstracting from the specific problem domain of tunnel crack analysis we identified a general problem space emerging from the combination of geometric and attribute views, including obstacles and recurring design questions. Therefore, as a secondary contribution, we present VISAR (Visual Analytics and Rendering), a methodology to support system designers in creating integrated solutions that combine geometric and attribute data.</p></sec></sec><sec id="Sec3"><title>Related work</title><p id="Par10">A vast number of commercial and research-based tools for multivariate analysis exist, which allow users to explore data from different points of view&#x000a0;[<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR40">40</xref>]. Roberts&#x000a0;[<xref ref-type="bibr" rid="CR26">26</xref>] provides an extensive survey on coordinated multiple view (CMV) systems, which typically do not offer geometric rendering. 3D rendering engines are capable of rendering large sets of geometric data at interactive frame rates, but do not provide analytical capabilities. Widespread in engineering and urban planning are geographic information systems (GIS) or cartographic visualizations. These applications form a compromise between detailed spatial rendering and multivariate analysis, but typically either lack complex analytical capabilities&#x000a0;[<xref ref-type="bibr" rid="CR9">9</xref>] or are restricted to cartographic views&#x000a0;[<xref ref-type="bibr" rid="CR15">15</xref>].</p><p id="Par11">Solutions providing sufficiently detailed geometric rendering and means for the analysis of multivariate attributes are often tailored to a specific use case, such as in disaster management&#x000a0;[<xref ref-type="bibr" rid="CR25">25</xref>], 3D visibility analysis&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR22">22</xref>], urban data analysis&#x000a0;[<xref ref-type="bibr" rid="CR7">7</xref>], or lighting design&#x000a0;[<xref ref-type="bibr" rid="CR30">30</xref>]. These works combine visualizations of multivariate attributes with a detailed 3D spatial visualization for supporting decision-making in heterogeneous scenarios. Regardless of the problem domain, there are recurring challenges and design questions inherent to these heterogeneous scenarios, such as localization of an object in 3D space or identifying patterns across data domains. Although many authors use similar integration patterns, their discussion is often focused on a particular application rather than being generalized.</p><p id="Par12">In the context of CMV, multiple models and frameworks have emerged. Wang Baldonado et al.&#x000a0;[<xref ref-type="bibr" rid="CR39">39</xref>] formulate several guidelines on when and how to use CMV. Boukhelifa et al.&#x000a0;[<xref ref-type="bibr" rid="CR3">3</xref>] present a model on how coordination can be formalized and implemented, while Sorger et al.&#x000a0;[<xref ref-type="bibr" rid="CR31">31</xref>] developed a taxonomy to classify integration techniques of spatial and attribute visualizations. Actual frameworks include Snap-Together&#x000a0;[<xref ref-type="bibr" rid="CR20">20</xref>] and Improvise&#x000a0;[<xref ref-type="bibr" rid="CR40">40</xref>], which are highly focused on the design and coordination of multiple views. However, neither of these frameworks deal with the special intricacies of integrating spatial geometric views and multivariate attribute views.<fig id="Fig1"><label>Fig. 1</label><caption><p>
<bold>a</bold> Reconstructed mesh of the tunnel surface. Tunnels provide an additional metric coordinate system consisting of the value Sv running along the tunnels axis and B running perpendicular to it. Each crack comprises <bold>b</bold> its geometric representation, a 3D polyline, and <bold>c</bold> its attributes, such as length, width, moisture, and orientation. Red highlighting indicates the same crack across <bold>a</bold>, <bold>b</bold>, and <bold>c</bold>.</p></caption><graphic xlink:href="371_2016_1257_Fig1_HTML" id="MO1"/></fig>
</p></sec><sec id="Sec4"><title>Problem abstraction</title><p id="Par13">In the following section, we provide details on a tunnel maintenance scenario and the intricacies of tunnel crack data. We follow with a discussion of the problem space of combining geometric and attribute views and identify potential obstacles impeding an interactive visual analysis of the surface cracks.</p><sec id="Sec5"><title>Tunnel maintenance and tunnel crack analysis</title><p id="Par14">A tunnel goes through three phases, planning, construction, and maintenance. An essential part of its maintenance is the documentation and evaluation of the development of cracks in its concrete surface. Based on these evaluations, maintenance projects are issued to repair or completely replace parts of the concrete surface. Tunnel surveying companies increasingly perform virtual inspections on reconstructed 3D tunnel models instead of doing on-site inspections. This minimizes downtimes of tunnels and enables a more comprehensible analysis. The surface data are acquired by a rail-mounted laser scanner platform in very high geometric resolution. At the same time, images are taken by high-resolution cameras. Co-registration of geometry and images results in a 3D textured mesh (Fig&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a). Ortner et al.&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>] provide more detail on how the data are acquired, processed, and rendered. To extract the cracks, the image data are processed by a semi-automatic crack detection algorithm [<xref ref-type="bibr" rid="CR23">23</xref>].</p><p id="Par15">
<italic>Data abstraction</italic> The center of our design study is the visualization of and the interaction with tunnel cracks. The course of each tunnel crack on the tunnel surface is described by a 3D polyline as geometric representation (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). Along with the geometry, each crack is associated with an attribute vector, consisting of the attributes <italic>length, width, moisture, orientation</italic> (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c), and the additional coordinates <italic>Sv</italic> and <italic>B</italic>, which we discuss in the following paragraph. Moisture occurs in three categories, <italic>dry, moist, wet</italic> while orientation describes a crack&#x02019;s angle in relation to the course of the tunnel. Its values range from <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0^{\circ }$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mn>0</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq1.gif"/></alternatives></inline-formula> to <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$90^{\circ }$$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mn>90</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq2.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0^{\circ }$$\end{document}</tex-math><mml:math id="M6"><mml:msup><mml:mn>0</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq3.gif"/></alternatives></inline-formula> corresponds to cracks aligned with the tunnel direction.</p><p id="Par16">
<italic>The Sv/B coordinate system</italic> Experts dealing with linear infrastructures often use a one-dimensional metric value to describe a position along a highway, railroad track, river, or tunnel. This value is often called stationing. In our case, the coordinate <italic>Sv</italic> describes meters along the tunnel. With another coordinate <italic>B</italic> running on the surface and being orthogonal to the axis, the surface position of every crack can be determined (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a).</p><p id="Par17">
<italic>Tunnel sections</italic> Modern tunnel surfaces consist of spray concrete, which is wet concrete shot onto the raw excavation of the tunnel, resulting in a smooth surface. The concrete is applied continuously for a certain section of the tunnel, e.g., 120 m along the Sv coordinate, which forms a homogeneous surface. Individual cracks can be fixed by local injection of concrete, but larger damage typically requires the replacement of the concrete shell of a whole section. Therefore, it is essential for our users to investigate the number of cracks with respect to the Sv coordinate and identify potentially critical sections.</p></sec><sec id="Sec6"><title>Task analysis</title><p id="Par18">Cooperating with experts from tunnel maintenance and tunnel surveying, we identified a series of tasks in the context of tunnel crack analysis.<list list-type="bullet"><list-item><p id="Par19">Identification of cracks with anomalous values regarding moisture, length, and orientation.</p></list-item><list-item><p id="Par20">Analysis of moisture distribution in the tunnel.</p></list-item><list-item><p id="Par21">Identification of critical sections based on the density of cracks.</p></list-item><list-item><p id="Par22">Identifying intersecting cracks with high moisture that endanger the structural integrity of the tunnel surface.</p></list-item><list-item><p id="Par23">Investigation if moisture in one tunnel tube is also present in an adjacent tube.</p></list-item></list>
</p></sec><sec id="Sec7"><title>Problem space of combining geometric and attribute views</title><p id="Par24">Tory and M&#x000f6;ller&#x000a0;[<xref ref-type="bibr" rid="CR33">33</xref>] distinguish between data types where the spatialization is given and where it is chosen. To visualize each facet of our data effectively, we decided to visualize the geometric representation in a 3D real-time rendering view, which we refer to as the <italic>geometric view</italic>. The attributes of each crack are visualized in views with chosen spatialization, such as scatter plots, parallel coordinates, and histograms, which we denote as <italic>attribute views</italic>. Coordinating these views by linking &#x00026; brushing already allows analysts to utilize interactive visual analysis and discover phenomena that may not be apparent in a single view visualization&#x000a0;[<xref ref-type="bibr" rid="CR17">17</xref>].<fig id="Fig2"><label>Fig. 2</label><caption><p>
<bold>a</bold> 2D scatter plot showing orientation vs. length: linked selection of cracks, which are oriented along the tunnel direction, i.e., <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0^{\circ }$$\end{document}</tex-math><mml:math id="M8"><mml:msup><mml:mn>0</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq4.gif"/></alternatives></inline-formula>, and which are longer than 8 m. <bold>b</bold> The scene rendered from the current camera position showing only some of the selected cracks (<italic>red</italic>). <bold>c</bold> Some are partially outside, others are completely outside the view frustum. <bold>d</bold> Cracks occluded by the first tunnel wall are not visible from the current camera position.</p></caption><graphic xlink:href="371_2016_1257_Fig2_HTML" id="MO2"/></fig>
</p><p id="Par25">
<fig id="Fig3"><label>Fig. 3</label><caption><p>System overview: <bold>a</bold> the geometric view shows a textured mesh of north and south tunnel tube including tunnel cracks as polylines. <bold>b</bold> The scatter plot shows cracks as dots with respect to orientation on the <italic>x</italic>-axis vs. length on the <italic>y</italic>-axis. <bold>c</bold> The parallel coordinates plot enables the comparison and identification of trends for many dimensions. <bold>d</bold> The aggregation plot shows the distributions of moisture values in the north and south tunnel tube. Histograms are grouped to 120-m-long intervals of Sv. <bold>e</bold> Selections on length and orientation axis combined by logical AND. <bold>f</bold> Peek-brushing over the histogram causes blue highlights on the corresponding cracks in <bold>a</bold>, <bold>b</bold>, and <bold>c</bold>
</p></caption><graphic xlink:href="371_2016_1257_Fig3_HTML" id="MO3"/></fig>
</p><p id="Par26">However, the coordination of geometric and attribute views has numerous recurring challenges. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a shows an approach for enabling analysts to brush certain cracks in a scatter plot. To judge the spatial distribution, they need to identify the corresponding geometric representations in the geometric view. Some of these cracks may fully or partially lie outside (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>c) the view frustum of the geometric view (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b), or they may be fully or partially occluded (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>d) by other geometric objects in the scene.</p><p id="Par27">Elmqvist and Tsigas&#x000a0;[<xref ref-type="bibr" rid="CR8">8</xref>] define three <italic>visual perception tasks</italic>. In this context, we denote brushed cracks as part of the <italic>focus</italic> while the remaining cracks and the tunnel geometry are part of the <italic>context</italic> [<xref ref-type="bibr" rid="CR14">14</xref>]. Applying their abstraction to our use case, the visual perception tasks can be described as follows:<list list-type="bullet"><list-item><p id="Par28">
<italic>Discovery</italic> concerns locating a crack in the geometric view after it has been brushed in an attribute view.</p></list-item><list-item><p id="Par29">
<italic>Access</italic> is concerned with retrieving the shape or color of a crack in the geometric view.</p></list-item><list-item><p id="Par30">
<italic>Spatial relation</italic> concerns the assessment of spatial or geometric properties of brushed cracks with respect to other cracks or the tunnel surface geometry.</p></list-item></list>
</p></sec><sec id="Sec8"><title>Design goals</title><p id="Par31">The situation depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> illustrates that cracks in focus can easily fail the visual perception tasks, due to occlusion or being positioned outside the view frustum. Our visual analysis tool must integrate geometric and attribute views as such that users are able to perform their specific tasks, while preventing the visual perception tasks from failing. Based on this conclusion, we derived the following design goals:<list list-type="bullet"><list-item><p id="Par32">
<italic>G1</italic> Tight integration of the geometric view and the attribute views to allow users a simultaneous visual analysis of both data facets.</p></list-item><list-item><p id="Par33">
<italic>G2</italic> Support the localization of single and multiple cracks in the geometric view based on attribute criteria.</p></list-item><list-item><p id="Par34">
<italic>G3</italic> Encode attribute values in the geometric view to allow users to judge their spatial distribution.</p></list-item><list-item><p id="Par35">
<italic>G4</italic> Identification of clusters or outliers in the geometric and the attribute views.</p></list-item><list-item><p id="Par36">
<italic>G5</italic> Incorporating exploration and visualization metaphors that users are already familiar with.</p></list-item></list>
</p></sec></sec><sec id="Sec9"><title>Visualization and interaction design</title><p id="Par37">In this design study, we combine a geometric view and three attribute views. The geometric view utilizes the 3D real-time rendering framework Aardvark&#x000a0;[<xref ref-type="bibr" rid="CR36">36</xref>], while the attribute views are part of the visual analysis tool Visplore&#x000a0;[<xref ref-type="bibr" rid="CR37">37</xref>]. We briefly cover basic coordinations between both parts and discuss attribute views with a closer look on the aggregation plot. The main focus of this design study is the integration-specific design decisions concerning the geometric view. We conclude the section with a similarity-based analysis as a suitable tool to explore geometric and attribute data.</p><sec id="Sec10"><title>Linking and brushing, and color mapping</title><p id="Par38">Linking and brushing is one of the core concepts of CMVs and allows users to identify tunnel cracks across views. In our implementation, brushing of cracks in one view results in a red highlighting of the selected cracks, which is linked with all other views (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>e). Peek brushing [<xref ref-type="bibr" rid="CR1">1</xref>] causes a temporary selection of entities by hovering, which results in a blue highlighting (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>f). This allows users to instantly identify and compare tunnel cracks across all views. Users can map attribute values to colors, which are then shared among the views. If color mapping is active, we use size and transparency to emphasize selection and peek selection. Linking &#x00026; brushing and color mapping are fundamental to meeting <italic>G1</italic> and <italic>G3</italic>, respectively.</p></sec><sec id="Sec11"><title>Attribute views</title><p id="Par39">After implementing the aforementioned basic coordinations between the two tools, we discussed potential attribute views with our experts. We agreed on using a scatter plot (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b), a parallel coordinates plot (PCP) (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>c), and an aggregation plot (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>d). Users found the <italic>scatter plot</italic> very intuitive, and suitable for detecting outliers and clusters with respect to two dimensions. For instance, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>a shows an outlier in a scatter plot of length and orientation, which is a mineral deposit as shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>c. Users were less familiar with the <italic>PCP</italic>, but welcomed that it offers an instant overview of all relevant data columns. They were especially fond of the possibility to specify arbitrary criteria by combining selections on multiple axes, as it is illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>e. The scatter plot and the PCP partly address <italic>G4</italic> since they allow users to identify and brush outliers and clusters.</p><p id="Par40">The <italic>aggregation plot</italic> allows for splitting attribute data hierarchically along two dimensions. The individual parts of the data are presented as a matrix of counts or histograms. Together with our experts, we identified this view as suitable for estimating the density of cracks and identifying critical sections. To achieve this, we use the vertical dimension to distinguish between cracks in the south or in the north tunnel tube. We further map the Sv attribute to the horizontal axis and group it into intervals of 120 m, corresponding to the section size of the tunnel. This setup assigns cracks to individual tunnel tubes and tunnel sections. After mapping moisture values to the aggregation plot, each section shows individual counts of cracks for each of the three moisture values, as illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>d. Using the Sv coordinate as reference axis addresses <italic>G5</italic> and makes the aggregation plot very intuitive.<fig id="Fig4"><label>Fig. 4</label><caption><p>Visual discrimination: different visual variables are manipulated to emphasize the differences between peek-brushed, brushed, and context cracks. <bold>a</bold> Three different widths are used, while cracks in focus have a superimposed glow. <bold>b</bold> Using different widths and colors, <italic>blue</italic>, <italic>red</italic>, and <italic>yellow-green</italic> for peek brushed, brushed, and others. <bold>c</bold> Color is used for attribute mapping, while visual discrimination is only conveyed by width and glow.</p></caption><graphic xlink:href="371_2016_1257_Fig4_HTML" id="MO4"/></fig>
</p></sec><sec id="Sec12"><title>Geometric view</title><p id="Par41">The geometric view provides an interactive rendering of the geometric representations of cracks and the tunnel surface. It allows users to interactively navigate the scene and to assess the spatial extent and distribution of the cracks. We visualize the tunnel cracks using a line shader with screen-space scaling, so each polyline maintains a certain pixel width, regardless of the distance to the viewer. Brushed and peek-brushed cracks (cracks in focus) are highlighted in red and blue (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>b), respectively, while the color of context cracks is yellow-green. To further ensure their visibility, brushed and peek-brushed cracks are rendered with a higher pixel width than context cracks. We further use a separated Gaussian blur filter to create a glow effect&#x000a0;[<xref ref-type="bibr" rid="CR10">10</xref>] (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a), which we superimpose onto the cracks in focus. This also preserves visual discrimination of focus and context if color is used to encode attribute values (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>c).<fig id="Fig5"><label>Fig. 5</label><caption><p>Localization transition: <bold>a</bold> Users brush a single crack in the scatter plot. <bold>b</bold> A localization transition is triggered (<italic>1</italic>) the camera turns to selected crack (<italic>2</italic>) the camera moves to closest point on a computed orbit sphere (<italic>3</italic>) the camera rotates around the selected crack and ends up the characteristic viewpoint inside the tunnel. <bold>c</bold> The scene rendered from characteristic viewpoint.</p></caption><graphic xlink:href="371_2016_1257_Fig5_HTML" id="MO5"/></fig>
</p><p id="Par42">Our experts use the attribute views to get an overview of the multivariate part of their data. They either brush a single crack and seek to inspect its spatial representation (access), or they brush multiple cracks and are interested in their spatial distribution (spatial relation). In both cases, due to occlusion or cracks being outside of the current view frustum, this requires manual 3D navigation which is tedious and can lead to disorientation. To alleviate this and meet design goal <italic>G2</italic> we provide <italic>guided navigation</italic> techniques to ensure that all cracks in focus are inside the view frustum (Sect.&#x000a0;<xref rid="Sec13" ref-type="sec">4.3.1</xref>). We use a <italic>virtual X-ray technique</italic>&#x000a0;[<xref ref-type="bibr" rid="CR8">8</xref>] and a <italic>visual abstraction</italic>&#x000a0;[<xref ref-type="bibr" rid="CR28">28</xref>] to counteract occlusion (Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3.2</xref>).</p><sec id="Sec13"><title>Guided navigation</title><p id="Par43">
<italic>Single crack</italic> Together with our experts we defined what is a characteristic viewpoint&#x000a0;[<xref ref-type="bibr" rid="CR35">35</xref>] for a single crack, i.e., how should the 3D view provide access to a crack&#x02019;s spatial representation. During a virtual tunnel inspection, analysts inspect individual cracks by &#x02019;standing&#x02019; on the tunnel axis (i.e., inside the tunnel surface and 1.70&#x000a0;m above the ground) and viewing them at an almost orthogonal angle, which is based on an actual on-site inspection (<italic>G5</italic>). Since each crack has an Sv coordinate, we can compute a corresponding position 1.70&#x000a0;m above the tunnel axis. Setting the look-at vector to the center of the crack results in the characteristic viewpoint for a single crack.</p><p id="Par44">After users select a single crack in an attribute view (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>a), we employ an animated camera transition, the localization transition, illustrated in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>b: (1) we animate the camera&#x02019;s look-at vector to focus on the brushed crack. Before translating, we compute a transitional viewpoint. We create a sphere centered on the crack, where the radius corresponds to the distance between the center of the sphere and the characteristic viewpoint. (2) We compute the closest point on said sphere as a transitional viewpoint and animate the camera position. (3) The camera is focused on the crack, orbits along the sphere, and reaches the characteristic viewpoint (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>c).<fig id="Fig6"><label>Fig. 6</label><caption><p>
<bold>a</bold> Users investigate the distribution of cracks, which are orthogonal to the tunnel direction. <bold>b</bold> Selection triggers an automatic camera transition to a user-defined overview viewpoint. <bold>c</bold> Clicking on individual cracks seen from the overview viewpoint issues a localization transition, an additional click transitions back to the overview.</p></caption><graphic xlink:href="371_2016_1257_Fig6_HTML" id="MO6"/></fig>
</p><p id="Par45">
<italic>Multiple cracks</italic> The aggregation plot provides experts with exact distributions of attribute values with respect to tunnel sections. However, many tasks require analysts to judge the spatial distribution of attribute values more accurately and to gain immediate access to the corresponding spatial representations. Therefore, we employ camera transitions to allow users to intuitively investigate multiple cracks from overview and detail viewpoints.</p><p id="Par46">After users select multiple cracks in an attribute view (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>a), which are typically distributed along the tunnel, we trigger an animated camera transition to a user-defined overview viewpoint (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>b). Right clicking on a crack from this position triggers a localization transition. An additional right-click transitions the camera back to the overview. This provides immediate access to detailed spatial representations, for instance, when identifying a cluster of moist cracks.</p></sec><sec id="Sec14"><title>Handling occlusion and clutter</title><p id="Par47">
<italic>Virtual X-ray</italic> Our localization transition moves the camera to a characteristic viewpoint that is free of occlusion. However, it might be confusing when users are guided to a crack that is occluded during most of the transition. Further, when inspecting the tunnel from an overview position, many cracks are occluded by the tunnel surface. Elmqvist and Tsigas&#x000a0;[<xref ref-type="bibr" rid="CR8">8</xref>] present a survey on 3D occlusion management techniques. Based on their categorization, we developed a virtual X-ray technique, which allows users to see cracks in focus through the tunnel geometry. We achieve this by rendering the aforementioned glow for each focus crack without depth testing. Consequently, as it is illustrated in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>a, the glows of the cracks in focus shine through the tunnel wall.</p><p id="Par48">
<italic>Visual abstraction</italic> In some sections of the tunnel, the cracks occur in a high density. Viewing these sections from an overview position, the display is easily cluttered and it becomes difficult for users to distinguish between individual cracks. To counteract this, we replace the polyline of a crack with a point sprite if a certain distance threshold is reached. This levels-of-detail approach, or more generally levels-of-abstraction&#x000a0;[<xref ref-type="bibr" rid="CR28">28</xref>], reduces visual clutter and allows users to identify individual crack positions (discovery) and their color (partial access). Consequently, using point sprites as visual abstractions also meets design goals <italic>G3</italic> and <italic>G4</italic>.<fig id="Fig7"><label>Fig. 7</label><caption><p>
<bold>a</bold> Brushed cracks are highlighted in <italic>red</italic> including a glow effect. The glow effect persists even when the respective cracks are occluded. <bold>b</bold> Detailed tunnel crack representations are replaced by point sprites to avoid clutter.</p></caption><graphic xlink:href="371_2016_1257_Fig7_HTML" id="MO7"/></fig>
</p></sec></sec><sec id="Sec15"><title>Similarity-based analysis</title><p id="Par49">In some scenarios, analysts want to compare entities with respect to a typical pattern of attribute values. For instance, if there is a dominant pattern of long, dry cracks, oriented along the tunnel direction, analysts are interested in cracks that deviate from this. Therefore, we provide a similarity-based analysis, which allows users to specify a point of interest in their data, i.e., the <italic>focal point</italic>. We quantify similarity by a <italic>distance metric</italic> and treat the resulting distance as another attribute value for each crack. Color mapping enables the identification of tunnel cracks that are similar to or deviate from the specified focal point, which serves <italic>G4</italic>.</p><p id="Par50">Focal points are typically used in the context of parameter-space exploration&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. In general, a focal point is a user-defined <italic>n</italic>-tuple specifying concrete values for all or a subset of the <italic>n</italic> attributes of a data entity. Further, Berger et al.&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>] discriminate <italic>global</italic> and <italic>local</italic> updates of a focal point. Local updates only affect a subset of the attributes, while global updates affect all attributes at once.</p><p id="Par51">For the distance computation, we use a normalized Euclidean distance metric between a crack and the focal point with equally weighted components. Users can select the attributes they want to incorporate into the similarity computation. The focal point can be locally updated by specifying values on the axes of the scatter plot or the axes of the PCP. The respective coordinates of the focal point are represented by green lines. In the geometric view, users can perform a global update by selecting a crack as the focal point resulting in a green highlight.</p></sec><sec id="Sec16"><title>Expert feedback</title><p id="Par52">We conducted informal feedback sessions for confirming the usefulness of the developed tunnel-crack analysis tool. We interviewed four domain experts: two of whom are from the field of tunnel maintenance (A) and monitoring (B), whereas the others are from the fields of urban planning (C), and disaster management (D). Experts A and B were already familiar with using a 3D tunnel visualization for exploration of geometric data. Since multivariate analysis is mostly conducted in a paper-based, non-interactive form, they were eager to specify arbitrary selection criteria in the PCP and the scatter plot and successively refined them.</p><p id="Par53">When investigating multiple cracks, experts A and B found the overview and detail transitions very helpful. All experts found the localization of focus cracks and the localization transition essential for exploring the geometric view based on attribute criteria. Further, all experts deemed the overview viewpoint and the visual abstractions valuable, since many of their tasks involve the assessment of spatial distribution. Expert C explicitly complimented the implementation of user-specified overview viewpoints. He suggested to add a list for the management of multiple overview viewpoints.</p><p id="Par54">Experts A and B, found the glow implementation for occlusion management helpful for orientation, but found it confusing during virtual inspection. Consequently, we deactivate the effect when the camera is inside the tunnel. Expert C and D stated that the effect would be useful in an urban scenario, for instance, when objects are hidden behind buildings. When analyzing cracks in a detail viewpoint, two of the four experts desired an orbit navigation mode in addition to the implemented free-fly camera movement.</p><p id="Par55">After some explanation, the experts C and D could see the potential of similarity-based analysis, but they could not immediately imagine how this would translate to their use cases. The tunnel experts A and B on the other hand could immediately grasp the benefit of similarity-based analysis, when applied to the tunnel maintenance scenario. Expert B stated that similarity-based analysis would also translate very well to a tunnel monitoring use case. During construction of the tunnel, segments of it are allowed to shift within a given range of horizontal and vertical movement depending on the type of rock, which surrounds the segment. Considering a large number of shifting measurements along the tunnel over time, it would be helpful to explore them by dissimilarity to normative values. Further, it would be interesting to add time-dependent data from deviation measurements and surface geometry of the growing tunnel. In general, our integrated solution was well received. All experts could see the value of a system effectively combining geometric and attribute views on their data. The tunnel experts saw it as a solution to support currently cumbersome tasks.</p></sec></sec><sec id="Sec17"><title>Implementation</title><p id="Par56">We chose to combine two existing systems, the visual analysis tool Visplore&#x000a0;[<xref ref-type="bibr" rid="CR37">37</xref>] and the 3D real-time rendering engine Aardvark&#x000a0;[<xref ref-type="bibr" rid="CR36">36</xref>]. Visplore is designed to handle and visualize large amounts of multivariate tabular data. Besides typical 3D rendering features, Aardvark can import various data formats and manages out-of-core streaming of large geometry and texture data. Both systems are trimmed to scale well with their respective data facet and were able to render the scenario at interactive frame rates. Both systems are used separately for numerous application-oriented research projects, such as urban planning, tunnel documentation, engine design, and power management, and are not publicly available. We extended both systems with a custom-built communication layer based on web sockets, which allowed us to handle the coordination between the two systems by exchanging JSON messages.</p></sec><sec id="Sec18"><title>The VISAR framework</title><p id="Par57">As outlined in Sect.&#x000a0;<xref rid="Sec3" ref-type="sec">2</xref>, many authors combine geometric and attribute views through similar integration approaches. Their discussion, however, is often focused on the particular application rather than on general applicability. Based on the insights gained during conduction of the presented design study and based on the review of related literature, we could identify common obstacles and recurring design questions inherent to the integration of geometric and attribute data. As a result, we present VISAR, a methodological framework addressing this integration on a more general level to support visualization designers in effectively combining geometric and attribute views.</p><p id="Par58">Integrated solutions, which are tailored to a specific use case, typically focus on the analysis of a certain data entity: flooded buildings in disaster management&#x000a0;[<xref ref-type="bibr" rid="CR25">25</xref>], buildings&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>] and lines-of-sight&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>] in urban planning, illuminated surfaces&#x000a0;[<xref ref-type="bibr" rid="CR30">30</xref>] in interior lighting design, or tunnel cracks in the presented scenario. We generalize these entities as <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e = (g,a)$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq5.gif"/></alternatives></inline-formula>, where <italic>g</italic> is the geometric spatial representation and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a=(a_{1},\ldots ,a_{n})$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq6.gif"/></alternatives></inline-formula> is the attribute vector of length <italic>n</italic>. Analogous to the tunnel cracks, entities of the form <italic>e</italic> are subject to the problem space described in Sect.&#x000a0;<xref rid="Sec7" ref-type="sec">3.3</xref>, since <italic>g</italic> may lie outside the current view frustum or may be occluded by other geometry in the scene. Although concrete tasks and actual design decisions depend on the specific use case, it is essential that an integrated approach allows the accomplishment of all visual perception tasks.<fig id="Fig8"><label>Fig. 8</label><caption><p>The VISAR framework is divided into two layers: the mirroring layer and the integration layer. The mirroring layer contains simple coordinations, such as Selection, Peek Selection, and Color. The components of the integration layer are concerned with more complex coordinations that facilitate the visual perception tasks (see Sect.&#x000a0;<xref rid="Sec7" ref-type="sec">3.3</xref>): Guided Navigation, Visual Encoding, and Similarity-Based Analysis.</p></caption><graphic xlink:href="371_2016_1257_Fig8_HTML" id="MO8"/></fig>
</p><p id="Par59">To support visualization designers in achieving this goal, our methodological framework addresses the integration between heterogeneous systems on two levels, which are reflected in the two-layer structure of VISAR: the <italic>mirroring layer</italic> and the <italic>integration layer</italic> (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). The mirroring layer covers coordinations between views in general, while the integration layer explicitly deals with the coordination of geometric and attribute views to prevent visual perception tasks from failing.</p><sec id="Sec19"><title>Mirroring layer and integration layer</title><p id="Par60">The <italic>mirroring layer</italic> is responsible for straightforward coordinations, which are shared among all views regardless of their type. This includes the selection, i.e., brushing, of entities or the encoding of attribute values into entity colors. Since the mirroring layer encompasses principles implemented in most CMV systems, it acts as the foundation of the actual integration. This includes coordinated interactions, such as linking &#x00026; brushing [<xref ref-type="bibr" rid="CR12">12</xref>], focus &#x00026; context [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR34">34</xref>], or coordinated color mapping [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR16">16</xref>].</p><p id="Par61">The <italic>integration layer</italic> is concerned with the coordination between geometric and attribute views and its components explicitly address the discussed problem space to facilitate the visual perception tasks. Through literature review and generalization of the implementations discussed in Sect.&#x000a0;<xref rid="Sec9" ref-type="sec">4</xref>, we derived the components <italic>guided navigation</italic>, <italic>enhanced geometric rendering</italic>, and <italic>similarity-based analysis</italic> for the integration layer. We will elaborate on each component and its sub-components adhering to the following structure: purpose of the component, design goals of its subcomponents, design choices, and comparison to the literature.</p></sec><sec id="Sec20"><title>Guided navigation</title><p id="Par62">The purpose of the guided navigation component is to allow users the localization of the geometric representations of entities in focus by means of automated camera transitions. The intent of localization is typically preceded by brushing one or more entities in an attribute view. We distinguish two goals:<list list-type="order"><list-item><p id="Par63">Provide users with the discovery and detailed shape access of a single entity.</p></list-item><list-item><p id="Par64">Provide users with the discovery of multiple entities and allow users to judge their spatial relation.</p></list-item></list>
<italic>Localization</italic> We address (1) by a localization transition that animates the camera to a characteristic viewpoint. The transition itself and the characteristic viewpoint typically depend on the use case. Design choices are the computation of a characteristic viewpoint and the definition of a transition path that does not cause disorientation. Viola et al.&#x000a0;[<xref ref-type="bibr" rid="CR35">35</xref>] use an information theory approach to estimate characteristic viewpoints of internal organs in a medical data set. When selecting an organ a localization transition is triggered and moves the camera along a bounding sphere. Buchholz et al.&#x000a0;[<xref ref-type="bibr" rid="CR4">4</xref>] discuss a constraint-based framework for navigation in virtual 3D landscapes.</p><p id="Par65">
<italic>Overview and detail</italic> We address (2) by an overview transition that animates the camera to reach an overview viewpoint, which contains all entities in focus. Design choices concern the definition of a suitable overview viewpoint and how to provide a transition that prevents users from disorientation. Although there is no example actually computing a characteristic overview viewpoint for a changing set of focus entities, approaches can be found in the category of tour planning techniques in Elmqvist and Tsigas [<xref ref-type="bibr" rid="CR8">8</xref>].</p></sec><sec id="Sec21"><title>Enhanced geometric rendering</title><p id="Par66">The enhanced geometric rendering component manipulates the geometric representations of entities to achieve the following goals:<list list-type="order"><list-item><p id="Par67">Discrimination between entities in focus and entities or other geometric objects as part of the context.</p></list-item><list-item><p id="Par68">Completing all visual perception tasks for entities in focus despite suffering from occlusion.</p></list-item><list-item><p id="Par69">Allowing users to judge spatial relation and provide access to encoded values from overview viewpoints.</p></list-item></list>
<italic>Visual discrimination</italic> In general, (1) is achieved by highlighting geometry in focus and/or lowlighting geometry belonging to the context [<xref ref-type="bibr" rid="CR14">14</xref>]. In many cases, the highlighting / lowlighting of the geometric representation of an entity requires design decisions that preserve color, textures, size, or shape, which may be essential to an effective analysis. Trapp et al.&#x000a0;[<xref ref-type="bibr" rid="CR34">34</xref>] evaluated several techniques for highlighting in 3D geovirtual environments. Outline-based techniques and style-variant techniques, that consider use-case context, are suited best to enable visual discrimination and preserve relevant spatial properties.</p><p id="Par70">
<italic>Occlusion management</italic> To address (2), a suitable occlusion management technique is necessary to ensure that discovery, access, and spatial relation tasks do not fail, although the entities in focus are occluded. Elmqvist and Tsigas&#x000a0;[<xref ref-type="bibr" rid="CR8">8</xref>] provide a wide variety of design alternatives. Virtual X-ray techniques appear to be most suitable for geometric views combined with attribute views, since they are reliable and preserve the most geometric properties.</p><p id="Par71">
<italic>Visual abstraction</italic> With increasing distance, and also dependent on the density entities occur in, the display becomes cluttered and discovery is impeded. We address (3) by replacing the entities by visual abstractions. Such an abstraction may range from a geometric simplification, to a glyph representations summarizing multiple entities&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>], up to small attribute views integrated into the 3D scenes &#x000a0;[<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. Chang et al.&#x000a0;[<xref ref-type="bibr" rid="CR7">7</xref>] employ a hierarchical simplification of city models adhering to perception constraints derived from urban planning&#x000a0;[<xref ref-type="bibr" rid="CR6">6</xref>]. Semmo et al.&#x000a0;[<xref ref-type="bibr" rid="CR28">28</xref>] developed a levels-of-abstraction framework for blending between discrete simplification levels for various geospatial features.</p></sec><sec id="Sec22"><title>Similarity-based analysis</title><p id="Par72">Similarity-based analysis allows users to explore their data by means of similarity and deviation with the following goals:<list list-type="order"><list-item><p id="Par73">Comparison of entities with respect to a user-specified focal point in the data.</p></list-item><list-item><p id="Par74">Detecting clusters of similar entities or identification of entities deviating from a common pattern.</p></list-item></list>
<italic>Similarity</italic> We define the similarity of an entity <italic>e</italic> to the focal point <italic>f</italic> as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\text{ s }imilarity}(f,e) = \left( \displaystyle \sum \limits _{i=1}^n w_i \cdot {\text{ d }ist}\left( f_i, e_i\right) ^p\right) ^\frac{1}{p} \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable columnspacing="0.5ex"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>s</mml:mtext><mml:mspace width="0.333333em"/><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>d</mml:mtext><mml:mspace width="0.333333em"/><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mi>p</mml:mi></mml:msup></mml:mstyle></mml:mfenced><mml:mfrac><mml:mn>1</mml:mn><mml:mi>p</mml:mi></mml:mfrac></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="371_2016_1257_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>For dist(<italic>f</italic>,&#x000a0;<italic>e</italic>) different distance metrics can be inserted, such as the Euclidean distance (<inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=2$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="371_2016_1257_Article_IEq7.gif"/></alternatives></inline-formula>). Further, different weights can be applied in form of the <italic>n</italic>-dimensional weighting vector <italic>w</italic>. Which distance metric to use depends on the application scenario and the involved data types. Migut et al.&#x000a0;[<xref ref-type="bibr" rid="CR19">19</xref>] present approaches for metrics considering nominal attributes and weights. Mesh comparison metrics, as presented by Schmidt et al.&#x000a0;[<xref ref-type="bibr" rid="CR27">27</xref>], allow the integration of the geometric part of the data into the similarity computation.</p><p id="Par75">
<italic>Focal point updates</italic> Berger et al.&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>] distinguish <italic>global</italic> and <italic>local</italic> updates of the focal point. An integrated solution may allow local or global updates of the focal point in geometric or attribute views. Selecting a particular entity as the focal point in any view is a straightforward way for a global update. In Legible Cities&#x000a0;[<xref ref-type="bibr" rid="CR7">7</xref>], users can select individual neighborhoods as focal point in 3D by a pin metaphor in a census dataset.</p></sec></sec><sec id="Sec23"><title>Discussion</title><p id="Par76">We developed an effective solution for the visual analysis of tunnel cracks by integrating geometric and attribute views. We derived VISAR, which we see as an extended discussion of our design study in terms of generalization and believe in its applicability to a wide range of use cases involving geometric and attribute data. At this point we want to briefly reflect on the limitations and possible extensions of this methodology.</p><p id="Par77">The VISAR methodology mostly concerns the side of the geometric visualization, which is based on the fact that the three visual perception tasks are more likely to fail in 3D space than in the visualization space of the attribute visualizations. Further, our users tended to employ attribute views for exploring the geometric view, which is denoted as explore &#x00026; feedback [<xref ref-type="bibr" rid="CR31">31</xref>]. However, we realized, that the visual feedback of, for instance, scatter plots reacting to a selection in the geometric view is often not sufficient to direct a users&#x02019;s visual attention. This issue could be alleviated using visual links&#x000a0;[<xref ref-type="bibr" rid="CR38">38</xref>] connecting geometric representations and attribute representations. In general, our methodology strives to generate solutions employing a balanced integration to allow a continuous feedback loop between both visualization domains [<xref ref-type="bibr" rid="CR31">31</xref>].</p></sec><sec id="Sec24"><title>Conclusion and future work</title><p id="Par78">In this paper, we present an integrated solution, as the result of a design study, to support experts in tunnel crack analysis in the context of a tunnel maintenance scenario. We evaluated the resulting visual analysis tool with our experts and gained overall very positive feedback. We further derived VISAR, a methodological framework to assist visualization designers which building integrated solutions in similar heterogeneous scenarios. We plan to use VISAR in future projects, as for instance, in an extension of the tunnel maintenance scenario which will encompass tunnel deformation data over time, or in a project concerned with remote geological analysis on Mars involving derivation and interpretation of statistical data from thousands of geological measurements. While employing VISAR ourselves, we will promote it to fellow research groups to be able to gather valuable feedback concerning VISAR&#x02019;s true applicability and usefulness on a broader scope.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec25"><p>Below is the link to the electronic supplementary material.
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="371_2016_1257_MOESM1_ESM.mp4"><caption><p>Supplementary material 1 (mp4 25550 KB)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><ack><p>Open access funding provided by TU Wien (TUW). This work has been supported in the scope of the FWF-funded Project P24597-N23 (VISAR). Johannes Sorger has been partially supported by the Vienna Science and Technology Fund (WWTF) through project VRG11-010. The authors would like to thank the experts from the Dibit Messtechnik GmbH and from the GEODATA ZT GmbH for their valuable feedback.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Berger, W., Piringer, H.: Peek brush: A high-speed lightweight ad-hoc selection for multiple coordinated views. In: Proceedings of the IEEE Conference on Information Visualization, pp. 140&#x02013;145 (2010)</mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>W</given-names></name><name><surname>Piringer</surname><given-names>H</given-names></name><name><surname>Filzmoser</surname><given-names>P</given-names></name><name><surname>Gr&#x000f6;ller</surname><given-names>E</given-names></name></person-group><article-title>Uncertainty-aware exploration of continuous parameter spaces using multivariate prediction</article-title><source>Comput. Graph. Forum</source><year>2011</year><volume>30</volume><fpage>911</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2011.01940.x</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Boukhelifa, N., Roberts, J., Rodgers, P.: A coordination model for exploratory multiview visualization. In: Proceedings of the International Conference on Coordinated and Multiple Views in Exploratory Visualization, pp. 76&#x02013;85 (2003)</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Buchholz, H., Bohnet, J., D&#x000f6;llner, J.: Smart and physically-based navigation in 3d geovirtual environments. In: Proceedings of the International Conference on Information Visualisation, pp. 629&#x02013;635 (2005)</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butkiewicz</surname><given-names>T</given-names></name><name><surname>Dou</surname><given-names>W</given-names></name><name><surname>Wartell</surname><given-names>Z</given-names></name><name><surname>Ribarsky</surname><given-names>W</given-names></name><name><surname>Chang</surname><given-names>R</given-names></name></person-group><article-title>Multi-focused geospatial analysis using probes</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2008</year><volume>14</volume><issue>6</issue><fpage>1165</fpage><lpage>1172</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2008.149</pub-id><pub-id pub-id-type="pmid">18988960</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Chang, R., Butkiewicz, T., Ziemkiewicz, C., Wartell, Z., Pollard, N., Ribarsky, W.: Hierarchical simplification of city models to maintain urban legibility. In: Proceedings of ACM SIGGRAPH, vol. 6, p. 130 (2006)</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>R</given-names></name><name><surname>Wessel</surname><given-names>G</given-names></name><name><surname>Kosara</surname><given-names>R</given-names></name><name><surname>Sauda</surname><given-names>E</given-names></name><name><surname>Ribarsky</surname><given-names>W</given-names></name></person-group><article-title>Legible cities: focus-dependent multi-resolution visualization of urban relationships</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2007</year><volume>13</volume><issue>6</issue><fpage>1169</fpage><lpage>1175</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2007.70574</pub-id><pub-id pub-id-type="pmid">17968061</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elmqvist</surname><given-names>N</given-names></name><name><surname>Tsigas</surname><given-names>P</given-names></name></person-group><article-title>A taxonomy of 3d occlusion management for visualization</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2008</year><volume>14</volume><issue>5</issue><fpage>1095</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2008.59</pub-id><pub-id pub-id-type="pmid">18599920</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">ESRI: Arcgis.&#x000a0;<ext-link ext-link-type="uri" xlink:href="http://www.esri.com/software/arcgis">http://www.esri.com/software/arcgis</ext-link>. Accessed 7 Oct 2013</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Fernando, R.: GPU Gems: Programming Techniques. Tips and Tricks for Real-Time Graphics, Pearson Higher Education (2004)</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Ferreira, N., Lage, M., Doraiswamy, H., Vo, H., Wilson, L., Werner, H., Park, M., Silva, C.: Urbane: A 3d framework to support data driven decision making in urban development. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology, pp. 97&#x02013;104 (2015)</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuchs</surname><given-names>R</given-names></name><name><surname>Hauser</surname><given-names>H</given-names></name></person-group><article-title>Visualization of multi-variate scientific data</article-title><source>Comput. Graph. Forum</source><year>2009</year><volume>28</volume><issue>6</issue><fpage>1670</fpage><lpage>1690</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2009.01429.x</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Gresh, D.L., Rogowitz, B.E., Winslow, R.L., Scollan, D.F., Yung, C.K.: WEAVE: a system for visually linking 3-D and statistical visualizations, applied to cardiac simulation and measurement data. In: Proceedings of the Conference on Visualization, pp. 489&#x02013;492 (2000)</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Hauser, H.: Generalizing focus+ context visualization. In: Scientific visualization: The visual extraction of knowledge from data, pp. 305&#x02013;327. Springer, Berlin (2006)</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Jern, M., Johansson, S., Johansson, J., Franzen, J.: The gav toolkit for multiple linked views. In: Proceedings of the International Conference on Coordinated and Multiple Views in Exploratory Visualization, pp. 85&#x02013;97 (2007)</mixed-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jianu</surname><given-names>R</given-names></name><name><surname>Demiralp</surname><given-names>C</given-names></name><name><surname>Laidlaw</surname><given-names>DH</given-names></name></person-group><article-title>Exploring 3d dti fiber tracts with linked 2d representations</article-title><source>IEEE Trans. Vis. Comput. Graph</source><year>2009</year><volume>15</volume><issue>6</issue><fpage>1449</fpage><lpage>1456</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2009.141</pub-id><pub-id pub-id-type="pmid">19834220</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kehrer</surname><given-names>J</given-names></name><name><surname>Hauser</surname><given-names>H</given-names></name></person-group><article-title>Visualization and visual analysis of multifaceted scientific data: a survey</article-title><source>IEEE Trans. Vis. Comput. Graph</source><year>2013</year><volume>19</volume><issue>3</issue><fpage>495</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2012.110</pub-id><pub-id pub-id-type="pmid">22508905</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Matkovic, K., Freiler, W., Gracanin, D., Hauser, H.: Comvis: A coordinated multiple views system for prototyping new visualization technology. In: Proceedings of the IEEE Conference on Information Visualization, pp. 215&#x02013;220 (2008)</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Migut, M., van Gemert, J., Worring, M.: Interactive decision making using dissimilarity to visually represented prototypes. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology, pp. 141&#x02013;149 (2011)</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">North, C., Shneiderman, B.: Snap-together visualization: a user interface for coordinating visualizations via relational schemata. In: Proceedings of the Working Conference on Advanced Visual Interfaces, pp. 128&#x02013;135 (2000)</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Ortner, T., Paar, G., Hesina, G., Tobler, R.F., Nauschnegg, B.: Towards true underground infrastructure surface documentation. In: Proceedings of Real CORP (2010)</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Ortner, T., Sorger, J., Steinlechner, H., Hesina, G., Piringer, H., Gr&#x000f6;ller, E.: Vis-a-ware: Integrating spatial and non-spatial visualization for visibility-aware urban planning. IEEE Trans. Vis. Comput. Graph (2016) (in print)</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Paar, G., Caballo-Perucha, M.d.P., Kontrus, H., Sidla, O.: Optical crack following on tunnel surfaces. In: SPIE Proceedings of Two- and Three-Dimensional Methods for Inspection and Metrology IV, vol. 6382 (2006)</mixed-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piringer</surname><given-names>H</given-names></name><name><surname>Berger</surname><given-names>W</given-names></name><name><surname>Krasser</surname><given-names>J</given-names></name></person-group><article-title>Hypermoval: Interactive visual validation of regression models for real-time simulation</article-title><source>Comput. Graph. Forum</source><year>2010</year><volume>29</volume><fpage>983</fpage><lpage>992</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2009.01684.x</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ribicic</surname><given-names>H</given-names></name><name><surname>Waser</surname><given-names>J</given-names></name><name><surname>Fuchs</surname><given-names>R</given-names></name><name><surname>Bl&#x000f6;schl</surname><given-names>G</given-names></name><name><surname>Gr&#x000f6;ller</surname><given-names>E</given-names></name></person-group><article-title>Visual analysis and steering of flooding simulations</article-title><source>IEEE Trans. Vis. Comput. Graph</source><year>2013</year><volume>19</volume><issue>6</issue><fpage>1062</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2012.175</pub-id><pub-id pub-id-type="pmid">23559514</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Roberts, J.: State of the Art: Coordinated Multiple Views in Exploratory Visualization. In: Proceedings of the International Conference on Coordinated and Multiple Views in Exploratory Visualization, pp. 61&#x02013;71 (2007)</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Schmidt, J., Preiner, R., Auzinger, T., Wimmer, M., Gr&#x000f6;ller, M.E., Bruckner, S.: Ymca - your mesh comparison application. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology, pp. 153&#x02013;162 (2014)</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semmo</surname><given-names>A</given-names></name><name><surname>Trapp</surname><given-names>M</given-names></name><name><surname>Kyprianidis</surname><given-names>JE</given-names></name><name><surname>D&#x000f6;llner</surname><given-names>J</given-names></name></person-group><article-title>Interactive visualization of generalized virtual 3d city models using level-of-abstraction transitions</article-title><source>Comput. Graph. Forum</source><year>2012</year><volume>31</volume><fpage>885</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2012.03081.x</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Software, T.: Tableau.&#x000a0;<ext-link ext-link-type="uri" xlink:href="http://www.tableau.com/">http://www.tableau.com/</ext-link>. Accessed 07 Oct 2013</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorger</surname><given-names>J</given-names></name><name><surname>Ortner</surname><given-names>T</given-names></name><name><surname>Luksch</surname><given-names>C</given-names></name><name><surname>Schwarzler</surname><given-names>M</given-names></name><name><surname>Gr&#x000f6;ller</surname><given-names>E</given-names></name><name><surname>Piringer</surname><given-names>H</given-names></name></person-group><article-title>Litevis: integrated visualization for simulation-based decision support in lighting design</article-title><source>IEEE Trans. Vis. Comput. Graph</source><year>2015</year><volume>22</volume><issue>1</issue><fpage>290</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2015.2468011</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Sorger, J., Ortner, T., Piringer, H., Hesina, G., Gr&#x000f6;ller, M.E.: A taxonomy of integration techniques for spatial and non-spatial visualizations. In: Proceedings of the Eurographics Symposium on Vision, Modeling and Visualization (2015)</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">TIBCO: Spotfire. <ext-link ext-link-type="uri" xlink:href="http://spotfire.tibco.com/">http://spotfire.tibco.com/</ext-link>. Accessed 07 Oct 2013</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Tory, M., M&#x000f6;ller, T.: Rethinking Visualization: A High-Level Taxonomy. In: Proceedings of the IEEE Conference on Information Visualization, pp. 151&#x02013;158 (2004)</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Trapp, M., Beesk, C., Pasewaldt, S., D&#x000f6;llner, J.: Interactive rendering techniques for highlighting in 3d geovirtual environments. In: Advances in 3D Geo-Information Sciences, pp. 197&#x02013;210. Springer, Berlin (2011)</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>I</given-names></name><name><surname>Feixas</surname><given-names>M</given-names></name><name><surname>Sbert</surname><given-names>M</given-names></name><name><surname>Gr&#x000f6;ller</surname><given-names>M</given-names></name></person-group><article-title>Importance-driven focus of attention</article-title><source>IEEE Trans. Vis. Comput. Graph</source><year>2006</year><volume>12</volume><issue>5</issue><fpage>933</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2006.152</pub-id><pub-id pub-id-type="pmid">17080819</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">VRVis: Aardvark. <ext-link ext-link-type="uri" xlink:href="http://www.vrvis.at/projects/aardvark">http://www.vrvis.at/projects/aardvark</ext-link>. Accessed 22 Oct 2013</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">VRVis: Visplore. <ext-link ext-link-type="uri" xlink:href="http://www.vrvis.at/projects/visplore">http://www.vrvis.at/projects/visplore</ext-link>. Accessed 22 Oct 2013</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Waldner, M., Puff, W., Lex, A., Streit, M., Schmalstieg, D.: Visual links across applications. In: Proceedings of Graphics Interface, pp. 129&#x02013;136 (2010)</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Wang Baldonado, M.Q., Woodruff, A., Kuchinsky, A.: Guidelines for using multiple views in information visualization. In: Proceedings of the Working Conference on Advanced Visual Interfaces, pp. 110&#x02013;119 (2000)</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Weaver, C.: Building highly-coordinated visualizations in improvise. In: Proceedings of the IEEE Conference on Information Visualization, pp. 159&#x02013;166 (2004)</mixed-citation></ref></ref-list></back></article>