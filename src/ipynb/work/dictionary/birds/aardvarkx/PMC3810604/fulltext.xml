<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.3389/fpsyg.2013.00777</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Statistics of high-level scene context</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Greene</surname><given-names>Michelle R.</given-names></name><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref></contrib></contrib-group><aff><institution>Department of Computer Science, Stanford University</institution><country>Stanford, CA, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Gregory Zelinsky, Stony Brook University, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Ben Tatler, University of Dundee, UK; Ruth Rosenholtz, Massachusetts Institute of Technology, USA</p></fn><corresp id="fn001">*Correspondence: Michelle R. Greene, Department of Computer Science, Stanford University, 353 Serra Mall Rm 241, Stanford, CA 94305, USA e-mail: <email xlink:type="simple">mrgreene@stanford.edu</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Perception Science, a section of the journal Frontiers in Psychology.</p></fn></author-notes><pub-date pub-type="epub"><day>29</day><month>10</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>4</volume><elocation-id>777</elocation-id><history><date date-type="received"><day>25</day><month>4</month><year>2013</year></date><date date-type="accepted"><day>03</day><month>10</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 Greene.</copyright-statement><copyright-year>2013</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Context is critical for recognizing environments and for searching for objects within them: contextual associations have been shown to modulate reaction time and object recognition accuracy, as well as influence the distribution of eye movements and patterns of brain activations. However, we have not yet systematically quantified the relationships between objects and their scene environments. Here I seek to fill this gap by providing descriptive statistics of object-scene relationships. A total of 48, 167 objects were hand-labeled in 3499 scenes using the LabelMe tool (Russell et al., <xref ref-type="bibr" rid="B86">2008</xref>). From these data, I computed a variety of descriptive statistics at three different levels of analysis: the ensemble statistics that describe the density and spatial distribution of unnamed &#x0201c;things&#x0201d; in the scene; the bag of words level where scenes are described by the list of objects contained within them; and the structural level where the spatial distribution and relationships between the objects are measured. The utility of each level of description for scene categorization was assessed through the use of linear classifiers, and the plausibility of each level for modeling human scene categorization is discussed. Of the three levels, ensemble statistics were found to be the most informative (per feature), and also best explained human patterns of categorization errors. Although a bag of words classifier had similar performance to human observers, it had a markedly different pattern of errors. However, certain objects are more useful than others, and ceiling classification performance could be achieved using only the 64 most informative objects. As object location tends not to vary as a function of category, structural information provided little additional information. Additionally, these data provide valuable information on natural scene redundancy that can be exploited for machine vision, and can help the visual cognition community to design experiments guided by statistics rather than intuition.</p></abstract><kwd-group><kwd>context</kwd><kwd>scene</kwd><kwd>ensemble</kwd><kwd>bag of words</kwd><kwd>data mining</kwd><kwd>scene understanding</kwd></kwd-group><counts><fig-count count="14"/><table-count count="12"/><equation-count count="3"/><ref-count count="109"/><page-count count="31"/><word-count count="23936"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Imagine that you are attending a friend's housewarming party. Although you have never been in this house before, you are not surprised to find a coffee table next to a sofa in the living room, chairs surrounding the dining room table, or framed pictures hanging on the walls. As a considerate guest, you help with the cleanup afterwards; effortlessly finding the trash can under the sink for disposing the waste, and the dishwasher next to the cabinets to wash the dishes. Our interactions in the world are facilitated by virtue of the fact that objects are not randomly strewn about the world but follow some basic laws of where they may be located, how large they are, and what other objects will be found near them. Collectively, these regularities are known as <italic>context</italic>. While context appears to be crucial for human scene recognition and helpful for machine vision (see Bar, <xref ref-type="bibr" rid="B10">2004</xref>; Oliva and Torralba, <xref ref-type="bibr" rid="B75">2007</xref> for reviews), contextual relations between scenes and their objects have not yet been systematically measured and cataloged.</p><sec><title>Why measure statistics of object context?</title><p>The last two decades have seen a growing literature on the statistics of natural images. Knowing about the input received by our visual systems allows for a better understanding of visual coding in the brain. We have a growing understanding of the statistical regularities of natural scenes at the level of basic features such as luminance, contrast, color and Fourier amplitude spectra, as well as the relations between edges and contours (Olshausen and Field, <xref ref-type="bibr" rid="B76">1996</xref>; van Hateren and Ruderman, <xref ref-type="bibr" rid="B99">1998</xref>; Fine and MacLeod, <xref ref-type="bibr" rid="B41">2001</xref>; Geisler et al., <xref ref-type="bibr" rid="B45">2001</xref>; Schwartz and Simoncelli, <xref ref-type="bibr" rid="B88">2001</xref>; Golz and MacLeod, <xref ref-type="bibr" rid="B46">2002</xref>; Torralba and Oliva, <xref ref-type="bibr" rid="B95">2003</xref>; Howe and Purves, <xref ref-type="bibr" rid="B55">2004</xref>). Mid-level regularities have been found for scene textures (Torralba and Oliva, <xref ref-type="bibr" rid="B95">2003</xref>) as well as scene scale and depth (Ruderman, <xref ref-type="bibr" rid="B85">1994</xref>; Torralba and Oliva, <xref ref-type="bibr" rid="B94">2002</xref>). Higher-level statistical regularities, such as object location (Karklin and Lewicki, <xref ref-type="bibr" rid="B59">2003</xref>), scene background to objects (Torralba and Sinha, <xref ref-type="bibr" rid="B96">2001</xref>; Choi et al., <xref ref-type="bibr" rid="B28">2010</xref>) and scene spatial structure (Schyns and Oliva, <xref ref-type="bibr" rid="B89">1994</xref>) have also been measured. The importance of this work lies in the predictive power of image statistics for both behavior and neural responses (Rao et al., <xref ref-type="bibr" rid="B82">2002</xref>; for reviews, see Simoncelli and Olshausen, <xref ref-type="bibr" rid="B90">2001</xref>; Geisler, <xref ref-type="bibr" rid="B44">2008</xref>). It has been hypothesized that the visual system exploits statistical redundancies in order to efficiently code a complex visual world (Attneave, <xref ref-type="bibr" rid="B7">1954</xref>; Zetzsche et al., <xref ref-type="bibr" rid="B107">1993</xref>; Barlow, <xref ref-type="bibr" rid="B13">2001</xref>). Thus, knowing the statistical dependencies between objects and scenes can help us to understand the types of compressed visual codes that allow us to rapidly recognize our visual environments.</p><p>Despite a large and growing literature on the effects of context on object and scene recognition (Palmer, <xref ref-type="bibr" rid="B77">1975</xref>; Friedman, <xref ref-type="bibr" rid="B43">1979</xref>; Biederman et al., <xref ref-type="bibr" rid="B19">1982</xref>; Boyce et al., <xref ref-type="bibr" rid="B22">1989</xref>; De Graef et al., <xref ref-type="bibr" rid="B34">1990</xref>; Henderson, <xref ref-type="bibr" rid="B51">1992</xref>; Bar and Ullman, <xref ref-type="bibr" rid="B12">1996</xref>; Hollingworth and Henderson, <xref ref-type="bibr" rid="B54">1998</xref>; Henderson et al., <xref ref-type="bibr" rid="B53">1999</xref>; Davenport and Potter, <xref ref-type="bibr" rid="B32">2004</xref>; Eckstein et al., <xref ref-type="bibr" rid="B35">2006</xref>; Neider and Zelinsky, <xref ref-type="bibr" rid="B72">2006</xref>; Auckland et al., <xref ref-type="bibr" rid="B8">2007</xref>; Becker et al., <xref ref-type="bibr" rid="B15">2007</xref>; Davenport, <xref ref-type="bibr" rid="B31a">2007</xref>; Joubert et al., <xref ref-type="bibr" rid="B57">2007</xref>; V&#x000f5; and Henderson, <xref ref-type="bibr" rid="B102">2009</xref>; Mack and Palmeri, <xref ref-type="bibr" rid="B68">2010</xref>), there has yet to be a systematic quantification of scene-object relationships in the world. This is a critical step as recent work has found that principles of attention and perception learned from artificial laboratory stimuli have limited generalizability to real-world stimuli (Neider and Zelinsky, <xref ref-type="bibr" rid="B73">2008</xref>; Wolfe et al., <xref ref-type="bibr" rid="B105">2011a</xref>,<xref ref-type="bibr" rid="B106">b</xref>). Here I seek to fill this gap by providing both descriptive statistics of contextual relations and inferential statistics to show how much these types of context can contribute to scene categorization.</p><p>Suppose you wanted to know whether object recognition benefits from lawful scene context (e.g., Davenport and Potter, <xref ref-type="bibr" rid="B32">2004</xref>). Traditionally, one would approach the problem by embedding the object of interest in normal a scene context (e.g., a &#x0201c;blender&#x0201d; in a <italic>kitchen</italic>), or an abnormal scene context (e.g., a &#x0201c;blender&#x0201d; in a <italic>bathroom</italic>), and then have human observers perform an object categorization task on both types of stimuli. Similarly, what if you wanted to study the degree to which an object evokes a particular scene context (e.g., Bar and Aminoff, <xref ref-type="bibr" rid="B11">2003</xref>). Or perhaps you are interested in how scenes are formed from diagnostic objects (e.g., MacEvoy and Epstein, <xref ref-type="bibr" rid="B67">2011</xref>). In each of these cases, how do you choose the object and scene contexts that you will use? How do we define diagnosticity for objects, and how do we measure it? Are all abnormal contexts equally bad? In each of these cases, these questions have been answered through introspection and intuition. The aim of this work is to provide baseline statistics of objects in scenes so that these types of questions can be answered with quantitative measures.</p></sec><sec><title>Theories of object context</title><p>One of the first theories of object-scene context was known as <italic>frame</italic> or <italic>schema</italic> theory (Bartlet, <xref ref-type="bibr" rid="B14">1932</xref>; Minsky, <xref ref-type="bibr" rid="B71">1975</xref>; Friedman, <xref ref-type="bibr" rid="B43">1979</xref>; Biederman, <xref ref-type="bibr" rid="B16">1981</xref>). According to this theory, scene categories can be represented in a mental structure containing learned associations between the category and objects that are commonly found in it. For example, a <italic>kitchen</italic> schema might activate representations of objects such as &#x0201c;refrigerator,&#x0201d; &#x0201c;blender,&#x0201d; and &#x0201c;cutting board.&#x0201d;</p><p>Biederman et al. (<xref ref-type="bibr" rid="B19">1982</xref>) argued that there are five object-scene relationships that constitute well-formed visual scenes. Scenes must obey the laws of physics, with objects <italic>supported</italic> by a horizontal surface, and not occupying the same physical space (<italic>interposition</italic>). Furthermore, the objects themselves have a certain <italic>likelihood</italic> of being in a particular scene context, as well as some some probable <italic>position</italic> in it. Finally, every object is constrained to have a particular <italic>size</italic> relative to the other objects in the scene. The first two relationships describe physical constraints on the world, while the last three describe the semantic content of the scene. These authors found that violations in any of these relationships resulted in reaction time and accuracy deficits for object recognition within a scene, that multiple violations made performance worse, and that both types of relations (physical and semantic) disrupted scene and object processing to similar degrees.</p><p>Much of the experimental work on scene-object context has focused on the likelihood contextual relation, often referred to as <italic>consistency</italic>. It is generally accepted that a consistent object in a scene facilitates object and scene recognition (Palmer, <xref ref-type="bibr" rid="B77">1975</xref>; Loftus and Mackworth, <xref ref-type="bibr" rid="B66">1978</xref>; Boyce et al., <xref ref-type="bibr" rid="B22">1989</xref>; De Graef et al., <xref ref-type="bibr" rid="B34">1990</xref>; Bar and Ullman, <xref ref-type="bibr" rid="B12">1996</xref>; Hollingworth and Henderson, <xref ref-type="bibr" rid="B54">1998</xref>; Davenport and Potter, <xref ref-type="bibr" rid="B32">2004</xref>; Eckstein et al., <xref ref-type="bibr" rid="B35">2006</xref>; Becker et al., <xref ref-type="bibr" rid="B15">2007</xref>; Joubert et al., <xref ref-type="bibr" rid="B57">2007</xref>; V&#x000f5; and Henderson, <xref ref-type="bibr" rid="B102">2009</xref>, <xref ref-type="bibr" rid="B103">2011</xref>; Mack and Palmeri, <xref ref-type="bibr" rid="B68">2010</xref>). However, an open debate still exists over whether this facilitation is perceptually or cognitively based (Hollingworth and Henderson, <xref ref-type="bibr" rid="B54">1998</xref>; Henderson and Hollingworth, <xref ref-type="bibr" rid="B52">1999</xref>; Bar, <xref ref-type="bibr" rid="B10">2004</xref>). The details of this argument are beyond the scope of this paper.</p><p>As there are no existing norms for object frequencies in scenes, it is often left to the intuitions of the experimenters to determine which objects are consistent or inconsistent in a scene category. Two salient exceptions include Friedman (<xref ref-type="bibr" rid="B43">1979</xref>), who obtained normative rankings by asking participants to brainstorm lists of objects that have various probabilities of being in a particular scene, and Henderson (<xref ref-type="bibr" rid="B51">1992</xref>), who provided a pilot experiment where the object-scene pairs were verified by an independent group of observers. However, in the absence of ground truth measurements of object frequency, the notion of object consistency seems to be better capturing object <italic>plausibility</italic> rather than object probability. For example, in Davenport and Potter (<xref ref-type="bibr" rid="B32">2004</xref>), a &#x0201c;sand castle&#x0201d; was chosen to be the consistent object for a <italic>beach</italic> scene. While sand castle is a very plausible object in a beach scene, most beaches are unlikely to have sand castles, making &#x0201c;sand castle&#x0201d; a plausible, but low-probability object. By measuring contextual statistics of objects and scenes, we can revisit the consistency effect with experiments reflecting real-world probabilities rather than intuitions.</p><p>There is also general agreement that context involves some form of learned associations extracted from interactions in the world. For example, in the phenomenon of contextual cueing (Chun and Jiang, <xref ref-type="bibr" rid="B31">1998</xref>), observers' reaction times in repeated visual search displays become more efficient over the course of an experiment, suggesting that they implicitly learned the spatial layout of the displays. Brockmole and Henderson (<xref ref-type="bibr" rid="B23">2006</xref>) have shown that displaying a letter search array on a real-world scene and consistently pairing the target location with a location in the scene also produces contextual cueing. Furthermore, this result generalizes across different scenes in the same category (Brockmole and V&#x000f5;, <xref ref-type="bibr" rid="B24">2010</xref>), suggesting that object-scene relationships can be implicitly extracted and used for visual search. However, there remain a number of open questions: how strong are the contextual relations between objects and scenes? Does the strength of the relations differ among different types of scenes (e.g., indoor, natural landscapes, or urban environments)? Understanding and characterizing these relationships will allow the formulation of new experiments examining the extent to which human observers use various types of context for recognition, search and memory of complex natural scenes.</p></sec><sec><title>Using context for rapid scene recognition</title><p>The mechanism behind the remarkable rapid scene categorization performance of human observers has been a long-standing mystery. How is a scene recognized as quickly as a single object when scenes contain many objects? Biederman (<xref ref-type="bibr" rid="B16">1981</xref>) outlined three paths by which an initial scene representation could be generated: (1) by recognizing a prominent object that is diagnostic of the scene's category; (2) by perceiving and recognizing global, scene-emergent features that were not defined; or (3) by recognizing and spatially integrating a few contextually related objects.</p><p>Although global, scene-specific features have been shown to be useful for scene categorization (Greene and Oliva, <xref ref-type="bibr" rid="B48">2009</xref>), observers are able to also report a few objects after a brief glance at a scene (Fei-Fei et al., <xref ref-type="bibr" rid="B38">2007</xref>). The first and third paths outlined by Biederman have been sparsely explored, as what counts as a &#x0201c;diagnostic&#x0201d; or &#x0201c;contextual&#x0201d; object is not immediately obvious. In this work, I operationalize these concepts so we may begin to test these hypotheses.</p></sec><sec><title>Scope of the current work</title><p>In this paper, I introduce a large scene database whose objects and regions have been fully labeled using the LabelMe annotation tool (Russell et al., <xref ref-type="bibr" rid="B86">2008</xref>). The fully labeled data contain names, sizes, locations, and 2D shapes for each object in each scene. In this work, I will provide descriptive statistics on these data at three levels of description: statistical ensembles, bag of words and structural. At the ensemble level, I will examine the overall object density and spatial distribution of unnamed objects and regions across the scene categories. The bag of words level of description uses the object labels to determine which objects occur in which scene categories without regard to the spatial distribution of these objects. The structural description will then examine the spatial relations among objects across scene categories. For each level of description, I will also describe how sufficient these statistics are for predicting scene categories through use of a linear classifier, and discuss how human observers may employ such strategies for rapid scene recognition.</p></sec></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec><title>Scene database</title><p>The main scene database consists of 3499 full-color scene photographs from 16 basic-level categories. Eight of the basic-level categories are indoor environments (<italic>bathroom</italic>, <italic>bedroom, conference room, corridor, dining room, kitchen, living room</italic>, and <italic>office</italic>). These images were downloaded from the web. The remaining scene categories were outdoor environments taken from Oliva and Torralba (<xref ref-type="bibr" rid="B74">2001</xref>), with four categories representing urban environments (<italic>skyscrapers, street scenes, city centers</italic>, and <italic>highways</italic>) and four categories representing natural environments (<italic>coast, open country, mountain</italic>, and <italic>forest</italic>). There were at least 94 images in each of the 16 basic-level categories. The images varied in size and were selected from a large lab database amassed from the web, personal photographs and books. See Figure <xref ref-type="fig" rid="F1">1</xref> for example images from each basic-level category.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Example images of each basic-level category</bold>. The top two rows are from indoor scene categories and the bottom two are from outdoor scene categories. The top row of outdoor scenes is from natural landscape categories and the bottom row of outdoor scenes is from urban categories.</p></caption><graphic xlink:href="fpsyg-04-00777-g0001"/></fig></sec><sec><title>Labeling procedure</title><p>The image database<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref> was hand segmented and labeled using the LabelMe Open Annotation Tool (<ext-link ext-link-type="uri" xlink:href="http://labelme.csail.mit.edu">http://labelme.csail.mit.edu</ext-link>, Russell et al., <xref ref-type="bibr" rid="B86">2008</xref>) by four observers (including the author) over the period of several months. Observers were instructed to label all regions and objects in each image and affix the best basic-level name to each region as well as to label objects as individuals, size permitting (e.g., annotate each apple in a bowl of apples except in cases where apples were too small to create an accurate bounding region). It was decided in advance that objects that could be seen through windows would not be annotated, as these objects are not located in the given scene environment. Similarly, objects whose reflections appeared in mirrors were not annotated because this would artificially inflate the count of this object in the scene. Namable parts of objects that are not separable from the object (e.g., the leg of a chair, or headlight of a car) were not labeled. For the labelers, any visual, namable entity counted as an object, so items such as &#x0201c;fog,&#x0201d; &#x0201c;cloud,&#x0201d; or &#x0201c;sand&#x0201d; were considered objects. Although one typically thinks of &#x0201c;objects&#x0201d; as discrete entities that do not comprise the structure of a scene, regions vary in their &#x0201c;objectness.&#x0201d; In order to avoid idiosyncratic labeling strategies, all regions were considered. In cases of occlusion, labelers were instructed to interpolate object boundaries as to do otherwise would increase the count of this type of object. Statistical analysis on these annotations was performed in Matlab using the LabelMe toolbox (Russell et al., <xref ref-type="bibr" rid="B86">2008</xref>).</p></sec><sec><title>Cleaning the database</title><p>As the LabelMe interface accepts any label typed by an observer, the raw annotations contained numerous typos, misspellings and synonyms. Raw labels were hand-corrected to ameliorate these issues. These changes reduced the number of unique annotations from 1767 to 617. Misspelled items accounted for 21% of the changes (for example &#x0201c;automan&#x0201d; for &#x0201c;ottoman&#x0201d;). Plurals were changed to singular, accounting for 15% of the changes. Labels that were written at the subordinate level, including descriptions of position information (&#x0201c;side view of car&#x0201d; or &#x0201c;slated wooden panel&#x0201d;) were changed to the appropriate entry-level category. These accounted for 40% of the changes. Furthermore, items listed at the superordinate level were visually inspected and assigned to the appropriate entry-level category, accounting for 3% of the changes. For example, &#x0201c;art&#x0201d; was a label that referred to a &#x0201c;painting&#x0201d; in one image and a &#x0201c;sculpture&#x0201d; in another, and &#x0201c;island&#x0201d; could refer to either a landmass in water or counter space in the center of a kitchen. In cases where the entry-level category of an object was questionable, attempts were made to group objects by function. For example, &#x0201c;decoration&#x0201d; was chosen as an entry-level as all objects under this label served a common function (e.g., &#x0201c;decorative wall hanging&#x0201d; or &#x0201c;decorative fish&#x0201d;). Object labels that were synonyms according to WordNet (Miller, <xref ref-type="bibr" rid="B70">1995</xref>) were unified under one label (for example, &#x0201c;couch&#x0201d; and &#x0201c;sofa&#x0201d;). Synonyms accounted for 16% of the changes. Labels that encompassed multiple objects (for example, &#x0201c;basket of magazines&#x0201d;) were included as the containing, or larger object only (e.g., &#x0201c;basket,&#x0201d; 2% of changes). Labels that referred to object parts that are not independent of the object whole (e.g., &#x0201c;chair leg&#x0201d; is a part that is not removable from a chair without the chair losing its function) were deleted. Parts that could refer to the whole object (e.g., &#x0201c;back of chair&#x0201d; for a chair that was occluded except for the back) were changed to the object's name. These accounted for 2% of the changes. Finally, there were 288 labels that were simply called &#x0201c;object.&#x0201d; These referred to a variety of small objects that could not be accurately identified from the small images, so the label has not been changed. There were a total of 21 deletions. The list of deletions can be found in Appendix C. A list of raw and final labels can be found in Appendices A and B, respectively.</p></sec><sec><title>Auxiliary dataset</title><p>Although ~3500 images is a relatively large database and near the practical limit of what one can hand-annotate, a critical question for the utility of these statistics is the degree to which they generalize to the population of all real-world environments. Indeed, dataset bias is known to limit the knowledge gleaned from this type of inquiry (Torralba and Efros, <xref ref-type="bibr" rid="B93">2011</xref>). In order to address this question, I compared the contextual statistics from the main database with a completely independent labeled database. As every database has independent bias, the extent to which statistics measured in one database can be successfully applied to another reflects the generalizability of the database.</p><p>I created an auxiliary set of images taken from the LabelMe database and annotated by unknown observers. The dataset consisted of 1220 images from the same 16 basic-level scene categories that had at least 85% label coverage. There were 100 images per category for <italic>bathroom</italic>, <italic>kitchen</italic>, <italic>living room</italic>, <italic>city</italic>, <italic>street</italic>, <italic>coast</italic>, and <italic>forest</italic>, and 14&#x02013;59 in the others, as LabelMe does not have a sufficient number of fully labeled scenes for the other categories. These scenes were labeled by unknown observers without the rules used by the four observers who annotated the main set. As LabelMe allows users to upload their own photographs, this dataset differs from the main dataset in that the depicted environments are less idealized and stylized and seem to come from users snapping views of their own offices, kitchens and streets (see the Figure <xref ref-type="fig" rid="FA1">A1</xref>). This set was cleaned as described above. All analyses were repeated on this additional set, and all differences between the two databases are noted in Appendix D.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec><title>General findings</title><sec><title>Quality of annotations</title><p>How much of each image was labeled? Although labelers were instructed to label each pixel, some regions were too small to label, or may have been overlooked. Here I examined the percentage of image pixels assigned to a label. On average, 85.4% of an image's pixels were assigned to a label (standard deviation: 12.7%). Sixty one percent of images had more than 90% of its pixels labeled. By contrast, only 8.9% of images in the LabelMe database have this level of annotation (Russell et al., <xref ref-type="bibr" rid="B86">2008</xref>) making the main database better suited to describing contextual statistics.</p></sec></sec><sec><title>Ensemble statistics</title><p>Ensemble statistics are statistical summaries of a group of objects, such as mean size (Ariely, <xref ref-type="bibr" rid="B6">2001</xref>; Chong and Treisman, <xref ref-type="bibr" rid="B29">2003</xref>, <xref ref-type="bibr" rid="B30">2005</xref>), center of mass (Alvarez and Oliva, <xref ref-type="bibr" rid="B4">2008</xref>), or mean orientation (Parkes et al., <xref ref-type="bibr" rid="B78">2001</xref>). Although most work in this area has been on laboratory displays of simple shapes, human observers can estimate ensemble statistics over more complicated sets of features as well, such as the average emotion of a crowd of faces (Haberman and Whitney, <xref ref-type="bibr" rid="B49">2007</xref>). Recent work in visual cognition has shown that the human visual system is adept at representing such ensembles both rapidly and outside the focus of attention (Ariely, <xref ref-type="bibr" rid="B6">2001</xref>; Chong and Treisman, <xref ref-type="bibr" rid="B29">2003</xref>, <xref ref-type="bibr" rid="B30">2005</xref>; Alvarez and Oliva, <xref ref-type="bibr" rid="B4">2008</xref>, <xref ref-type="bibr" rid="B5">2009</xref>; for a review see Alvarez, <xref ref-type="bibr" rid="B3">2011</xref>). Although the use of statistical ensembles has been posited as a potential mechanism for scene gist recognition (Haberman and Whitney, <xref ref-type="bibr" rid="B50">2012</xref>), there has been little work on what statistical ensembles might exist in real-world images.</p><p>Here, I examined several summary statistics representing the general density and location of objects in scene categories. The utility of each measure for scene categorization is assessed both individually and as a group using a linear classifier.</p><sec><title>Object density and variety</title><p>The first ensemble statistic is simply the density of labeled objects in each scene. The number of objects in a scene ranged from 1 to 88 (median: 11). Do all scene categories have a similar number of objects? To answer this question, I examined the number of objects per scene as a function of basic- and superordinate-level scene category labels. While human observers have no problems recognizing scenes with a variety of object densities (Potter, <xref ref-type="bibr" rid="B79">1976</xref>; Wolfe et al., <xref ref-type="bibr" rid="B105">2011a</xref>,<xref ref-type="bibr" rid="B106">b</xref>), classical visual search experiments show clear performance costs as the number of objects in a display increases (Biederman, <xref ref-type="bibr" rid="B18">1988</xref>; Vickery et al., <xref ref-type="bibr" rid="B101">2005</xref>). In order to better understand how the number of objects in a scene affects categorization performance in that scene, it is important to first understand how scenes vary in terms of object density.</p><p>In this database, the mean object density ranged from 5.1 objects per <italic>mountain</italic> scene to 33.1 objects per <italic>kitchen</italic> (see Table <xref ref-type="table" rid="T1">1A</xref>). As shown in Figures <xref ref-type="fig" rid="F2">2A</xref>, <xref ref-type="fig" rid="F3">3</xref>, indoor scenes had a significantly higher mean object density than outdoor scenes [23.45 and 11.44 objects per scene, respectively, <italic>t</italic><sub>(14)</sub> = 4.14, <italic>p</italic> &#x0003c; 0.01], and among the outdoor scenes, urban environments had a significantly higher average density than natural [16.43 vs. 6.46 objects per scene, respectively, <italic>t</italic><sub>(6)</sub> = 4.52, <italic>p</italic> &#x0003c; 0.01]. This indicates that the degree of human intervention in an environment results in more labeled regions and objects.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>(A) The mean number of total labeled regions per scene for each of the basic-level scene categories; (B) the mean number of uniquely labeled regions per scene</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th rowspan="1" colspan="1"/><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Kit' n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">(A)</td><td align="left" valign="top" rowspan="1" colspan="1">20.2</td><td align="left" valign="top" rowspan="1" colspan="1">18.3</td><td align="left" valign="top" rowspan="1" colspan="1">25.8</td><td align="left" valign="top" rowspan="1" colspan="1">15.6</td><td align="left" valign="top" rowspan="1" colspan="1">22.6</td><td align="left" valign="top" rowspan="1" colspan="1">33.1</td><td align="left" valign="top" rowspan="1" colspan="1">24.8</td><td align="left" valign="top" rowspan="1" colspan="1">27.1</td><td align="left" valign="top" rowspan="1" colspan="1">12.3</td><td align="left" valign="top" rowspan="1" colspan="1">20.0</td><td align="left" valign="top" rowspan="1" colspan="1">20.1</td><td align="left" valign="top" rowspan="1" colspan="1">13.3</td><td align="left" valign="top" rowspan="1" colspan="1">5.5</td><td align="left" valign="top" rowspan="1" colspan="1">7.4</td><td align="left" valign="top" rowspan="1" colspan="1">5.1</td><td align="left" valign="top" rowspan="1" colspan="1">7.8</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">(B)</td><td align="left" valign="top" rowspan="1" colspan="1">14.7</td><td align="left" valign="top" rowspan="1" colspan="1">12.5</td><td align="left" valign="top" rowspan="1" colspan="1">8.6</td><td align="left" valign="top" rowspan="1" colspan="1">7.9</td><td align="left" valign="top" rowspan="1" colspan="1">12.2</td><td align="left" valign="top" rowspan="1" colspan="1">19.5</td><td align="left" valign="top" rowspan="1" colspan="1">15.2</td><td align="left" valign="top" rowspan="1" colspan="1">15.9</td><td align="left" valign="top" rowspan="1" colspan="1">5.0</td><td align="left" valign="top" rowspan="1" colspan="1">9.0</td><td align="left" valign="top" rowspan="1" colspan="1">8.4</td><td align="left" valign="top" rowspan="1" colspan="1">7.7</td><td align="left" valign="top" rowspan="1" colspan="1">4.4</td><td align="left" valign="top" rowspan="1" colspan="1">7.7</td><td align="left" valign="top" rowspan="1" colspan="1">3.2</td><td align="left" valign="top" rowspan="1" colspan="1">4.5</td></tr></tbody></table><table-wrap-foot><p><italic>Scene category abbreviations (left to right) are: bathroom, bedroom, conference room, corridor, dining room, kitchen, living room, office, tall building (skyscraper), city, street, highway, coast, open country, mountain and forest. This convention will be followed for all tables in this article</italic>.</p></table-wrap-foot></table-wrap><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>(A)</bold> Indoor scenes had more objects on average than outdoor scenes (left). Among the outdoor scenes, urban scenes had a greater number of objects than natural (right). <bold>(B)</bold> Indoor scenes had a greater number of unique labels in each scene than outdoor. Among outdoor categories, urban scenes had more unique objects than natural scenes. Error bars reflect &#x000b1; 1 SEM.</p></caption><graphic xlink:href="fpsyg-04-00777-g0002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Object density as a function of superordinate-level scene category (indoor, urban, and natural)</bold>. Image examples of the most and least dense images are shown for illustration.</p></caption><graphic xlink:href="fpsyg-04-00777-g0003"/></fig><p>I also examined the number of unique objects in each scene: the larger this number, the greater the heterogeneity and possibly the complexity of the scene. In the database, the number of unique items in a scene ranged from 1 to 42 (median: 6). The number of unique regions in a scene varied from 19.5 in a <italic>kitchen</italic>, to 3.2 in <italic>mountain</italic> scenes (see Table <xref ref-type="table" rid="T1">1B</xref>). As with total object density, there were more unique objects in indoor scenes when compared with outdoor scenes [13.3 and 5.7 unique items per scene, <italic>t</italic><sub>(14)</sub> = 4.78, <italic>p</italic> &#x0003c; 0.001], and among outdoor scene categories, more unique objects in urban scenes compared with natural scenes [7.55 and 3.93 unique items per scene, <italic>t</italic><sub>(6)</sub> = 3.76, <italic>p</italic> &#x0003c; 0.01, see Figure <xref ref-type="fig" rid="F2">2B</xref>). Manufactured environments therefore have both a greater number and greater variety of objects than natural environments.</p></sec><sec><title>Mean and variance of object size</title><p>Human observers are able to quickly and accurately compute the mean size of objects in laboratory displays (Ariely, <xref ref-type="bibr" rid="B6">2001</xref>; Chong and Treisman, <xref ref-type="bibr" rid="B29">2003</xref>, <xref ref-type="bibr" rid="B30">2005</xref>). Are statistical properties of object size diagnostic of scene category? Although object size and density are related, it is important to consider that the two-dimensional labeling of a three-dimensional scene results in overlapping polygons. For example, a pillow on a bed will overlap with the bed, or a chair in front of a table with overlap with the table. Thus, mean object size is not trivially the inverse of object density.</p><p>For each scene, the size of each object was expressed as percent of total image area. In general, labeled regions were relatively small (the median mean object size was 17% of the total image area). There was considerable range in mean object size in our database, from a minuscule 0.05% of image area to a nearly all-encompassing 99.4%. Among basic-level categories, <italic>living rooms</italic> had the smallest mean object size (5% of image area) and <italic>mountains</italic> had the largest (43%), see Table <xref ref-type="table" rid="T2">2</xref>. Predictably, indoor scenes had a smaller mean object size compared to outdoor scenes [7.5 vs. 24.4%, <italic>t</italic><sub>(14)</sub> = 4.94, <italic>p</italic> &#x0003c; 0.001]. Among the outdoor superordinate-level categories, natural scenes trended toward having a larger mean object size compared to urban [30.2 and 18.6%, respectively, <italic>t</italic><sub>(6)</sub> = 2.28, <italic>p</italic> = 0.063].</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Mean (top) and standard deviation (bottom) of mean object size in percentage of total image area</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Ki&#x00130;n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mn tn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">5.7</td><td align="left" valign="top" rowspan="1" colspan="1">6.9</td><td align="left" valign="top" rowspan="1" colspan="1">10.3</td><td align="left" valign="top" rowspan="1" colspan="1">14.0</td><td align="left" valign="top" rowspan="1" colspan="1">7.1</td><td align="left" valign="top" rowspan="1" colspan="1">5.4</td><td align="left" valign="top" rowspan="1" colspan="1">5.0</td><td align="left" valign="top" rowspan="1" colspan="1">5.3</td><td align="left" valign="top" rowspan="1" colspan="1">24.9</td><td align="left" valign="top" rowspan="1" colspan="1">17.4</td><td align="left" valign="top" rowspan="1" colspan="1">20.0</td><td align="left" valign="top" rowspan="1" colspan="1">12.0</td><td align="left" valign="top" rowspan="1" colspan="1">23.9</td><td align="left" valign="top" rowspan="1" colspan="1">25.2</td><td align="left" valign="top" rowspan="1" colspan="1">42.9</td><td align="left" valign="top" rowspan="1" colspan="1">29.0</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">2.7</td><td align="left" valign="top" rowspan="1" colspan="1">3.3</td><td align="left" valign="top" rowspan="1" colspan="1">5.5</td><td align="left" valign="top" rowspan="1" colspan="1">7.4</td><td align="left" valign="top" rowspan="1" colspan="1">3.8</td><td align="left" valign="top" rowspan="1" colspan="1">3.7</td><td align="left" valign="top" rowspan="1" colspan="1">2.1</td><td align="left" valign="top" rowspan="1" colspan="1">2.7</td><td align="left" valign="top" rowspan="1" colspan="1">15.4</td><td align="left" valign="top" rowspan="1" colspan="1">11.6</td><td align="left" valign="top" rowspan="1" colspan="1">9.5</td><td align="left" valign="top" rowspan="1" colspan="1">6.5</td><td align="left" valign="top" rowspan="1" colspan="1">11.6</td><td align="left" valign="top" rowspan="1" colspan="1">12.1</td><td align="left" valign="top" rowspan="1" colspan="1">15.3</td><td align="left" valign="top" rowspan="1" colspan="1">22.4</td></tr></tbody></table></table-wrap><p>Next, I examined object size variance across basic- and superordinate-level scene categories. For each scene, the size variance of all objects in the scene was computed. For each basic-level category, I computed the mean of object size variance, finding that <italic>living rooms</italic> had the smallest variance of object size, and <italic>mountains</italic> had the largest. Overall, indoor scenes had smaller variance of mean object size compared to outdoor [<italic>t</italic><sub>(14)</sub> = 5.84, <italic>p</italic> &#x0003c; 0.001], but no reliable difference was found between natural and urban scenes [<italic>t</italic><sub>(6)</sub> = 1.07, n.s.].</p></sec><sec><title>Center of mass</title><p>The previous ensemble statistics have shown us that, relative to outdoor environments, indoor scenes have a higher density of objects, and lower variance of object size. However, these do not tell us anything about where these objects are located in the scene. Previous work has shown that human observers are sensitive to the center of mass of a group of objects and can accurately compute this location even when attention is diverted elsewhere (Alvarez and Oliva, <xref ref-type="bibr" rid="B4">2008</xref>). Are there robust differences in the locations of objects in different basic- and superordinate-level scene categories?</p><p>For each scene, the center of each object was computed as (xMax-xMin, yMax-yMin) of the polygon vertices. The center of mass for the scene was then computed as the mean of these values, weighted by the size of the object (as computed above). As expected, there was a strong tendency for the objects to center along the vertical axis (basic-level category centroids were located between 46 and 53% of total horizontal extent), indicating that objects were located with equal probabilities in the left and right sides of a scene. I observed a certain degree of diversity in position in the vertical axis, with basic-level category centroids occupying 35&#x02013;75% of the vertical axis. This makes sense, as vertical location is a possible cue for scene depth. In particular, outdoor environments had a higher center of mass in the image plane (65% of vertical axis) than indoor environments [47% of vertical axis, <italic>t</italic><sub>(14)</sub> = 4.09, <italic>p</italic> &#x0003c; 0.01], reflecting the presence of objects such as skyscrapers, buildings and sky. However, no systematic difference was found between the natural and urban outdoor scenes [<italic>t</italic><sub>(6)</sub> &#x0003c; 1, n.s.]. Therefore, vertical center of object mass may contain diagnostic information for scene category.</p></sec><sec><title>Object spacing regularity</title><p>The center of object mass tells us about the general location of objects in an image, but this statistic does not tell us about the spacing of these objects. Objects that cluster together can be perceptually grouped (Gestalt law of proximity), and may have functional relations in a scene. Do scene categories vary in their object spacing regularity?</p><p>For each scene, pairwise distances between each of the objects in the scene were computed, using the (x,y) locations of each object's center of mass. Then for each scene, I computed the variability of object spacing as the standard deviation of distances, normalized by the mean distance between each object. Normalizing by the mean allows us to compare images that were not the same size. A low degree of variability indicates a very regular, grid-like spacing of objects while a high degree of variability suggests a more clustered spatial layout. While basic-level categories varied in their degrees of spacing variability, no systematic differences were found between indoor and outdoor scenes [t<sub>(14)</sub> = 1.04, n.s.] nor between the natural and urban outdoor scenes [<italic>t</italic><sub>(6)</sub> = 1.12, n.s.]. This seems to be partially due to the fact that indoor scene categories were themselves quite variable: <italic>bathroom</italic> scenes displayed the highest degree of object spacing regularity of all 16 categories while <italic>living rooms</italic> displayed the lowest.</p></sec><sec><title>Scene classification with an ensemble statistics model</title><p>To what extent do these ensemble statistics provide information about the basic- and superordinate-level scene categories? To examine this question, I expressed each image in the database according to its object density, unique object density, mean object size, object size variance, center of mass and variability of object spacing. Using a support vector machine (SVM) classifier [linear kernel, using LIBSVM, Chang and Lin (<xref ref-type="bibr" rid="B27">2011</xref>)], I tested basic- and superordinate-level scene categorization. Each image was separately used as a test image after training with the remaining 3498 images in the database. This procedure was the same for all SVM analyses in this manuscript. LIBSVM uses a one-against-one multi-class classification, with all parameters remaining the same for each classification task. All default parameters for LIBSVM were employed. For the superordinate-level categorization task, the classifier achieved an accuracy of 91% correct for natural, 63.4% for urban and 76.5% for indoor scenes (overall AUC: 0.83). This overall level of performance is well above the chance level of 33% (binomial test, all <italic>p</italic> &#x0003c; 0.0001).</p><p>For the basic-level categorization task, mean performance was 61% correct (AUC = 0.77), well above the chance level of 6.25%. Performance on each basic-level category ranged from 6% for <italic>offices</italic> to 81% for <italic>living rooms</italic>. Binomial tests on the performance of each basic-level category indicated that all categories except for <italic>office</italic> were classified above chance (<italic>p</italic> &#x0003c; 0.01). Basic-level classification performance did not differ significantly between outdoor and indoor scene categories [65 and 47% correct, respectively, <italic>t</italic><sub>(14)</sub> = 1.7, <italic>p</italic> = 0.10], nor did basic-level categorization performance differ among natural and urban scene categories [73 and 55% correct, respectively, <italic>t</italic><sub>(6)</sub> = 1.8, <italic>p</italic> = 0.12]. Incorrectly classified <italic>offices</italic> were frequently classified as <italic>living rooms</italic> (67% of mistakes), <italic>kitchens</italic> (13%), or <italic>bathrooms</italic> (10%), see Figure <xref ref-type="fig" rid="F4">4</xref> for full confusion matrix.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Confusion matrix for linear SVM classifier representing ensemble statistics from 16 scene categories</bold>. Light colors along diagonal show correct classifications while light colors on the off-diagonals represent misclassifications. Data from main database using SVM with linear kernel using leave-one-out cross validation.</p></caption><graphic xlink:href="fpsyg-04-00777-g0004"/></fig><p>What was the nature of the misclassifications? Twenty seven percent of misclassified scenes were misclassified across the indoor-outdoor superordinate distinction (i.e., a scene was indoor and was classified as one of the outdoor categories). This pattern is unlike human scene categorization, where mistakes are nearly always within the same superordinate-level category (Renninger and Malik, <xref ref-type="bibr" rid="B83">2004</xref>; Kadar and Ben-Shahar, <xref ref-type="bibr" rid="B58">2012</xref>). Although this classifier has remarkably high performance given its simplicity, the pattern of performance suggests that human observers use different or additional information for performing rapid scene categorization.</p><p>How does each of the ensemble statistics contribute to classification performance? To address this question, I performed the same SVM analysis as described above, using only one ensemble statistic at a time. Basic-level categorization performance was above chance for each of the ensemble statistics (binomial test, all <italic>p</italic> &#x0003c; 0.001), and ranged from 10.2% for center of mass to 31.6% for spacing regularity. A one-way ANOVA on the accuracy of each classifier revealed significant differences in performance (<italic>p</italic> &#x0003c; 0.001), suggesting that certain ensemble statistics are more useful than others for categorization. As shown in section <bold>Object Spacing Regularity</bold>, the regularity of object spacing did not differ reliably among superordinate level scene categories, even though it has the highest basic-level categorization performance when tested alone, indicating that this feature carries information about a scene's basic-level category, but not superordinate-level category.</p><p>In order to understand how the dimensionality of these features affects classification performance, I ran classifiers trained and tested on each combination of 2&#x02013;6 ensemble statistics. Classification performance grew linearly in this range (slope: 8.3% per feature, <italic>r</italic><sup>2</sup> = 0.98). Extrapolating, ceiling performance could be expected with 11 ensemble features.</p><p>How well do ensemble statistics from the main database generalize to the auxiliary database? I trained a linear SVM on ensemble statistics from the main database, and tested categorization performance on the auxiliary database. Above-chance performance of this classifier indicates shared information because bias between the two databases should not be strongly correlated. Indeed, basic-level categorization performance for a model trained on the main database and tested on the auxiliary set was 17% (AUC = 0.52), significantly above chance level (binomial test, <italic>p</italic> &#x0003c; 0.001), indicating that ensemble statistics measured from one database contain information about the pattern of ensemble statistics in an independent database.</p><p>How does classifier performance compare to human performance on a rapid scene categorization task? Here, I compared the classifier to the human categorization data of Kadar and Ben-Shahar (<xref ref-type="bibr" rid="B58">2012</xref>) who tested observers on 12 of the 16 categories in the current database. In their experiment, participants were briefly shown two images and were then asked to determine whether the images were in the same category. Images were presented for 27&#x02013;1000 ms and followed by a 1/f noise mask. The authors published confusion matrices for the scene categories averaged over presentation time. Overall, sensitivity of the ensemble statistics classifier was lower than that of the human observers [mean A' = 0.65 for classifier, 0.85 for participants <italic>t</italic><sub>(22)</sub> = 7.7, <italic>p</italic> &#x0003c; 0.0001]. When comparing the confusion matrices of the classifier to those of the human observers, I found that although the patterns of classifier confusion were not well correlated with human error patterns at the basic-level (<italic>r</italic> = 0.04), error patterns were quite similar when averaged over superordinate-level categories (<italic>r</italic> = 0.79). Therefore, the ensemble classifier can predict human performance at rapid scene categorization at a coarse level, adding support for the plausibility of such a coding scheme as a mechanism for scene gist perception.</p><p>Together, these analyses show that simple ensemble statistics, such as the number and location of nameless objects, are sufficient for above-chance scene categorization at both the basic and superordinate levels, and that the pattern of performance mimics human categorization performance at a coarse level.</p></sec><sec><title>Ensemble statistics discussion</title><p>In this section, I have described real-world images in terms of very simple statistics that express the quantity and coarse spatial distributions of &#x0201c;things&#x0201d; in a scene. These are of interest because they are rapidly computed by human observers on laboratory displays (Parkes et al., <xref ref-type="bibr" rid="B78">2001</xref>; Chong and Treisman, <xref ref-type="bibr" rid="B30">2005</xref>; Haberman and Whitney, <xref ref-type="bibr" rid="B49">2007</xref>) and may explain aspects visual representations outside the fovea (Balas et al., <xref ref-type="bibr" rid="B9">2009</xref>) or outside the focus of attention (Alvarez and Oliva, <xref ref-type="bibr" rid="B4">2008</xref>, <xref ref-type="bibr" rid="B5">2009</xref>).</p><p>Descriptively, these results show that statistical ensembles vary considerably with the degree of human manufacturing of an environment. In particular, indoor scenes have more total objects, a greater variety of objects, and a smaller average object size when compared to outdoor scenes. This same trend holds for urban scenes when compared to natural scenes, as urban scenes have a higher degree of manufacture. Spatially, indoor scenes had a lower center of mass compared to outdoor scenes. There are two reasons for this. Outdoor scenes have more objects further off the ground than indoor scenes (&#x0201c;sky,&#x0201d; &#x0201c;cloud,&#x0201d; &#x0201c;skyscraper,&#x0201d; &#x0201c;bird,&#x0201d; &#x0201c;tree&#x0201d;). Also, outdoor scenes also have a larger mean depth than indoor scenes (Torralba and Oliva, <xref ref-type="bibr" rid="B94">2002</xref>). Objects receding in depth tend to be located higher in the x-y image plane, leading to a higher center of mass for outdoor scenes. Therefore, although scenes are treated as a single class in the literature, this result suggests that scenes are a heterogeneous set of entities, leaving open the possibility that different environments may be processed differently by the visual system.</p><p>Through the use of a linear classifier, I have shown that such simple statistics carry sufficient information to categorize a scene at both the basic- and superordinate-levels significantly above chance, demonstrating for the first time that ensemble statistics could be a plausible mechanism for scene gist recognition in human observers. Although this classifier had lower performance than the human observers from Kadar and Ben-Shahar (<xref ref-type="bibr" rid="B58">2012</xref>), the patterns of errors made by this model were similar to those made by the human observers when averaged over superordinate-level categories, suggesting that human observers may build an initial scene representation using ensemble-like features. Of course, the majority of work on ensemble statistics has been on very sparse laboratory displays. It remains to be seen whether observers can accurately report statistical information from complex, real-world images.</p></sec></sec><sec><title>Bag of words models</title><p>The statistical ensemble model considered all annotations to be nameless &#x0201c;things.&#x0201d; However, the identity of these &#x0201c;things&#x0201d; is critical to scene identity. In linguistics, models that consider statistical patterns of word use independent of syntactical relations (so-called &#x0201c;bag of words&#x0201d; models) have been successful in document classification and spam detection (Deerwester et al., <xref ref-type="bibr" rid="B33">1990</xref>; Blei et al., <xref ref-type="bibr" rid="B20">2003</xref>). In computer vision, growing bodies of models perform similar operations on visual &#x0201c;words&#x0201d; given by interest-point or object detectors (Sivic and Zisserman, <xref ref-type="bibr" rid="B90a">2003</xref>; Fei-Fei and Perona, <xref ref-type="bibr" rid="B39">2005</xref>). Visual bag of words models have been very successful for scene classification in recent years (Bosch et al., <xref ref-type="bibr" rid="B21">2006</xref>; Lazebnik et al., <xref ref-type="bibr" rid="B62">2006</xref>; Li et al., <xref ref-type="bibr" rid="B63">2010</xref>).</p><p>In the model considered here, a scene is represented as a list of the objects contained in it. Measures such as object frequency (overall as well as conditioned on scene category) and mutual information between objects and scenes will be employed while still ignoring the spatial relations existing between these objects and regions. As before, I will examine the fidelity of a bag of words model for predicting basic- and superordinate-level scene categories through the use of a linear classifier, and evaluate proposed schemes by which human rapid scene categorization might occur via object recognition.</p><sec><title>Overall object frequency</title><p>Which objects are most common in the world? Just as certain words are more common than others in written text (&#x0201c;the&#x0201d; is more common than &#x0201c;aardvark&#x0201d;), certain objects appear in the world with greater frequency than others. Each of the 617 uniquely labeled regions in the database appeared between 1 and 3994 times in 1&#x02013;2312 images. Nearly a quarter of the labels (22.7%) appeared only once in the database while eight objects (0.23%) appeared more than 1000 times. Overall, the frequency of objects in the database is inversely proportional to the frequency rank of the object, a relationship known in the linguistics literature as Zipf's law (Li, <xref ref-type="bibr" rid="B64">1992</xref>; see Figure <xref ref-type="fig" rid="F5">5</xref>).</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Object frequency is inversely proportional to frequency rank</bold>. This pattern does not strongly depend on how the database was cleaned.</p></caption><graphic xlink:href="fpsyg-04-00777-g0005"/></fig><p>The 10 most common objects are listed in Table <xref ref-type="table" rid="T3">3</xref> where I list both the total counts for an object (right column), and the number of scenes that contain at least one instance of that object (middle column). It should be noted that these counts represent a lower bound for the number of objects in the scenes. In scenes where several exemplars of a small object were grouped together, but too small to individuate (e.g., &#x0201c;apples&#x0201d; in a &#x0201c;bowl,&#x0201d; or &#x0201c;books&#x0201d; on a &#x0201c;shelf&#x0201d;), it was typical for annotators to list these as a group using the plural.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p><bold>The 10 most common objects in the database</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Object name</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Nb scenes (%)</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Total counts</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">2312 (66)</td><td align="left" valign="top" rowspan="1" colspan="1">2393</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">1377 (39)</td><td align="left" valign="top" rowspan="1" colspan="1">3680</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">1139 (33)</td><td align="left" valign="top" rowspan="1" colspan="1">3994</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">963 (28)</td><td align="left" valign="top" rowspan="1" colspan="1">1615</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">871 (25)</td><td align="left" valign="top" rowspan="1" colspan="1">1064</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">821 (23)</td><td align="left" valign="top" rowspan="1" colspan="1">2981</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">635 (18)</td><td align="left" valign="top" rowspan="1" colspan="1">2943</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">488 (14)</td><td align="left" valign="top" rowspan="1" colspan="1">901</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">484 (14)</td><td align="left" valign="top" rowspan="1" colspan="1">513</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">465 (13)</td><td align="left" valign="top" rowspan="1" colspan="1">859</td></tr></tbody></table><table-wrap-foot><p><italic>The middle column shows the number of scenes containing at least one exemplar of this object. The percentage of scenes with this object is shown in parentheses. The right column shows the total number of these objects in the database</italic>.</p></table-wrap-foot></table-wrap></sec><sec><title>Object frequency</title><p>What are the most frequent objects in each scene category? Knowing object frequency will allow us to find out how sensitive human observers are to these frequencies, and thus better understand the role of expectation in scene perception.</p><p>Table <xref ref-type="table" rid="T4">4</xref> shows the 10 most frequent objects in each basic-level scene category. It is of note that there are relatively large differences between basic-level scene categories in terms of the frequency of the most typical objects: while &#x0201c;sofa&#x0201d; is an intuitively important object for <italic>living rooms</italic>, it was present in only 86% of living room scenes, while &#x0201c;faucet&#x0201d; was labeled in over 99% of <italic>bathrooms</italic>.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p><bold>The 10 most frequent objects in each basic-level category along with the proportion of scenes in each category that contain at least one exemplar of that object</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Kit' &#x00130;n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.90</td><td align="left" valign="top" rowspan="1" colspan="1">0.88</td><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.98</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.86</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.79</td><td align="left" valign="top" rowspan="1" colspan="1">0.81</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">0.83</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.51</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.77</td><td align="left" valign="top" rowspan="1" colspan="1">0.76</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">0.91</td><td align="left" valign="top" rowspan="1" colspan="1">0.70</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.80</td><td align="left" valign="top" rowspan="1" colspan="1">0.76</td><td align="left" valign="top" rowspan="1" colspan="1">0.80</td><td align="left" valign="top" rowspan="1" colspan="1">0.72</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.78</td><td align="left" valign="top" rowspan="1" colspan="1">0.44</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">0.30</td><td align="left" valign="top" rowspan="1" colspan="1">0.31</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.69</td><td align="left" valign="top" rowspan="1" colspan="1">0.73</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">0.76</td><td align="left" valign="top" rowspan="1" colspan="1">0.66</td><td align="left" valign="top" rowspan="1" colspan="1">0.63</td><td align="left" valign="top" rowspan="1" colspan="1">0.68</td><td align="left" valign="top" rowspan="1" colspan="1">0.70</td><td align="left" valign="top" rowspan="1" colspan="1">0.52</td><td align="left" valign="top" rowspan="1" colspan="1">064</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.77</td><td align="left" valign="top" rowspan="1" colspan="1">0.42</td><td align="left" valign="top" rowspan="1" colspan="1">0.51</td><td align="left" valign="top" rowspan="1" colspan="1">0.09</td><td align="left" valign="top" rowspan="1" colspan="1">0.25</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.68</td><td align="left" valign="top" rowspan="1" colspan="1">0.70</td><td align="left" valign="top" rowspan="1" colspan="1">0.43</td><td align="left" valign="top" rowspan="1" colspan="1">0.49</td><td align="left" valign="top" rowspan="1" colspan="1">0.56</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.22</td><td align="left" valign="top" rowspan="1" colspan="1">0.58</td><td align="left" valign="top" rowspan="1" colspan="1">0.77</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">0.33</td><td align="left" valign="top" rowspan="1" colspan="1">0.24</td><td align="left" valign="top" rowspan="1" colspan="1">0.09</td><td align="left" valign="top" rowspan="1" colspan="1">0.23</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Book</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Fence</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Hill</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Grass</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.63</td><td align="left" valign="top" rowspan="1" colspan="1">0.64</td><td align="left" valign="top" rowspan="1" colspan="1">0.35</td><td align="left" valign="top" rowspan="1" colspan="1">0.29</td><td align="left" valign="top" rowspan="1" colspan="1">0.55</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.64</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.19</td><td align="left" valign="top" rowspan="1" colspan="1">0.52</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">0.36</td><td align="left" valign="top" rowspan="1" colspan="1">0.17</td><td align="left" valign="top" rowspan="1" colspan="1">0.22</td><td align="left" valign="top" rowspan="1" colspan="1">0.08</td><td align="left" valign="top" rowspan="1" colspan="1">0.17</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toilet</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Column</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Ground</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.60</td><td align="left" valign="top" rowspan="1" colspan="1">0.64</td><td align="left" valign="top" rowspan="1" colspan="1">0.33</td><td align="left" valign="top" rowspan="1" colspan="1">0.21</td><td align="left" valign="top" rowspan="1" colspan="1">0.53</td><td align="left" valign="top" rowspan="1" colspan="1">0.60</td><td align="left" valign="top" rowspan="1" colspan="1">0.64</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.12</td><td align="left" valign="top" rowspan="1" colspan="1">0.37</td><td align="left" valign="top" rowspan="1" colspan="1">0.45</td><td align="left" valign="top" rowspan="1" colspan="1">0.34</td><td align="left" valign="top" rowspan="1" colspan="1">0.15</td><td align="left" valign="top" rowspan="1" colspan="1">0.17</td><td align="left" valign="top" rowspan="1" colspan="1">0.07</td><td align="left" valign="top" rowspan="1" colspan="1">0.14</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Nightstand</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Paper</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Van</td><td align="left" valign="top" rowspan="1" colspan="1">Median</td><td align="left" valign="top" rowspan="1" colspan="1">Sun</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td><td align="left" valign="top" rowspan="1" colspan="1">Ground</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.59</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.30</td><td align="left" valign="top" rowspan="1" colspan="1">0.21</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.55</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.11</td><td align="left" valign="top" rowspan="1" colspan="1">0.32</td><td align="left" valign="top" rowspan="1" colspan="1">0.27</td><td align="left" valign="top" rowspan="1" colspan="1">0.33</td><td align="left" valign="top" rowspan="1" colspan="1">0.14</td><td align="left" valign="top" rowspan="1" colspan="1">0.14</td><td align="left" valign="top" rowspan="1" colspan="1">0.06</td><td align="left" valign="top" rowspan="1" colspan="1">0.13</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bottle</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Armchair</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Streetlight</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Streetlight</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Path</td><td align="left" valign="top" rowspan="1" colspan="1">Mtn pass</td><td align="left" valign="top" rowspan="1" colspan="1">Path</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.45</td><td align="left" valign="top" rowspan="1" colspan="1">0.40</td><td align="left" valign="top" rowspan="1" colspan="1">0.27</td><td align="left" valign="top" rowspan="1" colspan="1">0.15</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.55</td><td align="left" valign="top" rowspan="1" colspan="1">0.51</td><td align="left" valign="top" rowspan="1" colspan="1">0.38</td><td align="left" valign="top" rowspan="1" colspan="1">0.09</td><td align="left" valign="top" rowspan="1" colspan="1">0.31</td><td align="left" valign="top" rowspan="1" colspan="1">0.25</td><td align="left" valign="top" rowspan="1" colspan="1">0.32</td><td align="left" valign="top" rowspan="1" colspan="1">0.14</td><td align="left" valign="top" rowspan="1" colspan="1">0.10</td><td align="left" valign="top" rowspan="1" colspan="1">0.05</td><td align="left" valign="top" rowspan="1" colspan="1">0.13</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Dresser</td><td align="left" valign="top" rowspan="1" colspan="1">Pr.</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Bowl</td><td align="left" valign="top" rowspan="1" colspan="1">Rug</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Antenna</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Streetlight</td><td align="left" valign="top" rowspan="1" colspan="1">Cloud</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Land</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.43</td><td align="left" valign="top" rowspan="1" colspan="1">0.39</td><td align="left" valign="top" rowspan="1" colspan="1">screen</td><td align="left" valign="top" rowspan="1" colspan="1">0.13</td><td align="left" valign="top" rowspan="1" colspan="1">0.36</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">0.47</td><td align="left" valign="top" rowspan="1" colspan="1">0.37</td><td align="left" valign="top" rowspan="1" colspan="1">0.08</td><td align="left" valign="top" rowspan="1" colspan="1">0.25</td><td align="left" valign="top" rowspan="1" colspan="1">0.23</td><td align="left" valign="top" rowspan="1" colspan="1">0.28</td><td align="left" valign="top" rowspan="1" colspan="1">0.11</td><td align="left" valign="top" rowspan="1" colspan="1">0.10</td><td align="left" valign="top" rowspan="1" colspan="1">0.05</td><td align="left" valign="top" rowspan="1" colspan="1">0.12</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">0.19</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr></tbody></table></table-wrap><p>What is the overall frequency-rank relationship for each of the 16 scene categories? For each basic-level scene category, I computed the number of objects that were present in at least half of the images. Indoor scenes had a greater number of frequent objects compared to outdoor scenes [7.1 vs. 3.9 objects, <italic>t</italic><sub>(14)</sub> = 3.1, <italic>p</italic> &#x0003c; 0.01]. Among the outdoor scenes, urban scenes had a greater number of frequent objects compared to natural [5.3 vs. 2.5, <italic>t</italic><sub>(6)</sub> = 4.0, <italic>p</italic> &#x0003c; 0.01]. Again, this pattern shows that the degree of human manufacture affects the distribution of object frequencies. To probe at a finer level of detail, I computed the number of objects at frequency levels between 0.1 and 0.9. Figure <xref ref-type="fig" rid="F6">6</xref> shows the average of outdoor and indoor scenes (top, A) and the average of natural and urban scenes (bottom, B). <italic>T</italic>-tests performed at each threshold level showed that no statistical difference exists between the number of objects in outdoor and indoor scenes for frequency thresholds above 0.5 (Bonferroni corrected), suggesting that although indoor scenes have more objects than outdoor scenes, all scenes have similar numbers of very frequent objects. Among the outdoor scene categories, natural and urban scenes did not reliably differ.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Object frequency at various thresholds. (A)</bold> Indoor scenes have more frequent objects than outdoor scenes at frequencies &#x02264;0.5 (left). <bold>(B)</bold> Urban scene categories trend toward having more frequent objects than natural scene categories.</p></caption><graphic xlink:href="fpsyg-04-00777-g0006"/></fig></sec><sec><title>Object diagnosticity</title><p>How important is an object to scene identity? Important objects may frequently occur in scenes, but not all frequent objects provide information about scene category. For example, some objects, such as &#x0201c;tree&#x0201d; can occur in many environments, while other objects such as &#x0201c;toilet&#x0201d; can only occur in a specific context, such as a <italic>bathroom</italic>. To formalize this notion, I introduce <italic>diagnosticity</italic>, which is the probability of a scene belonging to a particular scene category conditioned on the presence of a particular object [p(scene|object)]. Although &#x0201c;chair&#x0201d; is a frequent object in <italic>dining rooms</italic>, chairs are not diagnostic of dining rooms because they are also found in <italic>bedrooms, conference rooms, offices</italic>, etc. Similarly, there may be objects that are diagnostic that are not frequent, and these might reflect object-scene pairs that have been used in the object consistency literature (recall the example of the &#x0201c;sand castle&#x0201d; on the <italic>beach</italic>). This measure is of particular interest as some models of human rapid scene categorization posit that categorization can be mediated through the recognition of one or more diagnostic objects (Friedman, <xref ref-type="bibr" rid="B43">1979</xref>; Biederman, <xref ref-type="bibr" rid="B16">1981</xref>).</p><p>Diagnosticity was measured for every object and scene category in the database. However, this metric over-represents rare objects. As nearly one quarter of labeled objects occurred only once in the database, all of these objects have full diagnosticity for the scene category they were found in. However, because they are rare, these objects may not be informative. Therefore, I am reporting the diagnosticity of objects with at least 10 instances in the database. The most diagnostic objects for each scene category are listed in Table <xref ref-type="table" rid="T5">5</xref>.</p><table-wrap id="T5" position="float"><label>Table 5</label><caption><p><bold>The 10 most diagnostic objects for each basic-level scene category</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Ki&#x00130;n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Str</bold><italic>t</italic>.</th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath mat</td><td align="left" valign="top" rowspan="1" colspan="1">Bedspread</td><td align="left" valign="top" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" rowspan="1" colspan="1">Exit sign</td><td align="left" valign="top" rowspan="1" colspan="1">Buffet</td><td align="left" valign="top" rowspan="1" colspan="1">Cttngbrd</td><td align="left" valign="top" rowspan="1" colspan="1">Coffee table</td><td align="left" valign="top" rowspan="1" colspan="1">Desk mat</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Attic</td><td align="left" valign="top" rowspan="1" colspan="1">Crosswalk</td><td align="left" valign="top" rowspan="1" colspan="1">Median</td><td align="left" valign="top" rowspan="1" colspan="1">Seagull</td><td align="left" valign="top" rowspan="1" colspan="1">Cow</td><td align="left" valign="top" rowspan="1" colspan="1">Mtn pass</td><td align="left" valign="top" rowspan="1" colspan="1">Waterfall</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.90</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.97</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.92</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Shower</td><td align="left" valign="top" rowspan="1" colspan="1">Headboard</td><td align="left" valign="top" rowspan="1" colspan="1">Podium</td><td align="left" valign="top" rowspan="1" colspan="1">Arch</td><td align="left" valign="top" rowspan="1" colspan="1">Silverware</td><td align="left" valign="top" rowspan="1" colspan="1">Dish towel</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">File</td><td align="left" valign="top" rowspan="1" colspan="1">Dock</td><td align="left" valign="top" rowspan="1" colspan="1">Porch</td><td align="left" valign="top" rowspan="1" colspan="1">Curb</td><td align="left" valign="top" rowspan="1" colspan="1">Cabin</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Desert</td><td align="left" valign="top" rowspan="1" colspan="1">Fog</td><td align="left" valign="top" rowspan="1" colspan="1">Branch</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.84</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.78</td><td align="left" valign="top" rowspan="1" colspan="1">orgnzr</td><td align="left" valign="top" rowspan="1" colspan="1">0.92</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.68</td><td align="left" valign="top" rowspan="1" colspan="1">0.83</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Shwr crtn</td><td align="left" valign="top" rowspan="1" colspan="1">Nightstand</td><td align="left" valign="top" rowspan="1" colspan="1">Prjctn. scm</td><td align="left" valign="top" rowspan="1" colspan="1">Fire alarm</td><td align="left" valign="top" rowspan="1" colspan="1">Wine glass</td><td align="left" valign="top" rowspan="1" colspan="1">Kettle</td><td align="left" valign="top" rowspan="1" colspan="1">Ottoman</td><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td align="left" valign="top" rowspan="1" colspan="1">Antenna</td><td align="left" valign="top" rowspan="1" colspan="1">Spotlight</td><td align="left" valign="top" rowspan="1" colspan="1">Motorcycle</td><td align="left" valign="top" rowspan="1" colspan="1">Slope</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Goose</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">Stick</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.84</td><td align="left" valign="top" rowspan="1" colspan="1">0.78</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.72</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.90</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.66</td><td align="left" valign="top" rowspan="1" colspan="1">0.65</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.88</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Soap dish</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Whiteboard</td><td align="left" valign="top" rowspan="1" colspan="1">Blltn board</td><td align="left" valign="top" rowspan="1" colspan="1">Napkin</td><td align="left" valign="top" rowspan="1" colspan="1">Oven</td><td align="left" valign="top" rowspan="1" colspan="1">Television</td><td align="left" valign="top" rowspan="1" colspan="1">Mouse</td><td align="left" valign="top" rowspan="1" colspan="1">Mast</td><td align="left" valign="top" rowspan="1" colspan="1">Terrace</td><td align="left" valign="top" rowspan="1" colspan="1">Van</td><td align="left" valign="top" rowspan="1" colspan="1">Fence</td><td align="left" valign="top" rowspan="1" colspan="1">Lighthouse</td><td align="left" valign="top" rowspan="1" colspan="1">Hay</td><td align="left" valign="top" rowspan="1" colspan="1">Valley</td><td align="left" valign="top" rowspan="1" colspan="1">Leaves</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.73</td><td align="left" valign="top" rowspan="1" colspan="1">0.52</td><td align="left" valign="top" rowspan="1" colspan="1">0.70</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td align="left" valign="top" rowspan="1" colspan="1">0.94</td><td align="left" valign="top" rowspan="1" colspan="1">0.55</td><td align="left" valign="top" rowspan="1" colspan="1">0.64</td><td align="left" valign="top" rowspan="1" colspan="1">092</td><td align="left" valign="top" rowspan="1" colspan="1">bale</td><td align="left" valign="top" rowspan="1" colspan="1">0.59</td><td align="left" valign="top" rowspan="1" colspan="1">0.85</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toothbrush</td><td align="left" valign="top" rowspan="1" colspan="1">Dresser</td><td align="left" valign="top" rowspan="1" colspan="1">Trophy</td><td align="left" valign="top" rowspan="1" colspan="1">Column</td><td align="left" valign="top" rowspan="1" colspan="1">Piacemat</td><td align="left" valign="top" rowspan="1" colspan="1">Pan</td><td align="left" valign="top" rowspan="1" colspan="1">Armchair</td><td align="left" valign="top" rowspan="1" colspan="1">Mouse</td><td align="left" valign="top" rowspan="1" colspan="1">City</td><td align="left" valign="top" rowspan="1" colspan="1">Arcade</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewelk</td><td align="left" valign="top" rowspan="1" colspan="1">Bridge</td><td align="left" valign="top" rowspan="1" colspan="1">Sun</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Land</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.47</td><td align="left" valign="top" rowspan="1" colspan="1">0.55</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.60</td><td align="left" valign="top" rowspan="1" colspan="1">pad</td><td align="left" valign="top" rowspan="1" colspan="1">0.67</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">0.50</td><td align="left" valign="top" rowspan="1" colspan="1">0.58</td><td align="left" valign="top" rowspan="1" colspan="1">0.84</td><td align="left" valign="top" rowspan="1" colspan="1">benk</td><td align="left" valign="top" rowspan="1" colspan="1">0.37</td><td align="left" valign="top" rowspan="1" colspan="1">0.53</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">0.89</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Toy</td><td align="left" valign="top" rowspan="1" colspan="1">Dsply case</td><td align="left" valign="top" rowspan="1" colspan="1">Alcove</td><td align="left" valign="top" rowspan="1" colspan="1">Chandelier</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Fireplace</td><td align="left" valign="top" rowspan="1" colspan="1">Computer</td><td align="left" valign="top" rowspan="1" colspan="1">Quay</td><td align="left" valign="top" rowspan="1" colspan="1">Entrance</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Wire</td><td align="left" valign="top" rowspan="1" colspan="1">Cloud</td><td align="left" valign="top" rowspan="1" colspan="1">Flowers</td><td align="left" valign="top" rowspan="1" colspan="1">Cloud</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.99</td><td align="left" valign="top" rowspan="1" colspan="1">0.84</td><td align="left" valign="top" rowspan="1" colspan="1">0.52</td><td align="left" valign="top" rowspan="1" colspan="1">0.43</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.58</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">0.46</td><td align="left" valign="top" rowspan="1" colspan="1">0.48</td><td align="left" valign="top" rowspan="1" colspan="1">0.69</td><td align="left" valign="top" rowspan="1" colspan="1">0.88</td><td align="left" valign="top" rowspan="1" colspan="1">0.27</td><td align="left" valign="top" rowspan="1" colspan="1">0.50</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toilet</td><td align="left" valign="top" rowspan="1" colspan="1">Carpet</td><td align="left" valign="top" rowspan="1" colspan="1">Ashtray</td><td align="left" valign="top" rowspan="1" colspan="1">Tile</td><td align="left" valign="top" rowspan="1" colspan="1">China htch</td><td align="left" valign="top" rowspan="1" colspan="1">Stove hd</td><td align="left" valign="top" rowspan="1" colspan="1">Decoration</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Tower</td><td align="left" valign="top" rowspan="1" colspan="1">Belcony</td><td align="left" valign="top" rowspan="1" colspan="1">Bus</td><td align="left" valign="top" rowspan="1" colspan="1">Truck</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Path</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.98</td><td align="left" valign="top" rowspan="1" colspan="1">0.76</td><td align="left" valign="top" rowspan="1" colspan="1">0.46</td><td align="left" valign="top" rowspan="1" colspan="1">0.37</td><td align="left" valign="top" rowspan="1" colspan="1">0.51</td><td align="left" valign="top" rowspan="1" colspan="1">1.00</td><td align="left" valign="top" rowspan="1" colspan="1">0.58</td><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.46</td><td align="left" valign="top" rowspan="1" colspan="1">0.86</td><td align="left" valign="top" rowspan="1" colspan="1">0.44</td><td align="left" valign="top" rowspan="1" colspan="1">0.47</td><td align="left" valign="top" rowspan="1" colspan="1">0.62</td><td align="left" valign="top" rowspan="1" colspan="1">0.70</td><td align="left" valign="top" rowspan="1" colspan="1">0.14</td><td align="left" valign="top" rowspan="1" colspan="1">0.37</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Twl rck</td><td align="left" valign="top" rowspan="1" colspan="1">Wardrobe</td><td align="left" valign="top" rowspan="1" colspan="1">Wtr bottle</td><td align="left" valign="top" rowspan="1" colspan="1">Railing</td><td align="left" valign="top" rowspan="1" colspan="1">Crtn rod</td><td align="left" valign="top" rowspan="1" colspan="1">Dshwashr</td><td align="left" valign="top" rowspan="1" colspan="1">Blanket</td><td align="left" valign="top" rowspan="1" colspan="1">Filing cbnt</td><td align="left" valign="top" rowspan="1" colspan="1">Garden</td><td align="left" valign="top" rowspan="1" colspan="1">Garage dr</td><td align="left" valign="top" rowspan="1" colspan="1">Awning</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Hill</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.97</td><td align="left" valign="top" rowspan="1" colspan="1">0.61</td><td align="left" valign="top" rowspan="1" colspan="1">0.35</td><td align="left" valign="top" rowspan="1" colspan="1">0.34</td><td align="left" valign="top" rowspan="1" colspan="1">0.48</td><td align="left" valign="top" rowspan="1" colspan="1">0.97</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">0.46</td><td align="left" valign="top" rowspan="1" colspan="1">0.85</td><td align="left" valign="top" rowspan="1" colspan="1">0.44</td><td align="left" valign="top" rowspan="1" colspan="1">0.45</td><td align="left" valign="top" rowspan="1" colspan="1">0.42</td><td align="left" valign="top" rowspan="1" colspan="1">0.60</td><td align="left" valign="top" rowspan="1" colspan="1">0.12</td><td align="left" valign="top" rowspan="1" colspan="1">0.33</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Soap</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Light swtch</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Glass</td><td align="left" valign="top" rowspan="1" colspan="1">Microwave</td><td align="left" valign="top" rowspan="1" colspan="1">End table</td><td align="left" valign="top" rowspan="1" colspan="1">Priter</td><td align="left" valign="top" rowspan="1" colspan="1">Dome</td><td align="left" valign="top" rowspan="1" colspan="1">Wheel</td><td align="left" valign="top" rowspan="1" colspan="1">Truck</td><td align="left" valign="top" rowspan="1" colspan="1">Pole</td><td align="left" valign="top" rowspan="1" colspan="1">Land</td><td align="left" valign="top" rowspan="1" colspan="1">Water</td><td align="left" valign="top" rowspan="1" colspan="1">Ground</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.45</td><td align="left" valign="top" rowspan="1" colspan="1">0.34</td><td align="left" valign="top" rowspan="1" colspan="1">0.34</td><td align="left" valign="top" rowspan="1" colspan="1">0.35</td><td align="left" valign="top" rowspan="1" colspan="1">0.96</td><td align="left" valign="top" rowspan="1" colspan="1">0.50</td><td align="left" valign="top" rowspan="1" colspan="1">0.93</td><td align="left" valign="top" rowspan="1" colspan="1">0.39</td><td align="left" valign="top" rowspan="1" colspan="1">0.80</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.43</td><td align="left" valign="top" rowspan="1" colspan="1">0.29</td><td align="left" valign="top" rowspan="1" colspan="1">0.54</td><td align="left" valign="top" rowspan="1" colspan="1">0.11</td><td align="left" valign="top" rowspan="1" colspan="1">0.31</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Tltppr</td><td align="left" valign="top" rowspan="1" colspan="1">Blanket</td><td align="left" valign="top" rowspan="1" colspan="1">TV stand</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Candle</td><td align="left" valign="top" rowspan="1" colspan="1">Utensils</td><td align="left" valign="top" rowspan="1" colspan="1">TV stand</td><td align="left" valign="top" rowspan="1" colspan="1">Binder</td><td align="left" valign="top" rowspan="1" colspan="1">Fountain</td><td align="left" valign="top" rowspan="1" colspan="1">Air cndtr</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Hedge</td><td align="left" valign="top" rowspan="1" colspan="1">Quay</td><td align="left" valign="top" rowspan="1" colspan="1">Valley</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Grass</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">0.95</td><td align="left" valign="top" rowspan="1" colspan="1">0.42</td><td align="left" valign="top" rowspan="1" colspan="1">0.29</td><td align="left" valign="top" rowspan="1" colspan="1">0.32</td><td align="left" valign="top" rowspan="1" colspan="1">0.35</td><td align="left" valign="top" rowspan="1" colspan="1">0.92</td><td align="left" valign="top" rowspan="1" colspan="1">0.50</td><td align="left" valign="top" rowspan="1" colspan="1">0.66</td><td align="left" valign="top" rowspan="1" colspan="1">0.32</td><td align="left" valign="top" rowspan="1" colspan="1">0.76</td><td align="left" valign="top" rowspan="1" colspan="1">0.40</td><td align="left" valign="top" rowspan="1" colspan="1">0.38</td><td align="left" valign="top" rowspan="1" colspan="1">0.20</td><td align="left" valign="top" rowspan="1" colspan="1">0.41</td><td align="left" valign="top" rowspan="1" colspan="1">0.10</td><td align="left" valign="top" rowspan="1" colspan="1">0.31</td></tr></tbody></table><table-wrap-foot><p>Diagnosticity is the probability of a scene being in a particular category, given the presence of a particular object. Only objects with at least ten instances are included in this table.</p></table-wrap-foot></table-wrap><p>In addition, I examined the number of completely diagnostic objects (diagnosticity = 1) across scene categories. All objects were included in this analysis. I found that indoor scenes tended to have a higher number of completely diagnostic objects compared to outdoor scenes [25.3 vs. 11.8, <italic>t</italic><sub>(14)</sub> = 1.93, <italic>p</italic> = 0.07], although both urban and natural scene categories had the same number of completely diagnostic objects on average (11.8). Again, this is not surprising as indoor scenes had more objects overall, as well as more infrequent objects.</p><p>As noted in section Using Context for Rapid Scene Recognition, the notion of diagnosticity can be used to test hypotheses on the mechanisms of rapid scene categorization. Biederman (<xref ref-type="bibr" rid="B16">1981</xref>) first posited that a scene might be recognized through the recognition of a prominent, diagnostic object. How diagnostic are the largest objects in the scene? For each of the 3499 scenes, I examined the diagnosticity of the largest object in that scene for the scene's category. On average, the largest object has a diagnosticity of 0.32 for the scene category it is in (95% CI: 0.04&#x02013;0.99). Thus, although knowing the identity of the largest object in the scene will allow you to guess the scene category at an above-chance level, it does not reflect the outstanding performance that human observers have with rapid scene categorization. What if you know the identity of the object nearest the center of the image? The mean diagnosticity of the center object was 0.33 (95% CI: 0.03&#x02013;1.00). Although this is a little better than knowing the largest object [<italic>t</italic><sub>(6996)</sub> = 2.3, <italic>p</italic> &#x0003c; 0.05], it seems unlikely that human scene gist performance can be explained from recognizing the center object alone.</p></sec><sec><title>Scene-object specificity</title><p>How many scene categories contain a particular object? Here, I investigated the question by computing the number of scene categories in which each object is found. This measure is useful in the design of experiments in object and scene perception, as it allows experimenters to choose objects that are strongly tied to only one scene category (for example, to study response bias, e.g., Castelhano and Henderson, <xref ref-type="bibr" rid="B26">2008</xref>) or to use objects found in a variety of scenes to de-couple object recognition from inferential effects.</p><p>As shown in Figure <xref ref-type="fig" rid="F7">7</xref>, the majority of objects are closely tied to one or two scene categories. The median number of scene categories containing an object was two. Forty eight percent of objects were only found in one scene category, and of these, 53% had at least two instances in the database, suggesting that this effect was not solely driven by infrequent objects. In fact, 31 of the objects found in only one scene category (5% of the total) had 10 or more instances. These are listed in Table <xref ref-type="table" rid="T6">6</xref>. On the other hand, there was only one object present in all 16 categories (&#x0201c;wall&#x0201d;), and 19 (3% of total) were present in at least nine of the 16 categories. These are also listed in Table <xref ref-type="table" rid="T6">6</xref>.</p><fig id="F7" position="float"><label>Figure 7</label><caption><p><bold>A histogram of the number of basic-level scene categories in which each of the 617 objects is found</bold>. The majority of objects are associated with only one or two categories.</p></caption><graphic xlink:href="fpsyg-04-00777-g0007"/></fig><table-wrap id="T6" position="float"><label>Table 6</label><caption><p><bold>Objects with 10 or more database instances found in only scene category (left), and objects found in at least nine of the 16 basic-level scene categories (right)</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="2" rowspan="1"><bold>Objects in 1 category</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Objects in 9+ categories</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Attic</td><td align="left" valign="top" rowspan="1" colspan="1">Pan</td><td align="left" valign="top" rowspan="1" colspan="1">Bench</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath mat</td><td align="left" valign="top" rowspan="1" colspan="1">Podium</td><td align="left" valign="top" rowspan="1" colspan="1">Box</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bedspread</td><td align="left" valign="top" rowspan="1" colspan="1">Porch</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Cow</td><td align="left" valign="top" rowspan="1" colspan="1">Projection screen</td><td align="left" valign="top" rowspan="1" colspan="1">Clock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Cutting board</td><td align="left" valign="top" rowspan="1" colspan="1">Seagull</td><td align="left" valign="top" rowspan="1" colspan="1">Column</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Desert</td><td align="left" valign="top" rowspan="1" colspan="1">Shower</td><td align="left" valign="top" rowspan="1" colspan="1">Decoration</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Dish towel</td><td align="left" valign="top" rowspan="1" colspan="1">Shower curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">File organizer</td><td align="left" valign="top" rowspan="1" colspan="1">Soap dish</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Goose</td><td align="left" valign="top" rowspan="1" colspan="1">Spotlight</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Hay bale</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Headboard</td><td align="left" valign="top" rowspan="1" colspan="1">Stove hood</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Kettle</td><td align="left" valign="top" rowspan="1" colspan="1">Toothbrush</td><td align="left" valign="top" rowspan="1" colspan="1">Poster</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Microphone</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Staircase</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mountain pass</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Statue</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mouse</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Table</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mouse pad</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Nightstand</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Wall</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Oven</td><td rowspan="1" colspan="1"/><td align="left" valign="top" rowspan="1" colspan="1">Window</td></tr></tbody></table></table-wrap></sec><sec><title>Object pairs and groups</title><p>While the previous statistics have examined relationships between the scenes and single objects in them, it is also important to examine the relationships between multiple objects in a scene. Object co-occurrence has been shown to guide visual search in naturalistic scenes (Mack and Eckstein, <xref ref-type="bibr" rid="B69">2011</xref>); interacting objects tend to be perceptually grouped (Green and Hummel, <xref ref-type="bibr" rid="B47">2006</xref>); object interactions have been shown to increase activity in object-selective cortex (Kim and Biederman, <xref ref-type="bibr" rid="B61">2010</xref>); and scene identity can be predicted from pairs of objects in object-selective cortex (MacEvoy and Epstein, <xref ref-type="bibr" rid="B67">2011</xref>). How informative are groups of objects, and how many objects do you need to be able to predict the scene's category?</p><p>First, I examined the frequency of co-occurrence of object pairs in each basic-level scene category. The 10 most frequent object pairs for each basic-level category are shown in Table <xref ref-type="table" rid="T7">7</xref>.</p><table-wrap id="T7" position="float"><label>Table 7</label><caption><p><bold>The 10 most frequent pairs of objects in each basic-level scene category</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Ki&#x00130;n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>&#x000ce;ff</bold>.</th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Trees</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Wtndow</td><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">Grass</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Trees</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Trees</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ground</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Bunding</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wail</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Trees</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Keyboard</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toilet</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Book</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Trees</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Nightstand</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Book</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr></tbody></table></table-wrap><p>As shown in Table <xref ref-type="table" rid="T7">7</xref>, some object pairs are functionally related (such as &#x0201c;faucet&#x0201d; and &#x0201c;sink&#x0201d; for <italic>bathroom</italic>), while many are not (e.g., &#x0201c;sky&#x0201d; and &#x0201c;building&#x0201d; in <italic>skyscraper</italic> scenes). There are 20 object pairs in this table that are listed in multiple basic-level categories. In fact, <italic>conference rooms</italic> and <italic>dining rooms</italic> share 8 of the 10 most frequent object pairs. However, of these 20 object pairs, only two are shared across superordinate-level categories (&#x0201c;sky&#x0201d; + &#x0201d;building&#x0201d; and &#x0201c;tree&#x0201d; + &#x0201d;sky&#x0201d;). Both of these pairs are shared across natural and urban scene categories. No object pair in this group was observed in both indoor and outdoor scenes. Therefore, although single objects may be found across all superordinate categories, pairs of objects do not share this property.</p><p>Next, I examined the 617 by 617 object co-occurrence matrix collapsed over all scene categories. Overall, the object co-occurrence matrix was sparse, with only 9% of possible object pairings having been observed. Of the observed object pairings, 8% had a co-occurrence probability of 1, indicating that these pairs of objects were always found together, and of these, 9% (<italic>n</italic> = 254, 0.73% of total pairings) were for objects with more than one instance in the database. Thus, requisite object pairs are relatively rare in the world, and arbitrary pairs of objects are generally not seen together.</p><p>What are the most frequent groups of <italic>n</italic> objects in each of the basic-level scene categories? Table <xref ref-type="table" rid="T8">8</xref> shows the most frequent groups of three, four, and five objects for each of the basic-level scene categories. Larger groups are not shown because many natural landscape images have fewer than 6 total objects.</p><table-wrap id="T8" position="float"><label>Table 8</label><caption><p><bold>The most frequent groups of three, four, and five objects found in each of the 16 basic-level scene categories</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Category</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Three-objects</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Four-objects</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Five-objects</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Bathroom</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet, sink, towel</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet, mirror, sink, towel</td><td align="left" valign="top" rowspan="1" colspan="1">Bath, faucet, mirror, sink, towel</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bedroom</td><td align="left" valign="top" rowspan="1" colspan="1">Bed, pillow, window</td><td align="left" valign="top" rowspan="1" colspan="1">Bed, ceiling, pillow, window</td><td align="left" valign="top" rowspan="1" colspan="1">Bed, ceiling, painting, pillow, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Conference</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, chair, table</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, chair, light, table</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, chair, light, table, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Corridor</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, floor, wall</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, door, floor, wall</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling, door, floor, light, wall</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Dining room</td><td align="left" valign="top" rowspan="1" colspan="1">Chair, table, window</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet, chair, table, window</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet, ceiling, chair, curtain, wine glass</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Kitchen</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet, counter, plant</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet, counter, faucet, sink</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet, counter, faucet, sink, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Living room</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow, sofa, table</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow, sofa, table, window</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp, pillow, sofa, table, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Office</td><td align="left" valign="top" rowspan="1" colspan="1">Book, chair, desk</td><td align="left" valign="top" rowspan="1" colspan="1">Chair, monitor, desk, window</td><td align="left" valign="top" rowspan="1" colspan="1">Chair, monitor, desk, whiteboard, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Tail building</td><td align="left" valign="top" rowspan="1" colspan="1">Building, sky, skyscraper</td><td align="left" valign="top" rowspan="1" colspan="1">Building, sky, skyscraper, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Building, road, sky, skyscraper, tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Inside city</td><td align="left" valign="top" rowspan="1" colspan="1">Building, door, window</td><td align="left" valign="top" rowspan="1" colspan="1">Building, door, sky, window</td><td align="left" valign="top" rowspan="1" colspan="1">Building, door, road, sidewalk, window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Street</td><td align="left" valign="top" rowspan="1" colspan="1">Building, car, road</td><td align="left" valign="top" rowspan="1" colspan="1">Building, car, road, sky</td><td align="left" valign="top" rowspan="1" colspan="1">Building, car, road, sidewalk, sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Highway</td><td align="left" valign="top" rowspan="1" colspan="1">Car, road, sky</td><td align="left" valign="top" rowspan="1" colspan="1">Car, road, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Car, road, sign, sky, tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Coast</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean, rock, sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain, ocean, rock, sky</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain, ocean, rock, sand, sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Open country</td><td align="left" valign="top" rowspan="1" colspan="1">Field, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Field, mountain, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Building, field, mountain, river bank, sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Ground, mountain, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Ground, mountain, road, sky, tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Forest</td><td align="left" valign="top" rowspan="1" colspan="1">Bush, sky, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Bush, river, rock, tree</td><td align="left" valign="top" rowspan="1" colspan="1">Bush, river, rock, sky, tree</td></tr></tbody></table></table-wrap><p>How much information do these object groups provide about scene categories? More specifically, are these groups of multiple objects more diagnostic of a scene category than single objects? Here, I computed the diagnosticity [p(category|object)] of the most frequent groups of one to five objects. As shown in Figure <xref ref-type="fig" rid="F8">8</xref>, although the most common object in a scene category has an average diagnosticity of only 0.35, diagnosticity increases with increasing group size up to 0.78 for groups of five. The diagnosticity of object groups did not reliably differ across superordinate categories. This result gives some insight into the third path to scene recognition proposed by Biederman (<xref ref-type="bibr" rid="B16">1981</xref>), that scene recognition can arise through the spatial integration of a few contextually-related objects. Although this bag-of-words approach neglects the spatial relationships between objects, this analysis places a lower bound on the categorization performance that can be achieved by knowing the identities of a few objects. In section Structural Statistics, we will examine the effect of knowing coarse spatial relationships.</p><fig id="F8" position="float"><label>Figure 8</label><caption><p><bold>Diagnosticity of the most frequent object groups for scene categories</bold>.</p></caption><graphic xlink:href="fpsyg-04-00777-g0008"/></fig></sec><sec><title>Scene combinations</title><p>How many unique combinations of objects were observed in the database? Do certain scene categories have more object combinations than others? Let us first examine the theoretical limit: if all 617 objects in the database were independent, and could occur with equal probability in all scenes, then there would be 2<sup>617</sup> possible combinations of objects. Even if we examine only the possible combinations of 6 objects (the median number of unique objects in a scene from our database), this leaves us with an astounding 7.5 &#x000d7; 10<sup>13</sup> combinations!</p><p>In contrast, I observed only 2552 unique object combinations in the 3499-scene database. In other words, 26% of scenes had the exact same combination of objects as at least one other scene in the database. However, this redundancy was not evenly distributed among the different basic-level scene categories. Ninety nine percent of indoor scenes had unique object combinations compared to only 68.6% of outdoor scenes [<italic>t</italic><sub>(14)</sub> = 3.71, <italic>p</italic> &#x0003c; 0.01]. Among the outdoor scenes, 85.1% of urban scenes had a unique object combination vs. 52.1% of natural scenes [<italic>t</italic><sub>(6)</sub> = 2.89, <italic>p</italic> &#x0003c; 0.05]. <italic>Mountain</italic> scenes in particular had very high redundancy in terms of unique object combinations, as only 33.7% of these scenes had a unique combination of objects.</p></sec><sec><title>Entropy</title><p>Information theory provides a formal means of expressing redundancy between objects and scene categories. If all objects in the database were independent and equally probable, then the redundancy of the database could be expressed as log2<sub>(617)</sub> = 9.27 bits per object. However, object frequencies are not uniformly distributed: objects such as &#x0201c;chair&#x0201d; and &#x0201c;sky&#x0201d; are much more frequent than others such as &#x0201c;scaffolding&#x0201d; or &#x0201c;zebra&#x0201d; (section Object Diagnosticity). Relative object frequencies can be accounted for by computing the entropy of the database:
<disp-formula id="E1"><mml:math id="M1"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Where <italic>p</italic>(<italic>o</italic>) refers to the observed probability of each object in the database. In this instance, taking relative frequencies into account reduces the number of bits per object needed to encode the database to 6.25. Imagine that you are trying to guess an object's identity by playing the game &#x0201c;20 questions.&#x0201d; The rules of this game stimulate that you may only ask questions whose answer is &#x0201c;yes&#x0201d; or &#x0201c;no.&#x0201d; This entropy result tells us that you would be able to correctly guess the object by asking, on average, 6 binary questions.</p></sec><sec><title>Mutual information</title><p>How much information do objects and scenes provide about one another? For example, how much evidence do you have about the category <italic>dining room</italic> from the presence or absence of an object such as a &#x0201c;chair?&#x0201d; To formalize this notion, I computed the mutual information between all objects and their scene categories. While diagnosticity tells us how likely an image is to belong to a particular scene category given the presence of a particular object, it does not easily tell us which objects are important, as objects occurring only once in the database are by definition completely diagnostic of that category. Mutual information measures the degree of dependence between objects and scenes and is therefore more immune to the problem of small numbers. Formally, mutual information is computed as:
<disp-formula id="E2"><mml:math id="M2"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Where H represents entropy. More specifically, the frequency of each object O was computed for each scene category S. Thus, O and S are binary variables (O = 1 if the object is found in the image and 0 otherwise, S = 1 if the image belongs to the category and 0 otherwise). Therefore, the mutual information is:
<disp-formula id="E3"><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mtext>S</mml:mtext><mml:mo>|</mml:mo><mml:mtext>O</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;+</mml:mtext><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mtext>O</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>~</mml:mo><mml:mtext>O</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>~</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mtext>&#x000a0;&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>~</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>Log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>~</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>~</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This form of mutual information is similar to that of Ullman and colleagues in computing the information between image fragments and object category (Ullman et al., <xref ref-type="bibr" rid="B98">2002</xref>).</p><p>Table <xref ref-type="table" rid="T9">9</xref> lists the top 10 most informative objects for distinguishing between the 16 basic-level scene categories. An object can share information with a scene category either because it its presence provides strong evidence for a scene category or because its presence provides good evidence against a scene category. Thus, frequent objects that are never found in a scene category, such as &#x0201c;sky&#x0201d; in most indoor scenes or &#x0201c;chair&#x0201d; in many outdoor scenes, make the list.</p><table-wrap id="T9" position="float"><label>Table 9</label><caption><p><bold>The 10 objects with the highest mutual information for each of the 16 basic-level scene categories</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Kit'n.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscrpr</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Nghtstnd</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Armchair</td><td align="left" valign="top" rowspan="1" colspan="1">Keybrd</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toilet</td><td align="left" valign="top" rowspan="1" colspan="1">Dresser</td><td align="left" valign="top" rowspan="1" colspan="1">Prjtn Scrn</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Pot</td><td align="left" valign="top" rowspan="1" colspan="1">Coffee table</td><td align="left" valign="top" rowspan="1" colspan="1">Cmptr</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Median</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Hill</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Colling</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Buffet</td><td align="left" valign="top" rowspan="1" colspan="1">Oven</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Fence</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Podium</td><td align="left" valign="top" rowspan="1" colspan="1">Column</td><td align="left" valign="top" rowspan="1" colspan="1">Plate</td><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Ottoman</td><td align="left" valign="top" rowspan="1" colspan="1">Mouse</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Balcony</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Soap</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Whitebrd</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Napkin</td><td align="left" valign="top" rowspan="1" colspan="1">Dishwshr</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Book</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Shp wdw</td><td align="left" valign="top" rowspan="1" colspan="1">Van</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Sun</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Twl rack</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Candle</td><td align="left" valign="top" rowspan="1" colspan="1">Bowl</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Paper</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Staircase</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bottle</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Exit sign</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Fireplace</td><td align="left" valign="top" rowspan="1" colspan="1">Phone</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Crosswlk</td><td align="left" valign="top" rowspan="1" colspan="1">Bridge</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Carpet</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Placemat</td><td align="left" valign="top" rowspan="1" colspan="1">Plate</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Printer</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Terrance</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td></tr></tbody></table></table-wrap><p>In order to show the usefulness of objects that occur in the scene category, Table <xref ref-type="table" rid="T10">10</xref> lists the 10 most informative objects for each basic-level scene category, listing only those that are found in the scene category.</p><table-wrap id="T10" position="float"><label>Table 10</label><caption><p><bold>The 10 objects with the highest mutual information for each of the 16 basic-level categories</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="8" rowspan="1"><bold>Indoor</bold></th><th align="center" colspan="4" rowspan="1"><bold>Urban</bold></th><th align="center" colspan="4" rowspan="1"><bold>Natural</bold></th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bath</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Bed</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Conf.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Corr.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Dine</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Kit'&#x00130;n</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Liv.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Off.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Tall</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>City</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Strt.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>High</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Cst.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>OpC</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Mntn.</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Frst.</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Towel</td><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" rowspan="1" colspan="1">Sofa</td><td align="left" valign="top" rowspan="1" colspan="1">Desk</td><td align="left" valign="top" rowspan="1" colspan="1">Skyscrpr</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Field</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath</td><td align="left" valign="top" rowspan="1" colspan="1">Nghtstnd</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Table</td><td align="left" valign="top" rowspan="1" colspan="1">Stove</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Monitor</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Sand</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Bush</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td><td align="left" valign="top" rowspan="1" colspan="1">Prjtn Scrn</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Bouquet</td><td align="left" valign="top" rowspan="1" colspan="1">Cabinet</td><td align="left" valign="top" rowspan="1" colspan="1">Armchair</td><td align="left" valign="top" rowspan="1" colspan="1">Keybrd</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Toilet</td><td align="left" valign="top" rowspan="1" colspan="1">Dresser</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Buffet</td><td align="left" valign="top" rowspan="1" colspan="1">Pot</td><td align="left" valign="top" rowspan="1" colspan="1">Coffee table</td><td align="left" valign="top" rowspan="1" colspan="1">Cmptr</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Sidewalk</td><td align="left" valign="top" rowspan="1" colspan="1">Median</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Hill</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">River</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Podium</td><td align="left" valign="top" rowspan="1" colspan="1">Column</td><td align="left" valign="top" rowspan="1" colspan="1">Plate</td><td align="left" valign="top" rowspan="1" colspan="1">Oven</td><td align="left" valign="top" rowspan="1" colspan="1">Lamp</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Antenna</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Person</td><td align="left" valign="top" rowspan="1" colspan="1">Fence</td><td align="left" valign="top" rowspan="1" colspan="1">Sun</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Snow</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Mirror</td><td align="left" valign="top" rowspan="1" colspan="1">Curtain</td><td align="left" valign="top" rowspan="1" colspan="1">Whitebrd</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Napkin</td><td align="left" valign="top" rowspan="1" colspan="1">Sink</td><td align="left" valign="top" rowspan="1" colspan="1">Ottoman</td><td align="left" valign="top" rowspan="1" colspan="1">Mouse</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Balcony</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Boat</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td><td align="left" valign="top" rowspan="1" colspan="1">Land</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Soap</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Exit sign</td><td align="left" valign="top" rowspan="1" colspan="1">Candle</td><td align="left" valign="top" rowspan="1" colspan="1">Dishwshr</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Book</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Shp wdw</td><td align="left" valign="top" rowspan="1" colspan="1">Van</td><td align="left" valign="top" rowspan="1" colspan="1">Tree</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">River</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Branch</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Twl rack</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Painting</td><td align="left" valign="top" rowspan="1" colspan="1">Bowl</td><td align="left" valign="top" rowspan="1" colspan="1">Fireplace</td><td align="left" valign="top" rowspan="1" colspan="1">Paper</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Staircase</td><td align="left" valign="top" rowspan="1" colspan="1">Crosswalk</td><td align="left" valign="top" rowspan="1" colspan="1">Window</td><td align="left" valign="top" rowspan="1" colspan="1">Cloud</td><td align="left" valign="top" rowspan="1" colspan="1">Rock</td><td align="left" valign="top" rowspan="1" colspan="1">Building</td><td align="left" valign="top" rowspan="1" colspan="1">Grass</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bottle</td><td align="left" valign="top" rowspan="1" colspan="1">Carpet</td><td align="left" valign="top" rowspan="1" colspan="1">Projector</td><td align="left" valign="top" rowspan="1" colspan="1">Arch</td><td align="left" valign="top" rowspan="1" colspan="1">Placemat</td><td align="left" valign="top" rowspan="1" colspan="1">Faucet</td><td align="left" valign="top" rowspan="1" colspan="1">Plant</td><td align="left" valign="top" rowspan="1" colspan="1">Phone</td><td align="left" valign="top" rowspan="1" colspan="1">Dock</td><td align="left" valign="top" rowspan="1" colspan="1">Terrace</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Bridge</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Desert</td><td align="left" valign="top" rowspan="1" colspan="1">Mtn pass</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Floor</td><td align="left" valign="top" rowspan="1" colspan="1">Bedsprd</td><td align="left" valign="top" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" rowspan="1" colspan="1">Fire alrm</td><td align="left" valign="top" rowspan="1" colspan="1">Light</td><td align="left" valign="top" rowspan="1" colspan="1">Plate</td><td align="left" valign="top" rowspan="1" colspan="1">Decor</td><td align="left" valign="top" rowspan="1" colspan="1">Printer</td><td align="left" valign="top" rowspan="1" colspan="1">Mountain</td><td align="left" valign="top" rowspan="1" colspan="1">Chair</td><td align="left" valign="top" rowspan="1" colspan="1">Stright</td><td align="left" valign="top" rowspan="1" colspan="1">Streetlight</td><td align="left" valign="top" rowspan="1" colspan="1">Road</td><td align="left" valign="top" rowspan="1" colspan="1">Door</td><td align="left" valign="top" rowspan="1" colspan="1">Car</td><td align="left" valign="top" rowspan="1" colspan="1">Wall</td></tr></tbody></table><table-wrap-foot><p><italic>Only objects making an appearance in the scene category are listed</italic>.</p></table-wrap-foot></table-wrap><p>Finally, Table <xref ref-type="table" rid="T11">11</xref> lists the 10 objects with the highest mutual information over the entire database. These objects are the most useful for distinguishing among the 16 basic-level scene categories.</p><table-wrap id="T11" position="float"><label>Table 11</label><caption><p><bold>Objects with the highest mutual information for all scene categories</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Object name</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Sky</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Building</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Chair</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Road</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Table</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Tree</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Car</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Window</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Pillow</td></tr></tbody></table></table-wrap></sec><sec><title>Scene classification with a bag of words model</title><p>Bag of words models consider a document to be represented by the list of words found within it. While visual bag of words models consider &#x0201c;words&#x0201d; output from object and feature detectors, the model we will consider here involves, literally, the list of object names within each scene. How sufficient is this representation for basic- and superordinate-level scene categorization?</p><p>I also employed a linear SVM classifier trained on the raw object occurrence matrix. Here, each scene is represented as a 617-object vector where each entry represents the count of each object in that scene. The training and testing procedure was identical to that of the ensemble statistics classifier. This classifier had 98% accuracy (AUC = 0.99) at superordinate-level categorization and 92% accuracy (AUC = 0.96) at basic-level categorization; see Figure <xref ref-type="fig" rid="F9">9</xref> for confusion matrix. A sign rank test indicated that the classifier's performance at superordinate-level classification was superior to basic-level classification performance (<italic>Z</italic> = 59, <italic>p</italic> &#x0003c; 0.001). There were no reliable differences in the accuracy of indoor vs. outdoor classification [<italic>t</italic><sub>(14)</sub> &#x0003c; 1], nor urban vs. natural [<italic>t</italic><sub>(6)</sub> &#x0003c; 1]. <italic>Forest</italic> images had the lowest categorization performance (82%), and <italic>bathrooms</italic> had the highest (99%). Thus, knowing all of the objects in a scene is sufficient to categorize scene images at both basic- and superordinate-levels.</p><fig id="F9" position="float"><label>Figure 9</label><caption><p><bold>Confusion matrix for bag-of-words model</bold>. Data from main database using SVM with linear kernel using leave-one-out cross validation.</p></caption><graphic xlink:href="fpsyg-04-00777-g0009"/></fig><p>To what extent is the higher performance of the bag-of-words model compared to the ensemble statistics model due to the higher dimensionality of this model? To answer this question, I ran SVM analyses on sets of six objects, either by randomly sampling from the 617 total objects, or by taking the objects with the highest overall mutual information. For sets of randomly selected objects, mean classification performance was 15.1% correct (95% CI: 13.9&#x02013;16.1%), well below the 61% achieved by the same number of ensemble features. When taking the six objects with the highest overall mutual information (see Table <xref ref-type="table" rid="T11">11</xref>), classification performance was 51.4%, only marginally worse than that of the ensemble statistic model (binomial test, <italic>p</italic> = 0.051). How many objects are necessary to reach ceiling performance? I ran additional SVM analyses on sets of 2&#x02013;512 objects, either by randomly sampling objects or selecting objects with the highest mutual information. As shown in Figure <xref ref-type="fig" rid="F10">10</xref>, ceiling performance is reached with the 64 best objects. Therefore, although higher performance was achieved using a bag-of-words approach, this performance can be attributed to the larger dimensionality as the features contained in the ensemble statistics model contained at least as much information as a similar number of object features.</p><fig id="F10" position="float"><label>Figure 10</label><caption><p><bold>SVM classification performance as a function of the number of objects used in a bag-of-words model</bold>. Blue points indicate randomly sampled objects and red points indicate objects with the highest mutual information. Error bars indicate 95% confidence intervals.</p></caption><graphic xlink:href="fpsyg-04-00777-g0010"/></fig><p>How does the performance of the bag of words model compare to the human rapid scene categorization performance reported in Kadar and Ben-Shahar (<xref ref-type="bibr" rid="B58">2012</xref>)? Overall sensitivity was similar between the bag of words classifier and human observers [<italic>A</italic>&#x02032; = 0.85 for both, <italic>t</italic><sub>(22)</sub> &#x0003c; 1]. However, the patterns of errors for the classifier and human observers were markedly dissimilar. As with the ensemble statistics classifier, error patterns were not well correlated at the basic-level (<italic>r</italic> = 0.04). However, error patterns at the superordinate level actually showed opposite trends from the human observers (<italic>r</italic> = &#x02212;0.88), suggesting that the bag of words representation, although similar in performance to the human observers, is not similar to the human scene gist representation.</p></sec><sec><title>Bag of words discussion</title><p>Here I have examined statistical regularities between object identities and scene categories, ignoring the spatial relationships between these objects. The measures include object frequency, object diagnosticity, the mutual information between an object and its scene category and the number of scene categories each object is found in. At this level of analysis, the relationships that objects have to one another was also considered by examining the co-occurrence frequencies of two or more objects.</p><p>Object frequencies are not equivalent to object &#x0201c;consistency&#x0201d; as used in the visual cognition literature, which tends to be a Boolean variable (a &#x0201c;blender&#x0201d; in a <italic>kitchen</italic> is consistent, a &#x0201c;fire hydrant&#x0201d; in a <italic>kitchen</italic> is inconsistent). Here, object frequencies are continuous and range from 0 (no observed instances of this object for this scene category) to 1 (each scene in this category contains this object). This continuous scale allows the design of new experiments, allowing researchers to ask questions about the perceptual processing or memory differences that might exist for objects that are present in nearly all scene exemplars (frequency = ~1) vs. objects that are present in only about half of the exemplars (frequency = 0.5), vs. objects that are plausible but rare (frequency &#x0003c;0.2).</p><p>The bag of words level of analysis shows additional ways that scene categories differ. The ensemble level of analysis showed large differences between superordinate-level categories in terms of the amount of unnamed objects in scenes: indoor scenes having more than outdoor, and urban having more than natural. At this level of analysis, I found that objects strongly segregate themselves into different basic level scene categories&#x02014;any given object was only found in a small number of scene categories, and when an object is found in multiple basic-level categories, these categories do not cross superordinate classes. A classifier given all object identities achieved near-ceiling performance at both superordinate- and basic-level scene classifications. Thus, knowledge of either a scene's category or an object's identity gives a great deal of information about the other, and full knowledge of all objects in a scene is sufficient for scene categorization.</p><p>Additionally, ceiling performance can be achieved with fewer objects, provided you have the &#x0201c;best&#x0201d; objects (i.e., the objects with the highest mutual information for distinguishing scene categories). Here, I demonstrated that ceiling performance could be reached with the 64 most informative objects. This is of use to those in the computer vision community who perform scene classification using hundreds of off-the-shelf object detectors (e.g., Li et al., <xref ref-type="bibr" rid="B63">2010</xref>). By choosing objects that are informative, rather than frequent, these systems could be made far more efficient.</p><p>The results of the linear SVM classifier suggest that if one knows the identities of all of the objects in a scene, one will know the category of the scene. Although this has been posited as a possible route to scene understanding (Biederman, <xref ref-type="bibr" rid="B16">1981</xref>), behavioral evidence suggests that human observers do not apprehend all of a scene's objects in a single glance (Fei-Fei et al., <xref ref-type="bibr" rid="B38">2007</xref>; Greene and Oliva, <xref ref-type="bibr" rid="B48">2009</xref>). Similarly, although the bag of words classifier had similar overall performance to human observers, it had markedly different patterns of errors, suggesting a representation different from humans. How many objects do people understand in a glance at a scene? This is a notoriously difficult problem as conceptual short term memory is relatively fragile (Potter, <xref ref-type="bibr" rid="B79">1976</xref>), human observers can inflate performance through elaborate rehearsal or guessing strategies (Liu and Jiang, <xref ref-type="bibr" rid="B65">2005</xref>), and observers can demonstrate sensitivity (in the form of negative priming) to objects that they cannot overtly name (VanRullen and Koch, <xref ref-type="bibr" rid="B100">2003</xref>). The most stringent tests estimate that observers can only accurately report one object from a scene after a 250 ms masked display (Liu and Jiang, <xref ref-type="bibr" rid="B65">2005</xref>).</p><p>Can scene recognition proceed from the recognition of just one object? When examining some plausible scenarios, such as perceiving the largest, or the most centered object, diagnosticity for the scene category is around 0.33, far below the performance of human observers in rapid scene classification. Of course, diagnosticity increases with increasing numbers of objects (section Object Pairs and Groups). However, classification performance for smaller numbers of objects, even the most informative objects, lagged behind that of the ensemble statistics model, suggesting that individual objects may not make the best features for human scene understanding and categorization.</p><p>While the bag of words level of analysis is a powerful and popular computer vision model of objects in scenes, the spatial relationships between objects and regions are also critical to scene identity. I explore this level of analysis in the next session.</p></sec></sec><sec><title>Structural statistics</title><p>The third level of object-scene relationships I will explore is aimed toward obtaining a &#x0201c;syntax&#x0201d; of visual scenes that includes the nature of the spatial relations between objects. Just as the relations between object parts are key to the object identity (e.g., a key difference between a pail and a mug is the placement of the handle, Biederman, <xref ref-type="bibr" rid="B17">1987</xref>), the relations between objects may provide additional information into scene identity. The spatial layout of a scene is created in part by the relative positioning of the objects within it, and regularities in layout allow a scene to be identified under highly degraded conditions, such as under sparse contours (Biederman, <xref ref-type="bibr" rid="B16">1981</xref>) or blur (Oliva and Torralba, <xref ref-type="bibr" rid="B75">2007</xref>) where object identities cannot be recovered. Indeed, two of the three pathways to scene gist outlined by Biederman (<xref ref-type="bibr" rid="B16">1981</xref>) can come from structural relations.</p><p>Here, I will examine the locations of objects in scenes, as well as the distances between objects and the spatial distributions of the important diagnostic and informative objects. As with the other two levels of analysis, I will examine the extent to which these structural statistics can be used to classify scenes at the basic- and superordinate-levels.</p><sec><title>Object position specificity</title><p>One basic structural description is the position specificity of individual objects. In other words, how stereotyped are the x-y locations of the objects in the database? Figure <xref ref-type="fig" rid="F11">11</xref> shows a heat map of the spatial locations of the 10 most common objects in the database. Some regions, such as &#x0201c;ceiling,&#x0201d; are tightly bound to a particular image location while others, such as &#x0201c;plant&#x0201d; or &#x0201c;building,&#x0201d; can be found throughout the image plane. To quantify this notion, I examined the variance in x-y position for each object center across the database as well as the position variance of objects in each of the basic-level scene categories.</p><fig id="F11" position="float"><label>Figure 11</label><caption><p><bold>Spatial distribution of the 10 most common objects in the database</bold>. The pixels included in the segmentation mask for each instance of an object were summed to show the most frequent locations of objects.</p></caption><graphic xlink:href="fpsyg-04-00777-g0011"/></fig><p>Table <xref ref-type="table" rid="T12">12</xref> shows the 10 objects with the most position variance as well as the 10 objects with the least position variance in the database. Unsurprisingly, objects with a great deal of position specificity (low variance in x-y position) are often objects that make up the spatial boundaries of a scene (such as &#x0201c;carpet&#x0201d; and &#x0201c;sky&#x0201d;).</p><table-wrap id="T12" position="float"><label>Table 12</label><caption><p><bold>The 10 objects with the least position variance (most static) and with the most position variance (least static)</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"><bold>Most static</bold></th><th align="left" valign="top" rowspan="1" colspan="1"><bold>Least static</bold></th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Carpet</td><td align="left" valign="top" rowspan="1" colspan="1">Molding</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Desert</td><td align="left" valign="top" rowspan="1" colspan="1">Leaves</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ceiling</td><td align="left" valign="top" rowspan="1" colspan="1">Dock</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Exit sign</td><td align="left" valign="top" rowspan="1" colspan="1">Dome</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bath mat</td><td align="left" valign="top" rowspan="1" colspan="1">Pan</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bedspread</td><td align="left" valign="top" rowspan="1" colspan="1">Basket</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ocean</td><td align="left" valign="top" rowspan="1" colspan="1">Grill</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Fan</td><td align="left" valign="top" rowspan="1" colspan="1">Lighthouse</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Bed</td><td align="left" valign="top" rowspan="1" colspan="1">Calendar</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sky</td><td align="left" valign="top" rowspan="1" colspan="1">Toy</td></tr></tbody></table><table-wrap-foot><p><italic>The search for these objects was constrained to objects with at least 10 instances in the database</italic>.</p></table-wrap-foot></table-wrap><p>For basic-level scene categories, <italic>bedrooms</italic> had the most position variance while <italic>open country</italic> scenes had the least. Overall, indoor scenes tended to have more position variance compared to outdoor scenes [<italic>t</italic><sub>(14)</sub> = 2.98, <italic>p</italic> &#x0003c; 0.01]. However, among the outdoor scenes, no distinct pattern emerged [<italic>t</italic><sub>(6)</sub> &#x0003c; 1].</p><p>Are objects found in different locations when they are found in different scene categories? If this is the case, then position can provide diagnostic scene information. Here, I took the 17 objects that had at least 10 instances in indoor categories and at least 10 instances in outdoor scene categories (&#x0201c;bench,&#x0201d; &#x0201c;box,&#x0201d; &#x0201c;chair,&#x0201d; &#x0201c;clock,&#x0201d; &#x0201c;column,&#x0201d; &#x0201c;door,&#x0201d; &#x0201c;light,&#x0201d; &#x0201c;person,&#x0201d; &#x0201c;plant,&#x0201d; &#x0201c;poster,&#x0201d; &#x0201c;railing,&#x0201d; &#x0201c;sign,&#x0201d; &#x0201c;staircase,&#x0201d; &#x0201c;statue,&#x0201d; &#x0201c;trash can,&#x0201d; &#x0201c;wall,&#x0201d; and &#x0201c;window&#x0201d;) and examined image locations for the object when found outdoors and compared it to the locations where the object was found indoors. For each of these objects at each pixel location, I subtracted the number of instances the object was found in that location in an outdoor scene from the number of times the object was found in that location in an indoor scene. Significance was determined by Bonferroni corrected <italic>t</italic>-tests. Only three objects (&#x0201c;door,&#x0201d; &#x0201c;window,&#x0201d; and &#x0201c;plant&#x0201d;) had different location patterns in indoor scenes compared to outdoor scenes. Figure <xref ref-type="fig" rid="F12">12</xref> shows that these objects are found in higher positions in the image plane when found indoors compared to where they are found outdoors. Therefore, most objects are found in similar scene locations regardless of category, so position information generally does not add additional information beyond that of object identity. Additionally, these small differences may reflect both differences in the structure of these environments (such as depth differences, as discussed in section Center of Mass), as well as differing strategies of photographers for capturing the relevant information in different environments. Our knowledge of the three-dimensional world tells us that a &#x0201c;door&#x0201d; is located in a &#x0201c;wall,&#x0201d; and just above the &#x0201c;floor.&#x0201d; Therefore, these differences reflect statistics of photographs, as well as statistics of the external world.</p><fig id="F12" position="float"><label>Figure 12</label><caption><p><bold>Heat maps of the locations of doors (left), plants (center), and windows (right), conditioned on whether the object was found in an outdoor or indoor scene</bold>. Warmer colors indicate locations that are more probable in an indoor scene while cooler colors indicate locations that are more probable for the object in an outdoor scene.</p></caption><graphic xlink:href="fpsyg-04-00777-g0012"/></fig></sec><sec><title>Spatial distribution of diagnostic and informative objects</title><p>Where are the most informative regions of a scene? Photographers tend to center pictures on objects of interest (Tatler et al., <xref ref-type="bibr" rid="B92">2005</xref>), and objects in LabelMe tend to be labeled from the center out (Elazary and Itti, <xref ref-type="bibr" rid="B36">2008</xref>). Do these centered objects have the highest diagnosticity or mutual information for their scene category?</p><p>For each of the 16 scene categories, I plotted all pixels associated with that most informative object or the object with the highest diagnosticity for the scene category. As shown in Figure <xref ref-type="fig" rid="F13">13</xref>, diagnostic objects tend to be centered overall, while informative objects tend to be centered lower in the image. This is not just due to spatial regression to the center, as random selections of objects do not display this behavior.</p><fig id="F13" position="float"><label>Figure 13</label><caption><p><bold>Spatial distribution of the most informative objects for all scene categories (top), most diagnostic object for each scene category (middle) and a random object for each scene (bottom)</bold>.</p></caption><graphic xlink:href="fpsyg-04-00777-g0013"/></fig><p>This analysis formalizes the notion of center-bias in photographs, demonstrating that photographs tend to be centered on scene regions that contain highly diagnostic objects. Highly informative regions, on the other hand, tend to cluster near the bottom of the image. This analysis also shows key differences between the notions of mutual information and diagnosticity. Many of the most informative objects are structural or boundary elements of a scene that can coarsely distinguish between categories, but are not necessarily the most important or interesting objects in a scene (see section Mutual Information and Table <xref ref-type="table" rid="T11">11</xref>). For example, although &#x0201c;carpet&#x0201d; is highly informative because it distinguishes between outdoor and indoor environments, it is not a terribly interesting region. As it is known that the central fixation bias of human observers persists even when important features are moved to the periphery (Tatler, <xref ref-type="bibr" rid="B91">2007</xref>), this finding is unlikely to provide additional insight into human scene perception mechanisms. However, researchers in computer vision might find greater scene classification efficiency in applying object detectors from the center out rather than in a sliding window.</p></sec><sec><title>Scene classification with a structural model</title><p>How much information do object locations provide about scene categories? To answer this question, I divided all of the 3499 scene images into quadrants and computed the number of times each object was found in each quadrant for each image. Thus, compared to the bag of words model, each object is represented four times, once in each of the four quadrant locations. This matrix was fed into a linear SVM classifier with the same training and testing procedures outlined earlier. Superordinate and basic-level categorizations were assessed. Any increase in performance above the bag of words level can be taken as evidence for the utility of spatial object information for scene categorization.</p><p>Overall, this classifier had 98% accuracy (AUC = 0.99) at superordinate-level scene categorization (not significantly different from the 98% correct performance of the bag-of-words model, <italic>Z</italic> &#x0003c; 1) and 89.6% accuracy (AUC = 0.95) at basic-level categorization (significantly lower than the 92% correct from the bag-of-words model, <italic>Z</italic> = 3.7, <italic>p</italic> &#x0003c; 0.001). There was no reliable difference in basic-level categorization accuracy for indoor (90.8% correct) vs. outdoor (89.4% correct) scenes [<italic>t</italic><sub>(14)</sub> &#x0003c; 1], nor between urban and natural scene categories [<italic>t</italic><sub>(6)</sub> &#x0003c; 1]. Performance by category was similar to the bag of words classifier&#x02014;best performance was achieved by <italic>bathroom</italic> (98%, tied with <italic>corridor</italic> and <italic>coast</italic>), while the classifier had the poorest performance on <italic>open country</italic> images (75%). These <italic>open country</italic> images were frequently confused with <italic>mountains</italic> (38.2%), <italic>forests</italic> (34.3%), and <italic>highways</italic> (14.7%).</p><p>Altogether, adding coarse spatial information to the bag of words classifier did not result in higher classification performance. This is not very surprising as the bag of words classifier was at near ceiling performance, as the majority of objects were only found in one or two scene categories (section Scene-Object Specificity), and even objects found in multiple scene categories were generally found in similar locations regardless of category (section Object Position Specificity). The lower performance for basic-level classification is likely due to an increased number of features (617 vs. 2468) with the same number of training examples.</p></sec><sec><title>Structural discussion</title><p>In this section, I have described scenes in terms of objects and their locations in the image plane. First, I described the location variability of each object, showing that objects that describe a scene's boundaries, such as &#x0201c;floor&#x0201d; or &#x0201c;sky&#x0201d; show less position variance than non-structural objects. Interestingly, most objects are found in similar locations in all scene categories. Of the objects found frequently in both outdoor and indoor scene environments, only &#x0201c;door,&#x0201d; &#x0201c;window,&#x0201d; and &#x0201c;plant&#x0201d; showed different patterns. For each of these cases, the object is found higher in the image plane in indoor scenes relative to outdoor scenes. This makes sense as the spatial enclosure of indoor scenes allows objects to be found in these locations. However, knowing an object's position in the x-y image plane does not provide much additional information over knowing its identity.</p><p>Next, I demonstrated that the center bias of photographs shows up in this database as a tendency for the most diagnostic and informative objects to be located near the center of the image. This may reflect the photographer's inherit sensitivity to object diagnosticity, and desire to convey the maximum amount of information about an environment in a single viewpoint. However, as informative objects tend to be large structural areas of a scene, diagnostic objects were more centered in the image.</p><p>Of course, both of these measures reflect statistical regularities of photographs rather than statistical regularities of the world. Although I have shown a tendency of photographers to photograph a &#x0201c;door&#x0201d; higher in the image plane in an indoor environment, we know that doors in the world are located above the &#x0201c;ground,&#x0201d; and within &#x0201c;walls&#x0201d; in all environments. Similarly, &#x0201c;center bias&#x0201d; has no meaning in the immersive, three-dimensional real world. Despite these limitations, statistics of photographs provide insight into how human observers choose to represent information from the real world when forced to choose a single view.</p><p>A linear classifier trained on the bag of words model with coarse spatial location information did not outperform the pure bag of words model, and in fact, fared a little worse in basic-level categorization. There are two reasons for this: (1) most objects are only found in one or two scene categories (section Scene-Object Specificity), so the position of these objects is not going to provide additional category-related information; and (2) of the objects that are found in several scene categories, the majority are found in similar locations regardless of category (section Object Position Specificity).</p><p>This does not mean that structural information does not contribute unique scene information, however. One limitation of measuring structural relationships on scene photographs is that we lose the three spatial dimensions that are available in the world. The third dimension would allow the disambiguation of a variety of object relationships, including containment, support and adjacency. Indeed, these types of object relations can be easily extracted using 3D models (Fisher and Hanrahan, <xref ref-type="bibr" rid="B42">2010</xref>). Additionally, object pairs and groups may have spatial arrangements that are diagnostic for scene category and a more sophisticated learning approach could glean these from the data. For example, although both <italic>dining rooms</italic> and <italic>conference rooms</italic> tend to have centrally located &#x0201c;table&#x0201d; and &#x0201c;chairs,&#x0201d; and may also contain a &#x0201c;telephone,&#x0201d; the presence of telephone <italic>on top of</italic> the table is diagnostic of <italic>conference room</italic>. On the other hand, a structural description on a scene may not be a good model for human scene gist as it has been shown that human scene classification performance can be well explained as the perception of a set of unbound features (Evans and Treisman, <xref ref-type="bibr" rid="B37">2005</xref>). Similarly, electrophysiological markers structural scene processing occur later than markers of semantic processing (V&#x000f5; and Wolfe, <xref ref-type="bibr" rid="B104">2013</xref>). Taken together, these suggest that the first scene representation may include little structural information.</p><p>As ensemble statistics had better classification performance, feature-for-feature, compared to individual objects, a structural model that coarsely localizes these types of features may prove to be more fruitful for future work.</p></sec></sec></sec><sec><title>General discussion</title><p>In this work, I have provided a set of real world image statistics at the level of labeled objects, and assessed the utility of these measurements for scene categorization. By understanding the regularities of natural images, we can design experiments to understand how these redundancies are exploited by the human visual system to efficiently recognize environments and search for objects in those environments.</p><sec><title>Category information comes from different levels of analysis</title><p>I have examined scene-object relationships at three levels of analysis: the ensemble level, the bag of words level, and the structural level. Statistics measured at each level of analysis contained sufficient information to categorize scene environments into basic- and superordinate-level categories. Although we intuitively know that <italic>kitchens</italic> and <italic>offices</italic> differ in terms of the objects found in them, this work also demonstrates that scene categories differ in terms of the amount and types of &#x0201c;things&#x0201d; found in them (ensemble statistics), and to a certain degree in the spatial distribution of their objects (structural statistics).</p><p>Additionally, quantitative analysis of objects in scenes allows us to test the plausibility of hypotheses on the role of object perception in rapid scene categorization. Biederman (<xref ref-type="bibr" rid="B16">1981</xref>) suggested that scenes might be recognized by first recognizing a single, prominent object in the scene. In section Object Pairs and Groups, I demonstrated that knowledge of either the largest object or the most centered object was insufficient to reproduce the high classification performance of human observers. Adding additional objects increases the diagnosticity for the scene, so a path for future work will be to examine how small groups of objects might be rapidly perceived to give rise to scene gist. Classification performance using a few objects as features lagged behind classification performance of ensemble statistics, suggesting that the coarse object information provided by the ensembles was more informative about scene category than individual objects.</p></sec><sec><title>Not all scene categories are created equally</title><p>Similarly, scene categories in different superordinate categories (indoor vs. outdoor, or natural vs. urban) differ markedly from one another at each level of analysis. Compared to outdoor scene categories, indoor environments have a higher object density as well as greater object variety.</p><p>The identities of the objects found in scenes also differs between superordinates, as very few objects were found in both indoor and outdoor scenes. The majority of objects in the database were found in only one or two scene categories, so knowing that an object is present in a scene provides considerable information about the scene environment. However, when considering the few objects that are found in many scene categories (such as &#x0201c;door,&#x0201d; &#x0201c;window,&#x0201d; or &#x0201c;trash can&#x0201d;), object position in the image can (but tends not to) differ by superordinate category, thus giving little additional predictive information about the scene category above that of the object identity.</p><p>Why do these indoor scene categories differ from the outdoor scene categories? One limitation of this database is that the indoor scene categories reflect small-scale indoor environments in the home and workplace. Perhaps larger indoor environments such as <italic>department store</italic> or <italic>warehouse</italic> would show patterns more similar to the outdoor environments, as larger environments mean that more objects will be too small to individually label, leading to a smaller number of measured objects.</p><p>Interestingly, categorization accuracy for all superordinate-level categories was found to be similar for each of the classifiers considered here. This was unexpected, as indoor scene categorization is often considered to be a harder problem than outdoor scene categorization (Quattoni and Torralba, <xref ref-type="bibr" rid="B81">2009</xref>). This result suggests that machine vision systems performing indoor scene categorization can be improved in at least two ways: first, the use of &#x0201c;objectness&#x0201d; detectors (Alexe et al., <xref ref-type="bibr" rid="B2">2012</xref>) could be employed to understand object density and other ensemble statistics that are somewhat diagnostic of scene categories, and second, to use object detectors for the objects that provide the most mutual information for distinguishing scene categories.</p></sec><sec><title>Are all object types created equally?</title><p>Throughout this paper, I have treated each annotated label equally for the purposes of statistical analysis. &#x0201c;Sky&#x0201d; is just as much of an object as &#x0201c;book&#x0201d; in the database, even though it is not tangible and has no clear boundaries in the world. Although defining what counts as an object is a notoriously difficult problem (for a review see Feldman, <xref ref-type="bibr" rid="B40">2003</xref>), one might want to consider sub-types of objects. For example, one might distinguish between object labels that refer to count nouns vs. mass nouns (Burge, <xref ref-type="bibr" rid="B25">1972</xref>; Adelson, <xref ref-type="bibr" rid="B1">2001</xref>; Huntley-Fenner et al., <xref ref-type="bibr" rid="B56">2002</xref>; Prasada et al., <xref ref-type="bibr" rid="B80">2002</xref>). Count nouns are labeled objects that are discrete and countable (&#x0201c;mug,&#x0201d; &#x0201c;building,&#x0201d; &#x0201c;car,&#x0201d; &#x0201c;book&#x0201d;) while mass nouns are regions with no fixed units or boundaries (&#x0201c;field,&#x0201d; &#x0201c;water,&#x0201d; &#x0201c;smoke,&#x0201d; &#x0201c;sky&#x0201d;). This distinction appears to be a fundamental difference in object representation that is present from a very early age (Huntley-Fenner et al., <xref ref-type="bibr" rid="B56">2002</xref>). Alternatively, some of the annotated labels reflect background or boundary elements of a scene, such as &#x0201c;ground,&#x0201d; &#x0201c;sky,&#x0201d; &#x0201c;wall,&#x0201d; or &#x0201c;ceiling.&#x0201d; As a well-accepted definition of a visual scene includes the lawful arrangement of objects on a background (Henderson and Hollingworth, <xref ref-type="bibr" rid="B52">1999</xref>), it is possible that these labeled regions have a different perceptual status than other labels such as &#x0201c;bowl&#x0201d; or &#x0201c;book.&#x0201d; Indeed, objects that make up scene boundaries have the highest mutual information for distinguishing between scene categories (see Table <xref ref-type="table" rid="T11">11</xref>). However, a glance at the labels in Appendix B will convince the reader that it is very easy to find unclear cases.</p></sec><sec><title>Generalizability and database bias</title><p>How generalizable are these findings? In other words, how much do they say about the distribution of objects in the world, and how much do they say about the idiosyncrasies of this particular database? Although the eight-category database from Oliva and Torralba (<xref ref-type="bibr" rid="B74">2001</xref>) used in the main database is a standard scene classification set in computer vision, more modern work has criticized this database for relying too heavily on the Corel Stock Photo collection (Torralba and Efros, <xref ref-type="bibr" rid="B93">2011</xref>), which may represent only over-stylized representations of scenes. Similarly, the indoor images largely consist of highly idealized environments from real estate websites. Do these generalize to more everyday environments? In order to address this question, I have computed all statistics on a separate auxiliary database, and I have shown the similarities and differences between the two datasets whenever possible in Appendix D. Assuming that the bias in these datasets is independent, their degree of overlap reflects the generalizability of these statistics (Torralba and Efros, <xref ref-type="bibr" rid="B93">2011</xref>; Khosla et al., <xref ref-type="bibr" rid="B60">2012</xref>). This assumption is likely to be optimistic, however, as both datasets are part of the larger set of scenes that people find remarkable enough to photograph and share on the web in the first place.</p><p>The two datasets examined in this work showed considerable but not perfect overlap. It is likely that the noted differences between natural landscapes and indoor environments are robust to dataset bias, but perhaps not the differences between urban and indoor scenes. The auxiliary set showed that the differences in these superordinates is driven primarily by natural landscape images, as this database contained very complex urban environments whose images had object density similar to indoor environments. Both datasets showed remarkable overlap in object frequency and mutual information, making these measures generally useful for the design of new experiments on object-scene context. Similarly, the measured entropy was very similar between the two datasets, suggesting that this statistic is robust to any dataset bias. The specificity of objects to a particular scene category was also observed in both sets. However, other measurements should be taken with more caution. The main dataset showed more redundancy (scenes having the same combination of objects) than the auxiliary set, and this manifested itself in higher classifier performance across the board. Appendix D contains more details on the specific differences in the findings between the two data sets.</p><p>Future investigations will continue to validate the generalizability of these data via comparison to other annotated databases such as SUn (Xiao et al., <xref ref-type="bibr" rid="B106a">2010</xref>), or through modeling the bias directly (Khosla et al., <xref ref-type="bibr" rid="B60">2012</xref>). Separately, one can see how these statistics match the intuitions of human observers, although observers' intuitions should not necessarily be counted as ground truth, because we are insensitive to statistical base rates in some domains (Tversky and Kahneman, <xref ref-type="bibr" rid="B97">1974</xref>).</p></sec><sec><title>The utility of object context statistics</title><p>Although it is generally recognized that lawful contextual relationships facilitate scene and object recognition, work in this area has been limited because these contextual relationships have not been fully characterized and quantified. Previous work has characterized contextual relationships as merely being the intuitive plausibility of an object for a given scene environment. Many of the scene-object pairs in these experiments include informative but rare objects, such as a &#x0201c;moose&#x0201d; in a <italic>forest</italic>. Although a moose is more likely to be found in a forest when compared to other types of environments, the vast majority of <italic>forest</italic> images will not include a &#x0201c;moose.&#x0201d; By measuring object frequency, diagnosticity and mutual information, experimenters will be able to determine the perceptual and memory consequences of these relationships individually. Furthermore, current experiments treat contextual relationships as binary&#x02014;an object is either contextually related to an environment or it is not. However, the statistics measured here are continuous, allowing for more subtle questions to be asked.</p><p>More broadly, it has been argued that we cannot yet perform well-controlled studies on natural scene images because it is too difficult to understand or control the stimuli (Rust and Movshon, <xref ref-type="bibr" rid="B87">2005</xref>). The results presented here take a necessary step toward this goal by characterizing complex scene stimuli in terms of quantified object-scene relationships. At all levels of analysis, real-world scene images show remarkable redundancy that can be utilized by the brain to represent the world efficiently. Therefore, measuring these statistics allows us to better understand and control our stimuli and to move forward into more real-world vision research.</p></sec><sec><title>Conflict of interest statement</title><p>The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>Thanks to Antonio Torralba, Aude Oliva, and Jeremy Wolfe for providing advice and guidance for this project. Thanks to Karla Evans, Melissa Vo, Talia Konkle, Fei-Fei Li Adam Katz, Marius C. Iordan, Chris Baldassano, and Olga Russakovsky for useful conversations about this project. This work was supported by an NSF-GRF to Michelle R. Greene, as well as NRSA F32 EY19815 to Michelle R. Greene.</p></ack><fn-group><fn id="fn0001"><p><sup>1</sup>Upon publication, this database and Matlab structures containing statistical data will be publically available for download on the author's website.</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>E. H.</given-names></name></person-group> (<year>2001</year>). <article-title>On seeing stuff: the perception of materials by humans and machines</article-title>. <source>Proc. SPIE</source>
<volume>4299</volume>, <fpage>1</fpage>&#x02013;<lpage>12</lpage>
<pub-id pub-id-type="doi">10.1117/12.429489</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexe</surname><given-names>B.</given-names></name><name><surname>Deselaers</surname><given-names>T.</given-names></name><name><surname>Ferrari</surname><given-names>V.</given-names></name></person-group> (<year>2012</year>). <article-title>Measuring the objectness of image windows</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>34</volume>, <fpage>2189</fpage>&#x02013;<lpage>2202</lpage>
<pub-id pub-id-type="doi">10.1109/TPAMI.2012.28</pub-id><?supplied-pmid 22248633?><pub-id pub-id-type="pmid">22248633</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname><given-names>G. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Representing multiple objects as an ensemble enhances visual cognition</article-title>. <source>Trends Cogn. Sci</source>. <volume>15</volume>, <fpage>122</fpage>&#x02013;<lpage>131</lpage>
<pub-id pub-id-type="doi">10.1016/j.tics.2011.01.003</pub-id><?supplied-pmid 21292539?><pub-id pub-id-type="pmid">21292539</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname><given-names>G.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>The representation of simple ensemble visual features outside the focus of attention</article-title>. <source>Psychol. Sci</source>. <volume>19</volume>, <fpage>392</fpage>&#x02013;<lpage>398</lpage>
<pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02098.x</pub-id><?supplied-pmid 18399893?><pub-id pub-id-type="pmid">18399893</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname><given-names>G. A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>Spatial ensemble statistics are efficient codes that can be represented with reduced attention</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>106</volume>, <fpage>7345</fpage>&#x02013;<lpage>7350</lpage>
<pub-id pub-id-type="doi">10.1073/pnas.0808981106</pub-id><?supplied-pmid 19380739?><pub-id pub-id-type="pmid">19380739</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ariely</surname><given-names>D.</given-names></name></person-group> (<year>2001</year>). <article-title>Seeing sets: representation by statistical properties</article-title>. <source>Psychol. Sci</source>. <volume>12</volume>, <fpage>157</fpage>&#x02013;<lpage>162</lpage>
<pub-id pub-id-type="doi">10.1111/1467-9280.00327</pub-id><?supplied-pmid 11340926?><pub-id pub-id-type="pmid">11340926</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F.</given-names></name></person-group> (<year>1954</year>). <article-title>Some informational aspects of visual perception</article-title>. <source>Psychol. Rev</source>. <volume>61</volume>, <fpage>183</fpage>&#x02013;<lpage>193</lpage>
<pub-id pub-id-type="doi">10.1037/h0054663</pub-id><?supplied-pmid 13167245?><pub-id pub-id-type="pmid">13167245</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auckland</surname><given-names>M. E.</given-names></name><name><surname>Cave</surname><given-names>K.</given-names></name><name><surname>Donnelly</surname><given-names>N.</given-names></name></person-group> (<year>2007</year>). <article-title>Nontarget objects can influence perceptual processes during object recognition</article-title>. <source>Psychon. Bull. Rev</source>. <volume>14</volume>, <fpage>332</fpage>&#x02013;<lpage>337</lpage>
<pub-id pub-id-type="doi">10.3758/BF03194073</pub-id><?supplied-pmid 17694922?><pub-id pub-id-type="pmid">17694922</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balas</surname><given-names>B.</given-names></name><name><surname>Nakano</surname><given-names>L.</given-names></name><name><surname>Rosenholtz</surname><given-names>R.</given-names></name></person-group> (<year>2009</year>). <article-title>A summary-statistic representation in peripheral vision explains visual crowding</article-title>. <source>J. Vis</source>. <volume>9</volume>, <fpage>1</fpage>&#x02013;<lpage>18</lpage>
<pub-id pub-id-type="doi">10.1167/9.12.13</pub-id><?supplied-pmid 20053104?><pub-id pub-id-type="pmid">20053104</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Visual objects in context</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>5</volume>, <fpage>617</fpage>&#x02013;<lpage>625</lpage>
<pub-id pub-id-type="doi">10.1038/nrn1476</pub-id><?supplied-pmid 15263892?><pub-id pub-id-type="pmid">15263892</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M.</given-names></name><name><surname>Aminoff</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>Cortical analysis of visual context</article-title>. <source>Neuron</source>
<volume>38</volume>, <fpage>347</fpage>&#x02013;<lpage>358</lpage>
<pub-id pub-id-type="doi">10.1016/S0896-6273(03)00167-3</pub-id><?supplied-pmid 12718867?><pub-id pub-id-type="pmid">12718867</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M.</given-names></name><name><surname>Ullman</surname><given-names>S.</given-names></name></person-group> (<year>1996</year>). <article-title>Spatial context in recognition</article-title>. <source>Perception</source>
<volume>25</volume>, <fpage>343</fpage>&#x02013;<lpage>352</lpage>
<pub-id pub-id-type="doi">10.1068/p250343</pub-id><?supplied-pmid 8804097?><pub-id pub-id-type="pmid">8804097</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H.</given-names></name></person-group> (<year>2001</year>). <article-title>The exploitation of regularities in the environment by the brain</article-title>. <source>Behav. Brain Sci</source>. <volume>24</volume>, <fpage>602</fpage>&#x02013;<lpage>607</lpage>
<pub-id pub-id-type="doi">10.1017/S0140525X01000024</pub-id><?supplied-pmid 12048943?><pub-id pub-id-type="pmid">12048943</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bartlet</surname><given-names>F. C.</given-names></name></person-group> (<year>1932</year>). <source>Remembering: A Study in Experimental and Social Psychology</source>. <publisher-loc>Camrbidge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>M.</given-names></name><name><surname>Pashler</surname><given-names>H.</given-names></name><name><surname>Lubin</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Object-instrinsic oddities draw early saccades</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>33</volume>, <fpage>20</fpage>&#x02013;<lpage>30</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.33.1.20</pub-id><?supplied-pmid 17311476?><pub-id pub-id-type="pmid">17311476</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name></person-group> (<year>1981</year>). <article-title>On the semantics of a glance at a scene</article-title>, in <source>Perceptual Organization</source> (<publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>), <fpage>213</fpage>&#x02013;<lpage>253</lpage></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name></person-group> (<year>1987</year>). <article-title>Recognition-by-components: a theory of human image understanding</article-title>. <source>Psychol. Rev</source>. <volume>94</volume>, <fpage>115</fpage>&#x02013;<lpage>147</lpage>
<pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><?supplied-pmid 3575582?><pub-id pub-id-type="pmid">3575582</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name></person-group> (<year>1988</year>). <article-title>Aspects and extensions of a theory of human image understanding</article-title>, in <source>Computational Processes in Human Vision: An Interdisciplinary Perspective</source>, ed <person-group person-group-type="editor"><name><surname>Pylyshyn</surname><given-names>Z. W.</given-names></name></person-group> (<publisher-loc>Norwood, NJ</publisher-loc>: <publisher-name>Ablex</publisher-name>), <fpage>370</fpage>&#x02013;<lpage>428</lpage></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name><name><surname>Mezzanotte</surname><given-names>R.</given-names></name><name><surname>Rabinowitz</surname><given-names>J.</given-names></name></person-group> (<year>1982</year>). <article-title>Scene perception: detecting and judging objects undergoing relational violations</article-title>. <source>Cogn. Psychol</source>. <volume>14</volume>, <fpage>143</fpage>&#x02013;<lpage>177</lpage>
<pub-id pub-id-type="doi">10.1016/0010-0285(82)90007-X</pub-id><?supplied-pmid 7083801?><pub-id pub-id-type="pmid">7083801</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>D. M.</given-names></name><name><surname>Ng</surname><given-names>A. Y.</given-names></name><name><surname>Jordan</surname><given-names>M. I.</given-names></name></person-group> (<year>2003</year>). <article-title>Latent dirichlet allocation</article-title>. <source>J. Mach. Learn. Res</source>. <volume>3</volume>, <fpage>993</fpage>&#x02013;<lpage>1022</lpage></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bosch</surname><given-names>A.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name><name><surname>Mu&#x000f1;oz</surname><given-names>X.</given-names></name></person-group> (<year>2006</year>). <article-title>Scene classification via pLSA</article-title>, in <source>Computer Vision&#x02014;ECCV</source>
<volume>2006</volume>, eds <person-group person-group-type="editor"><name><surname>Leonardis</surname><given-names>A.</given-names></name><name><surname>Bischof</surname><given-names>H.</given-names></name><name><surname>Pinz</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>Graz</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>), <fpage>517</fpage>&#x02013;<lpage>530</lpage></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyce</surname><given-names>S. J.</given-names></name><name><surname>Pollatsek</surname><given-names>A.</given-names></name><name><surname>Rayner</surname><given-names>K.</given-names></name></person-group> (<year>1989</year>). <article-title>Effect of background information on object identification</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>15</volume>, <fpage>556</fpage>&#x02013;<lpage>566</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.15.3.556</pub-id><?supplied-pmid 2527962?><pub-id pub-id-type="pmid">2527962</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brockmole</surname><given-names>J. R.</given-names></name><name><surname>Henderson</surname><given-names>J. M.</given-names></name></person-group> (<year>2006</year>). <article-title>Using real-world scenes as contextual cues for search</article-title>. <source>Vis. Cogn</source>. <volume>13</volume>, <fpage>99</fpage>&#x02013;<lpage>108</lpage>
<pub-id pub-id-type="doi">10.1080/13506280500165188</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brockmole</surname><given-names>J. R.</given-names></name><name><surname>V&#x000f5;</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Semantic memory for contextual regularities within and across scene categories: evidence from eye movements</article-title>. <source>Attent. Percept. Psychophys</source>. <volume>72</volume>, <fpage>1803</fpage>&#x02013;<lpage>1813</lpage>
<pub-id pub-id-type="doi">10.3758/APP.72.7.1803</pub-id><?supplied-pmid 20952779?><pub-id pub-id-type="pmid">20952779</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>T.</given-names></name></person-group> (<year>1972</year>). <article-title>Truth and mass terms</article-title>. <source>J. Philos</source>. <volume>69</volume>, <fpage>263</fpage>&#x02013;<lpage>282</lpage>
<pub-id pub-id-type="doi">10.2307/2024729</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelhano</surname><given-names>M. S.</given-names></name><name><surname>Henderson</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>The influence of color on the perception of scene gist</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>34</volume>, <fpage>660</fpage>&#x02013;<lpage>675</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.34.3.660</pub-id><?supplied-pmid 18505330?><pub-id pub-id-type="pmid">18505330</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C. C.</given-names></name><name><surname>Lin</surname><given-names>C.-J.</given-names></name></person-group> (<year>2011</year>). <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Trans. Intell. Syst. Technol</source>. <volume>2</volume>, <fpage>1</fpage>&#x02013;<lpage>27</lpage>
<pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id><?supplied-pmid 17217518?><pub-id pub-id-type="pmid">17217518</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>M.</given-names></name><name><surname>Lim</surname><given-names>J.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Willsky</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Exploiting hierarchical context on a large database of object categories</article-title>. in <source>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</source>, (<publisher-loc>San Francisco, CA</publisher-loc>), <fpage>129</fpage>&#x02013;<lpage>136</lpage></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chong</surname><given-names>S. C.</given-names></name><name><surname>Treisman</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>Representation of statistical properties</article-title>. <source>Vision Res</source>. <volume>43</volume>, <fpage>393</fpage>&#x02013;<lpage>404</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(02)00596-5</pub-id><?supplied-pmid 12535996?><pub-id pub-id-type="pmid">12535996</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chong</surname><given-names>S. C.</given-names></name><name><surname>Treisman</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>Attentional spread in the statistical processing of visual displays</article-title>. <source>Percept. Psychophys</source>. <volume>67</volume>, <fpage>1</fpage>&#x02013;<lpage>13</lpage>
<pub-id pub-id-type="doi">10.3758/BF03195009</pub-id><?supplied-pmid 15912869?><pub-id pub-id-type="pmid">15912869</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname><given-names>M. M.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name></person-group> (<year>1998</year>). <article-title>Contextual cueing: implicit learning and memory of visual context guides spatial attention<sup>*</sup>1, <sup>*</sup>2</article-title>. <source>Cogn. Psychol</source>. <volume>36</volume>, <fpage>28</fpage>&#x02013;<lpage>71</lpage>
<pub-id pub-id-type="doi">10.1006/cogp.1998.0681</pub-id><?supplied-pmid 9679076?><pub-id pub-id-type="pmid">9679076</pub-id></mixed-citation></ref><ref id="B31a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Consistency effects between objects and scenes</article-title>. <source>Mem. Cognit</source>. <volume>35</volume>, <fpage>393</fpage>&#x02013;<lpage>401</lpage>
<pub-id pub-id-type="doi">10.3758/BF03193280</pub-id><?supplied-pmid 17691140?><pub-id pub-id-type="pmid">17691140</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname><given-names>J.</given-names></name><name><surname>Potter</surname><given-names>M. C.</given-names></name></person-group> (<year>2004</year>). <article-title>Scene consistency in object and background perception</article-title>. <source>Psychol. Sci</source>. <volume>15</volume>, <fpage>559</fpage>&#x02013;<lpage>564</lpage>
<pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00719.x</pub-id><?supplied-pmid 15271002?><pub-id pub-id-type="pmid">15271002</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deerwester</surname><given-names>S.</given-names></name><name><surname>Dumais</surname><given-names>S.</given-names></name><name><surname>Landauer</surname><given-names>T.</given-names></name><name><surname>Furnas</surname><given-names>G.</given-names></name><name><surname>Harshman</surname><given-names>R.</given-names></name></person-group> (<year>1990</year>). <article-title>Indexing by latent semantic analysis</article-title>. <source>J. Am. Soc. Inf. Sci</source>. <volume>41</volume>, <fpage>391</fpage>&#x02013;<lpage>407</lpage></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Graef</surname><given-names>P.</given-names></name><name><surname>Christiaens</surname><given-names>D.</given-names></name><name><surname>D'Ydewalle</surname><given-names>G.</given-names></name></person-group> (<year>1990</year>). <article-title>Perceptual effects of scene context on object identification</article-title>. <source>Psychol. Res</source>. <volume>52</volume>, <fpage>317</fpage>&#x02013;<lpage>329</lpage>
<pub-id pub-id-type="doi">10.1007/BF00868064</pub-id><?supplied-pmid 2287695?><pub-id pub-id-type="pmid">2287695</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname><given-names>M.</given-names></name><name><surname>Drescher</surname><given-names>B.</given-names></name><name><surname>Shimozaki</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <article-title>Attentional cues in real scenes, saccadic targeting, and Bayesian priors</article-title>. <source>Psychol. Sci</source>. <volume>17</volume>, <fpage>973</fpage>&#x02013;<lpage>980</lpage>
<pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01815.x</pub-id><?supplied-pmid 17176430?><pub-id pub-id-type="pmid">17176430</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elazary</surname><given-names>L.</given-names></name><name><surname>Itti</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>Interesting objects are visually salient</article-title>. <source>J. Vis</source>. <volume>8</volume>, <fpage>1</fpage>&#x02013;<lpage>15</lpage>
<pub-id pub-id-type="doi">10.1167/8.3.3</pub-id><?supplied-pmid 18484809?><pub-id pub-id-type="pmid">18484809</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>K. K.</given-names></name><name><surname>Treisman</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>Perception of objects in natural scenes: is it really attention free?</article-title>
<source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>31</volume>, <fpage>1476</fpage>&#x02013;<lpage>1492</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.31.6.1476</pub-id><?supplied-pmid 16366803?><pub-id pub-id-type="pmid">16366803</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fei-Fei</surname><given-names>L.</given-names></name><name><surname>Iyer</surname><given-names>A.</given-names></name><name><surname>Koch</surname><given-names>C.</given-names></name><name><surname>Perona</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>What do we perceive in a glance of a real-world scene?</article-title>
<source>J. Vis</source>. <volume>7</volume>, <fpage>1</fpage>&#x02013;<lpage>29</lpage>
<pub-id pub-id-type="doi">10.1167/7.1.10</pub-id><?supplied-pmid 17461678?><pub-id pub-id-type="pmid">17997664</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fei-Fei</surname><given-names>L.</given-names></name><name><surname>Perona</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). <article-title>A Bayesian hierarchical model for learning natural scene categories</article-title>, in <source>Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), IEEE Computer Society</source>, <volume>Vol. 2</volume>, (<publisher-loc>San Diego, CA</publisher-loc>), <fpage>524</fpage>&#x02013;<lpage>531</lpage>
<pub-id pub-id-type="doi">10.1109/CVPR.2005.16</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>). <article-title>What is a visual object?</article-title>
<source>Trends Cogn. Sci</source>. <volume>7</volume>, <fpage>252</fpage>&#x02013;<lpage>256</lpage>
<pub-id pub-id-type="doi">10.1016/S1364-6613(03)00111-6</pub-id><pub-id pub-id-type="pmid">12804691</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fine</surname><given-names>I.</given-names></name><name><surname>MacLeod</surname><given-names>D. I. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Visual segmentation based on the luminance and chromaticity statistics of natural scenes</article-title>. <source>J. Vis</source>. <volume>1</volume>, <fpage>63</fpage>
<pub-id pub-id-type="doi">10.1167/1.3.63</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>M.</given-names></name><name><surname>Hanrahan</surname><given-names>P.</given-names></name></person-group> (<year>2010</year>). <article-title>Context-based search for 3D models</article-title>. <source>ACM Trans. Graph</source>. <volume>29</volume>, <fpage>182:1</fpage>&#x02013;<lpage>182:10</lpage>
<pub-id pub-id-type="doi">10.1145/1882261.1866204</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>A.</given-names></name></person-group> (<year>1979</year>). <article-title>Framing pictures: the role of knowledge in automatized encoding and memory for gist</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>108</volume>, <fpage>316</fpage>&#x02013;<lpage>355</lpage>
<pub-id pub-id-type="doi">10.1037/0096-3445.108.3.316</pub-id><?supplied-pmid 528908?><pub-id pub-id-type="pmid">528908</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>W. S.</given-names></name></person-group> (<year>2008</year>). <article-title>Visual perception and the statistical properties of natural scenes</article-title>. <source>Annu. Rev. Psychol</source>. <volume>59</volume>, <fpage>167</fpage>&#x02013;<lpage>192</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id><?supplied-pmid 17705683?><pub-id pub-id-type="pmid">17705683</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>W. S.</given-names></name><name><surname>Perry</surname><given-names>J. S.</given-names></name><name><surname>Super</surname><given-names>B. J.</given-names></name><name><surname>Gallogly</surname><given-names>D. P.</given-names></name></person-group> (<year>2001</year>). <article-title>Edge co-occurrence in natural images predicts contour grouping performance</article-title>. <source>Vision Res</source>. <volume>41</volume>, <fpage>711</fpage>&#x02013;<lpage>724</lpage>
<pub-id pub-id-type="doi">10.1016/S0042-6989(00)00277-7</pub-id><?supplied-pmid 11248261?><pub-id pub-id-type="pmid">11248261</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golz</surname><given-names>J.</given-names></name><name><surname>MacLeod</surname><given-names>D. I. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Influence of scene statistics on colour constancy</article-title>. <source>Nature</source>
<volume>415</volume>, <fpage>637</fpage>&#x02013;<lpage>640</lpage>
<pub-id pub-id-type="doi">10.1038/415637a</pub-id><?supplied-pmid 11832945?><pub-id pub-id-type="pmid">11832945</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>C.</given-names></name><name><surname>Hummel</surname><given-names>J.</given-names></name></person-group> (<year>2006</year>). <article-title>Familiar interacting object pairs are perceptually grouped</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>32</volume>, <fpage>1107</fpage>&#x02013;<lpage>1119</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.32.5.1107</pub-id><?supplied-pmid 17002525?><pub-id pub-id-type="pmid">17002525</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>M. R.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>Recognition of natural scenes from global properties: seeing the forest without representing the trees</article-title>. <source>Cogn. Psychol</source>. <volume>58</volume>, <fpage>137</fpage>&#x02013;<lpage>176</lpage>
<pub-id pub-id-type="doi">10.1016/j.cogpsych.2008.06.001</pub-id><?supplied-pmid 18762289?><pub-id pub-id-type="pmid">18762289</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haberman</surname><given-names>J.</given-names></name><name><surname>Whitney</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>). <article-title>Rapid extraction of mean emotion and gender from sets of faces</article-title>. <source>Curr. Biol</source>. <volume>17</volume>, <fpage>R751</fpage>&#x02013;<lpage>R753</lpage>
<pub-id pub-id-type="doi">10.1016/j.cub.2007.06.039</pub-id><?supplied-pmid 17803921?><pub-id pub-id-type="pmid">17803921</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haberman</surname><given-names>J.</given-names></name><name><surname>Whitney</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Ensemble perception: summarizing the scene and broadening the limits of visual processing</article-title>. in <source>From Perception to Consciousness: Searching with Anne Treisman</source>, eds <person-group person-group-type="editor"><name><surname>Wolfe</surname><given-names>J.</given-names></name><name><surname>Robertson</surname><given-names>L.</given-names></name></person-group> (<publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>339</fpage>&#x02013;<lpage>349</lpage></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>J. M.</given-names></name></person-group> (<year>1992</year>). <article-title>Object identification in context: the visual processing of natural scenes</article-title>. <source>Can. J. Psychol</source>. <volume>46</volume>, <fpage>319</fpage>&#x02013;<lpage>341</lpage>
<pub-id pub-id-type="doi">10.1037/h0084325</pub-id><?supplied-pmid 1486550?><pub-id pub-id-type="pmid">1486550</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>J. M.</given-names></name><name><surname>Hollingworth</surname><given-names>A.</given-names></name></person-group> (<year>1999</year>). <article-title>High-level scene perception</article-title>. <source>Annu. Rev. Psychol</source>. <volume>50</volume>, <fpage>243</fpage>&#x02013;<lpage>271</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.psych.50.1.243</pub-id><?supplied-pmid 10074679?><pub-id pub-id-type="pmid">10074679</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>J. M.</given-names></name><name><surname>Weeks</surname><given-names>P.</given-names></name><name><surname>Hollingworth</surname><given-names>A.</given-names></name></person-group> (<year>1999</year>). <article-title>The effects of semantic consistency on eye movements during complex scene viewing</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>25</volume>, <fpage>210</fpage>&#x02013;<lpage>228</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.25.1.210</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollingworth</surname><given-names>A.</given-names></name><name><surname>Henderson</surname><given-names>J. M.</given-names></name></person-group> (<year>1998</year>). <article-title>Does consistent scene context facilitate object perception?</article-title>
<source>J. Exp. Psychol. Gen</source>. <volume>127</volume>, <fpage>398</fpage>&#x02013;<lpage>415</lpage>
<pub-id pub-id-type="doi">10.1037/0096-3445.127.4.398</pub-id><?supplied-pmid 9857494?><pub-id pub-id-type="pmid">9857494</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname><given-names>C. Q.</given-names></name><name><surname>Purves</surname><given-names>D.</given-names></name></person-group> (<year>2004</year>). <article-title>Size contrast and assimilation explained by the statistics of natural scene geometry</article-title>. <source>J. Cogn. Neurosci</source>. <volume>16</volume>, <fpage>90</fpage>&#x02013;<lpage>102</lpage>
<pub-id pub-id-type="doi">10.1162/089892904322755584</pub-id><?supplied-pmid 15006039?><pub-id pub-id-type="pmid">15006039</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huntley-Fenner</surname><given-names>G.</given-names></name><name><surname>Carey</surname><given-names>S.</given-names></name><name><surname>Solimando</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Objects are individuals but stuff doesn't count: perceived rigidity and cohesiveness influence infants' representations of small groups of discrete entities</article-title>. <source>Cognition</source>
<volume>85</volume>, <fpage>203</fpage>&#x02013;<lpage>221</lpage>
<pub-id pub-id-type="doi">10.1016/S0010-0277(02)00088-4</pub-id><?supplied-pmid 12169409?><pub-id pub-id-type="pmid">12169409</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joubert</surname><given-names>O. R.</given-names></name><name><surname>Rousselet</surname><given-names>G. A.</given-names></name><name><surname>Fize</surname><given-names>D.</given-names></name><name><surname>Fabre-Thorpe</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Processing scene context: fast categorization and object interference</article-title>. <source>Vision Res</source>. <volume>47</volume>, <fpage>3286</fpage>&#x02013;<lpage>3297</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2007.09.013</pub-id><?supplied-pmid 17967472?><pub-id pub-id-type="pmid">17967472</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadar</surname><given-names>I.</given-names></name><name><surname>Ben-Shahar</surname><given-names>O.</given-names></name></person-group> (<year>2012</year>). <article-title>A perceptual paradigm and psychophysical evidence for hierarchy in scene gist processing</article-title>. <source>J. Vis</source>. <volume>12</volume>:<fpage>pii:16</fpage>
<pub-id pub-id-type="doi">10.1167/12.13.16</pub-id><?supplied-pmid 23255732?><pub-id pub-id-type="pmid">23255732</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karklin</surname><given-names>Y.</given-names></name><name><surname>Lewicki</surname><given-names>M. S.</given-names></name></person-group> (<year>2003</year>). <article-title>Learning higher-order structures in natural images</article-title>. <source>Network</source>
<volume>14</volume>, <fpage>483</fpage>&#x02013;<lpage>499</lpage>
<pub-id pub-id-type="doi">10.1088/0954-898X/14/3/306</pub-id><?supplied-pmid 12938768?><pub-id pub-id-type="pmid">12938768</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Khosla</surname><given-names>A.</given-names></name><name><surname>Zhou</surname><given-names>T.</given-names></name><name><surname>Malisiewicz</surname><given-names>T.</given-names></name><name><surname>Efros</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Undoing the damage of database bias</article-title>, in <source>European Conference on Computer Vision (ECCV)</source>, (<publisher-loc>Florence</publisher-loc>).</mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J. G.</given-names></name><name><surname>Biederman</surname><given-names>I.</given-names></name></person-group> (<year>2010</year>). <article-title>Where do objects become scenes?</article-title>
<source>Cereb. Cortex</source>
<volume>21</volume>, <fpage>1738</fpage>&#x02013;<lpage>1746</lpage>
<pub-id pub-id-type="doi">10.1093/cercor/bhq240</pub-id><?supplied-pmid 21148087?><pub-id pub-id-type="pmid">21148087</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lazebnik</surname><given-names>S.</given-names></name><name><surname>Schmid</surname><given-names>C.</given-names></name><name><surname>Ponce</surname><given-names>J.</given-names></name></person-group> (<year>2006</year>). <article-title>Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</article-title>, in <source>Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE Computer Society</source>, <volume>Vol. 2</volume>, (<publisher-loc>New York, NY</publisher-loc>), <fpage>2169</fpage>&#x02013;<lpage>2178</lpage></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L.-J.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Lim</surname><given-names>Y.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). <article-title>Objects as attributes for scene classification</article-title>, in <source>Trends and Topics in Computer Vision</source>, ed <person-group person-group-type="editor"><name><surname>Kutulakos</surname><given-names>K. N.</given-names></name></person-group> (<publisher-loc>Crete</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>), <fpage>57</fpage>&#x02013;<lpage>69</lpage></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W.</given-names></name></person-group> (<year>1992</year>). <article-title>Random texts exhibit Zipf's-law-like word frequency distribution</article-title>. <source>IEEE Trans. Inf. Theory</source>
<volume>38</volume>, <fpage>1842</fpage>&#x02013;<lpage>1845</lpage>
<pub-id pub-id-type="doi">10.1109/18.165464</pub-id><?supplied-pmid 20231884?><pub-id pub-id-type="pmid">20231884</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>K.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name></person-group> (<year>2005</year>). <article-title>Visual working memory for briefly presented scenes</article-title>. <source>J. Vis</source>. <volume>5</volume>, <fpage>650</fpage>&#x02013;<lpage>658</lpage>
<pub-id pub-id-type="doi">10.1167/5.7.5</pub-id><?supplied-pmid 16232000?><pub-id pub-id-type="pmid">16232000</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loftus</surname><given-names>G. R.</given-names></name><name><surname>Mackworth</surname><given-names>N. H.</given-names></name></person-group> (<year>1978</year>). <article-title>Cognitive determinants of fixation location during picture viewing</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>4</volume>, <fpage>565</fpage>&#x02013;<lpage>572</lpage>
<pub-id pub-id-type="doi">10.1037/0096-1523.4.4.565</pub-id><?supplied-pmid 722248?><pub-id pub-id-type="pmid">722248</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacEvoy</surname><given-names>S. P.</given-names></name><name><surname>Epstein</surname><given-names>R. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Constructing scenes from objects in human occipitotemporal cortex</article-title>. <source>Nat. Neurosci</source>. <volume>14</volume>, <fpage>1323</fpage>&#x02013;<lpage>1329</lpage>
<pub-id pub-id-type="doi">10.1038/nn.2903</pub-id><?supplied-pmid 21892156?><pub-id pub-id-type="pmid">21892156</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>M. L.</given-names></name><name><surname>Palmeri</surname><given-names>T. J.</given-names></name></person-group> (<year>2010</year>). <article-title>Modeling categorization of scenes containing consistent versus inconsistent objects</article-title>. <source>J. Vis</source>. <volume>10</volume>, <fpage>1</fpage>&#x02013;<lpage>11</lpage>
<pub-id pub-id-type="doi">10.1167/10.3.11</pub-id><?supplied-pmid 20377288?><pub-id pub-id-type="pmid">20377288</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>S. C.</given-names></name><name><surname>Eckstein</surname><given-names>M. P.</given-names></name></person-group> (<year>2011</year>). <article-title>Object co-occurrence serves as a contextual cue to guide and facilitate visual search in a natural viewing environment</article-title>. <source>J. Vis</source>. <volume>11</volume>, <fpage>1</fpage>&#x02013;<lpage>16</lpage>
<pub-id pub-id-type="doi">10.1167/11.9.9</pub-id><?supplied-pmid 21856869?><pub-id pub-id-type="pmid">21856869</pub-id></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>G. A.</given-names></name></person-group> (<year>1995</year>). <article-title>WordNet: a lexical database for English</article-title>. <source>Commun. ACM</source>
<volume>38</volume>, <fpage>39</fpage>&#x02013;<lpage>41</lpage>
<pub-id pub-id-type="doi">10.1145/219717.219748</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Minsky</surname><given-names>M.</given-names></name></person-group> (<year>1975</year>). <article-title>A framework for representing knowledge</article-title>, in <source>The Psychology of Computer Vision</source>, ed <person-group person-group-type="editor"><name><surname>Winston</surname><given-names>P. H.</given-names></name></person-group> (<publisher-loc>New York NY</publisher-loc>: <publisher-name>McGraw Hill</publisher-name>), <fpage>211</fpage>&#x02013;<lpage>277</lpage></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neider</surname><given-names>M. B.</given-names></name><name><surname>Zelinsky</surname><given-names>G. J.</given-names></name></person-group> (<year>2006</year>). <article-title>Scene context guides eye movements during visual search</article-title>. <source>Vision Res</source>. <volume>46</volume>, <fpage>614</fpage>&#x02013;<lpage>621</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2005.08.025</pub-id><?supplied-pmid 16236336?><pub-id pub-id-type="pmid">16236336</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neider</surname><given-names>M. B.</given-names></name><name><surname>Zelinsky</surname><given-names>G. J.</given-names></name></person-group> (<year>2008</year>). <article-title>Exploring set size effects in scenes: identifying the objects of search</article-title>. <source>Vis. Cogn</source>. <volume>16</volume>, <fpage>1</fpage>
<pub-id pub-id-type="doi">10.1080/13506280701381691</pub-id><?supplied-pmid 20698473?><pub-id pub-id-type="pmid">20698473</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>). <article-title>Modeling the shape of the scene: a holistic representation of the spatial envelope</article-title>. <source>Int. J. Comput. Vis</source>. <volume>42</volume>, <fpage>145</fpage>&#x02013;<lpage>175</lpage>
<pub-id pub-id-type="doi">10.1023/A:1011139631724</pub-id><?supplied-pmid 16387345?><pub-id pub-id-type="pmid">16387345</pub-id></mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <article-title>The role of context in object recognition</article-title>. <source>Trends Cogn. Sci</source>. <volume>11</volume>, <fpage>520</fpage>&#x02013;<lpage>527</lpage>
<pub-id pub-id-type="doi">10.1016/j.tics.2007.09.009</pub-id><?supplied-pmid 18024143?><pub-id pub-id-type="pmid">18024143</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>B. A.</given-names></name><name><surname>Field</surname><given-names>D. J.</given-names></name></person-group> (<year>1996</year>). <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>
<volume>381</volume>, <fpage>607</fpage>&#x02013;<lpage>609</lpage>
<pub-id pub-id-type="doi">10.1038/381607a0</pub-id><?supplied-pmid 8637596?><pub-id pub-id-type="pmid">8637596</pub-id></mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>S. E.</given-names></name></person-group> (<year>1975</year>). <article-title>The effects of contextual scenes on the identification of objects</article-title>. <source>Mem. Cogn</source>. <volume>3</volume>, <fpage>519</fpage>&#x02013;<lpage>526</lpage>
<pub-id pub-id-type="doi">10.3758/BF03197524</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkes</surname><given-names>L.</given-names></name><name><surname>Lund</surname><given-names>J.</given-names></name><name><surname>Angelucci</surname><given-names>A.</given-names></name><name><surname>Solomon</surname><given-names>J. A.</given-names></name><name><surname>Morgan</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Compulsory averaging of crowded orientation signals in human vision</article-title>. <source>Nat. Neurosci</source>. <volume>4</volume>, <fpage>739</fpage>&#x02013;<lpage>744</lpage>
<pub-id pub-id-type="doi">10.1038/89532</pub-id><?supplied-pmid 11426231?><pub-id pub-id-type="pmid">11426231</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname><given-names>M. C.</given-names></name></person-group> (<year>1976</year>). <article-title>Short-term conceptual memory for pictures</article-title>. <source>J. Exp. Psychol. Hum. Learn. Mem</source>. <volume>2</volume>, <fpage>509</fpage>&#x02013;<lpage>522</lpage>
<pub-id pub-id-type="doi">10.1037/0278-7393.2.5.509</pub-id><?supplied-pmid 1003124?><pub-id pub-id-type="pmid">1003124</pub-id></mixed-citation></ref><ref id="B80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prasada</surname><given-names>S.</given-names></name><name><surname>Ferenz</surname><given-names>K.</given-names></name><name><surname>Haskell</surname><given-names>T.</given-names></name></person-group> (<year>2002</year>). <article-title>Conceiving of entities as objects and as stuff</article-title>. <source>Cognition</source>
<volume>83</volume>, <fpage>141</fpage>&#x02013;<lpage>165</lpage>
<pub-id pub-id-type="doi">10.1016/S0010-0277(01)00173-1</pub-id><?supplied-pmid 11869722?><pub-id pub-id-type="pmid">11869722</pub-id></mixed-citation></ref><ref id="B81"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Quattoni</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>Recognizing indoor scenes</article-title>. in <source>IEEE Transactions on Computer Vision and Pattern Recognition (CVPR)</source>, (<publisher-loc>Miami, FL</publisher-loc>). <?supplied-pmid 23996589?></mixed-citation></ref><ref id="B82"><mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Rao</surname><given-names>R.</given-names></name><name><surname>Olshaussen</surname><given-names>B.</given-names></name><name><surname>Lewicki</surname><given-names>M.</given-names></name></person-group> (eds.). (<year>2002</year>). <source>Probabilistic Models of the Brain: Perception and Neural Function</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name></mixed-citation></ref><ref id="B83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renninger</surname><given-names>L. W.</given-names></name><name><surname>Malik</surname><given-names>J.</given-names></name></person-group> (<year>2004</year>). <article-title>When is scene indentification just texture recognition?</article-title>
<source>Vision Res</source>. <volume>44</volume>, <fpage>2301</fpage>&#x02013;<lpage>2311</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2004.04.006</pub-id><?supplied-pmid 15208015?><pub-id pub-id-type="pmid">15208015</pub-id></mixed-citation></ref><ref id="B85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname><given-names>D.</given-names></name></person-group> (<year>1994</year>). <article-title>The statistics of natural images</article-title>. <source>Network</source>
<volume>5</volume>, <fpage>517</fpage>&#x02013;<lpage>548</lpage>
<pub-id pub-id-type="doi">10.1088/0954-898X/5/4/006</pub-id></mixed-citation></ref><ref id="B86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>B.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Freeman</surname><given-names>W.</given-names></name></person-group> (<year>2008</year>). <article-title>LabelMe: a database and web-based tool for image annotation</article-title>. <source>Int. J. Comput. Vis</source>. <volume>77</volume>, <fpage>157</fpage>&#x02013;<lpage>173</lpage>
<pub-id pub-id-type="doi">10.1007/s11263-007-0090-8</pub-id></mixed-citation></ref><ref id="B87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>N. C.</given-names></name><name><surname>Movshon</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <article-title>In praise of artifice</article-title>. <source>Nat. Neurosci</source>. <volume>8</volume>, <fpage>1647</fpage>&#x02013;<lpage>1650</lpage>
<pub-id pub-id-type="doi">10.1038/nn1606</pub-id><?supplied-pmid 16306892?><pub-id pub-id-type="pmid">16306892</pub-id></mixed-citation></ref><ref id="B88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O.</given-names></name><name><surname>Simoncelli</surname><given-names>E. P.</given-names></name></person-group> (<year>2001</year>). <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nat. Neurosci</source>. <volume>4</volume>, <fpage>819</fpage>&#x02013;<lpage>825</lpage>
<pub-id pub-id-type="doi">10.1038/90526</pub-id><?supplied-pmid 11477428?><pub-id pub-id-type="pmid">11477428</pub-id></mixed-citation></ref><ref id="B89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>1994</year>). <article-title>From blobs to boundary edges: evidence for time- and spatial-scale-dependent scene recognition</article-title>. <source>Psychol. Sci</source>. <volume>5</volume>, <fpage>195</fpage>&#x02013;<lpage>200</lpage>
<pub-id pub-id-type="doi">10.1111/j.1467-9280.1994.tb00500.x</pub-id></mixed-citation></ref><ref id="B90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>E. P.</given-names></name><name><surname>Olshausen</surname><given-names>B. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Natural image statistics and neural representation</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>24</volume>, <fpage>1193</fpage>&#x02013;<lpage>1216</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><?supplied-pmid 11520932?><pub-id pub-id-type="pmid">11520932</pub-id></mixed-citation></ref><ref id="B90a"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sivic</surname><given-names>J.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>Video Google: a text retrieval approach to object matching in videos</article-title>, in <source>Proceedings of the Ninth IEEE International Conference on Computer Vision</source>, <volume>Vol. 2</volume> (<publisher-loc>Washington, DC</publisher-loc>), <fpage>1470</fpage>&#x02013;<lpage>1477</lpage>
<pub-id pub-id-type="doi">10.1109/ICCV.2003.1238663</pub-id><?supplied-pmid 19229077?><pub-id pub-id-type="pmid">19229077</pub-id></mixed-citation></ref><ref id="B91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname><given-names>B. W.</given-names></name></person-group> (<year>2007</year>). <article-title>The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor biases and image feature distributions</article-title>. <source>J. Vis</source>. <volume>7</volume>, <fpage>1</fpage>&#x02013;<lpage>17</lpage>
<pub-id pub-id-type="doi">10.1167/7.14.4</pub-id><?supplied-pmid 18217799?><pub-id pub-id-type="pmid">18217799</pub-id></mixed-citation></ref><ref id="B92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname><given-names>B. W.</given-names></name><name><surname>Baddeley</surname><given-names>R. J.</given-names></name><name><surname>Gilchrist</surname><given-names>I. D.</given-names></name></person-group> (<year>2005</year>). <article-title>Visual correlates of fixation selection: effects of scale and time</article-title>. <source>Vision Res</source>. <volume>45</volume>, <fpage>643</fpage>&#x02013;<lpage>659</lpage>
<pub-id pub-id-type="doi">10.1016/j.visres.2004.09.017</pub-id><?supplied-pmid 15621181?><pub-id pub-id-type="pmid">15621181</pub-id></mixed-citation></ref><ref id="B93"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Efros</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>An unbiased look at dataset bias</article-title>. in <source>IEEE Transactions on Computer Vision and Pattern Recognition (CVPR)</source>, (<publisher-loc>Colorado Springs, CO</publisher-loc>) <fpage>1521</fpage>&#x02013;<lpage>1528</lpage></mixed-citation></ref><ref id="B94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Depth estimation from image structure</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>24</volume>, <fpage>1</fpage>&#x02013;<lpage>13</lpage>
<pub-id pub-id-type="doi">10.1109/TPAMI.2002.1033214</pub-id></mixed-citation></ref><ref id="B95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>Statistics of natural image categories</article-title>. <source>Network</source>
<volume>14</volume>, <fpage>391</fpage>&#x02013;<lpage>412</lpage>
<pub-id pub-id-type="doi">10.1088/0954-898X/14/3/302</pub-id><?supplied-pmid 12938764?><pub-id pub-id-type="pmid">12938764</pub-id></mixed-citation></ref><ref id="B96"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A.</given-names></name><name><surname>Sinha</surname><given-names>P.</given-names></name></person-group> (<year>2001</year>). <article-title>Statistical context priming for object detection</article-title>. in <source>IEEE Proceedings of the International Conference in Computer Vision</source>, <volume>Vol. 1</volume>, (<publisher-loc>Vancouver, CA</publisher-loc>), <fpage>763</fpage>&#x02013;<lpage>770</lpage></mixed-citation></ref><ref id="B97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>A.</given-names></name><name><surname>Kahneman</surname><given-names>D.</given-names></name></person-group> (<year>1974</year>). <article-title>Judgment under uncertainty: heuristics and biases</article-title>. <source>Science</source>
<volume>185</volume>, <fpage>1124</fpage>&#x02013;<lpage>1131</lpage>
<pub-id pub-id-type="doi">10.1126/science.185.4157.1124</pub-id><?supplied-pmid 17835457?><pub-id pub-id-type="pmid">17835457</pub-id></mixed-citation></ref><ref id="B98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S.</given-names></name><name><surname>Vidal-Naquet</surname><given-names>M.</given-names></name><name><surname>Sali</surname><given-names>E.</given-names></name></person-group> (<year>2002</year>). <article-title>Visual features of intermediate complexity and their use in classification</article-title>. <source>Nat. Neurosci</source>. <volume>5</volume>, <fpage>682</fpage>&#x02013;<lpage>687</lpage>
<pub-id pub-id-type="doi">10.1038/nn870</pub-id><?supplied-pmid 12055634?><pub-id pub-id-type="pmid">12055634</pub-id></mixed-citation></ref><ref id="B99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>J. H.</given-names></name><name><surname>Ruderman</surname><given-names>D. L.</given-names></name></person-group> (<year>1998</year>). <article-title>Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex</article-title>. <source>Proc. R. Soc. B Biol. Sci</source>. <volume>265</volume>, <fpage>2315</fpage>&#x02013;<lpage>2320</lpage>
<pub-id pub-id-type="doi">10.1098/rspb.1998.0577</pub-id><?supplied-pmid 9881476?><pub-id pub-id-type="pmid">9881476</pub-id></mixed-citation></ref><ref id="B100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R.</given-names></name><name><surname>Koch</surname><given-names>C.</given-names></name></person-group> (<year>2003</year>). <article-title>Competition and selection during visual processing of natural scenes and objects</article-title>. <source>J. Vis</source>. <volume>3</volume>, <fpage>75</fpage>&#x02013;<lpage>85</lpage>
<pub-id pub-id-type="doi">10.1167/3.1.8</pub-id><?supplied-pmid 12678627?><pub-id pub-id-type="pmid">12678627</pub-id></mixed-citation></ref><ref id="B101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vickery</surname><given-names>T. J.</given-names></name><name><surname>King</surname><given-names>L.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name></person-group> (<year>2005</year>). <article-title>Setting up the target template in visual search</article-title>. <source>J. Vis</source>. <volume>5</volume>, <fpage>81</fpage>&#x02013;<lpage>92</lpage>
<pub-id pub-id-type="doi">10.1167/5.1.8</pub-id><?supplied-pmid 15831069?><pub-id pub-id-type="pmid">15831069</pub-id></mixed-citation></ref><ref id="B102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>V&#x000f5;</surname><given-names>M. L.</given-names></name><name><surname>Henderson</surname><given-names>J. M.</given-names></name></person-group> (<year>2009</year>). <article-title>Does gravity matter? Effects of semantic and syntactic inconsistencies on the allocation of attention during scene perception</article-title>. <source>J. Vis</source>. <volume>9</volume>, <fpage>1</fpage>&#x02013;<lpage>15</lpage>
<pub-id pub-id-type="doi">10.1167/9.3.24</pub-id><?supplied-pmid 19757963?><pub-id pub-id-type="pmid">19757963</pub-id></mixed-citation></ref><ref id="B103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>V&#x000f5;</surname><given-names>M. L.-H.</given-names></name><name><surname>Henderson</surname><given-names>J. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Object-scene inconsistencies do not capture gaze: evidence from the flash-preview moving-window paradigm</article-title>. <source>Attent. Percept. Psychophys</source>. <volume>73</volume>, <fpage>1742</fpage>&#x02013;<lpage>1753</lpage>
<pub-id pub-id-type="doi">10.3758/s13414-011-0150-6</pub-id><?supplied-pmid 21607814?><pub-id pub-id-type="pmid">21607814</pub-id></mixed-citation></ref><ref id="B104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>V&#x000f5;</surname><given-names>M. L.-H.</given-names></name><name><surname>Wolfe</surname><given-names>J. M.</given-names></name></person-group> (<year>2013</year>). <article-title>Differential electrophysiological signatures of semantic and syntactic scene processing</article-title>. <source>Psychol. Sci</source>. <volume>24</volume>, <fpage>1816</fpage>&#x02013;<lpage>1823</lpage>
<pub-id pub-id-type="doi">10.1177/0956797613476955</pub-id><?supplied-pmid 23842954?><pub-id pub-id-type="pmid">23842954</pub-id></mixed-citation></ref><ref id="B105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>J. M.</given-names></name><name><surname>Alvarez</surname><given-names>G. A.</given-names></name><name><surname>Rosenholtz</surname><given-names>R.</given-names></name><name><surname>Kuzmova</surname><given-names>Y. I.</given-names></name><name><surname>Sherman</surname><given-names>A. M.</given-names></name></person-group> (<year>2011a</year>). <article-title>Visual search for arbitrary objects in real scenes</article-title>. <source>Attent. Percept. Psychophys</source>. <volume>73</volume>, <fpage>1650</fpage>&#x02013;<lpage>1671</lpage>
<pub-id pub-id-type="doi">10.3758/s13414-011-0153-3</pub-id><?supplied-pmid 21671156?><pub-id pub-id-type="pmid">21671156</pub-id></mixed-citation></ref><ref id="B106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>J. M.</given-names></name><name><surname>V&#x000f5;</surname><given-names>M. L.-H.</given-names></name><name><surname>Evans</surname><given-names>K. K.</given-names></name><name><surname>Greene</surname><given-names>M. R.</given-names></name></person-group> (<year>2011b</year>). <article-title>Visual search in scenes involves selective and nonselective pathways</article-title>. <source>Trends Cogn. Sci</source>. <volume>15</volume>, <fpage>77</fpage>&#x02013;<lpage>84</lpage>
<pub-id pub-id-type="doi">10.1016/j.tics.2010.12.001</pub-id><?supplied-pmid 21227734?><pub-id pub-id-type="pmid">21227734</pub-id></mixed-citation></ref><ref id="B106a"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>J.</given-names></name><name><surname>Hays</surname><given-names>J.</given-names></name><name><surname>Ehinger</surname><given-names>K. A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>SUN database: Large-scale scene recognition from abbey to zoo</article-title>, in <source>Presented at the 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, (<publisher-loc>San Francisco, CA</publisher-loc>), <fpage>3485</fpage>&#x02013;<lpage>3492</lpage>
<pub-id pub-id-type="doi">10.1109/CVPR.2010.5539970</pub-id></mixed-citation></ref><ref id="B107"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zetzsche</surname><given-names>C.</given-names></name><name><surname>Barth</surname><given-names>E.</given-names></name><name><surname>Wegmann</surname><given-names>B.</given-names></name></person-group> (<year>1993</year>). <article-title>The importance of intrinsically two-dimensional image features in biological vision and picture coding</article-title>, in <source>Digital Images and Human Vision</source>, ed <person-group person-group-type="editor"><name><surname>Watson</surname><given-names>A. B.</given-names></name></person-group> (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>109</fpage>&#x02013;<lpage>138</lpage></mixed-citation></ref></ref-list><app-group><app id="A1"><title>Appendix A: raw labels (SIC)</title><p>&#x0003c;blank&#x0003e;, &#x0201c;?object,&#x0201d; abinet, adding machine, advert, aerial, aerial building, air conditining, air conditioner, air conditioning, air conditioning occluded, air filter, air vent, airco, airplane, alarm, alarm clock, alarmclock, alcove, all, American flag, animal, animal seal, animals, antique bureau, apple, apples, arcade, arcade occluded, arch, arches, archway, arearug, arid ground, arm chair, armchair, armchair back, armchair part, amchair top, armoir, art, art piece, art?, artichokes, ashtray ash tray, attic, automan, avenue, awning, awning crop, awning occluded, awning tidy up, back of chair, back wall, backpack, badge, bag, balcany occluded, balcony, balcony crop, balcony occluded, bale, balustrade, bandstand, banister, bank, bar, barbeque, barrier occluded, base board, basket, basket food, basket fruits, basket of fruit, basket of magazines, basket of towels, basket table, basket of brushes, baskets, bath, bath faucet, bath mat, bath tub, bath robe, bathtub, bathtub plug, beach, beam, bed, bed part, bed posts, bed skirt, bedskirt, bedspread, bell, bell pepper, bell tower, bench, bench occluded, bib, biclist occluded, bicycle, bicycle occluded, bicyclist, bicyclists, bidet, bidet faucet, bilding, bin, bin occluded, binder binders, bird, bird figurine, bird occluded, birdcade, blackboard, blancket, blanket, blender, blind, blinds, block, block of cheese, blocks, bluildings, boad, board, board games, boarder, boat, boat crop, boat cropped, boat decoration, boat occluded, boats, book, book case, book shelf, book shelves, bookcase, books, books on shelf, bookshelf, bookshelves, boooks, boot, boots, border, bottl;e, bottle, bottle top, bottles, bottom bunk, bottom of chair, bottom of chairs, bouldings, bouldings occluded, bouquet, bouquet flowers, bowel, bowl, bowl of apples, bowl of fruit, bowl of popcorn, bowl of strawberries, bowl of vegitables, bowl with food, bowl with fruit, bowl with vegis, bowls, box, box contants, box occluded, box?, boxes, braided garlic, branch, branches, branchs, brand name, brand name crop, brand name occluded, bread, bread tray, brick fireplace, brick wall, bridge, bridge crop, bridge handrail, bridge occluded, briefcase, broom, brushes, brush, brushes, bucket, buffet, buggy, buiding, buiding occluded, buidings occluded, buildig occluded, buildin, buildin occluded, building, building aerial, building crop, building fa&#x000e7;ade, building occluded, building occluded, building skyscraper, buildingl occluded, buildings, buildings crop, buildings occluded, buildings occludeds, buildingsoccluded, buildins, buildins occluded, buildintgs occluded, buildling, buildlings occluded, bulletin board, buoy, bus, bus stop, bus occluded, bus stop, bus stop occluded, bushes, business card, bus occluded, cabin, cabin occluded, cabinet, cabinet door, cabinet dresser, cabinets, cabinets, cabintet shelf, cake, cake dish, cake stand, calander, calendar, calender, can, canal water, candel, candle, candle holder, candle stand, candle stick, candles, canister, canister with brushes, canister with utencils, canisters, cannon, canoe, canopy, cans, captus, captus crop, car, car crop, car side, car az90deg, car crop, car frontal, car frontal occluded, car occluded, car ocluded, car rear, car rear az90deg, car rear occluded, car rear side, car rear side crop, car rear side crop, car side, car side az0deg, car side az180deg, car side crop, car side occlude, car side occluded, car side rear, car side rear crop, car sides, car_back, car_front, car_left, car_right, car_top_back, car_top_fornt, car_top_front, card, carpet, carpet floor, carrots, cars, cars occluded, cars side, cars side crop, cars side occluded, cars sides, cart, case, cassettes, castle, cat, cd, cd's, cdoor, cds, ceiing light, ceililng, ceiling, ceiling fan, ceiling lamp, ceiling lamps, ceiling light, ceiling molding, ceiling tile, ceiling vent, ceiling lamp, ceilng light, celing, cell phone, centerpiece, centra reservation, central reservation, ceramic rooster, certain, certificate, chair, chair back, chair bottom, chair crop, chair leg, chair legs, chair occluded, chair part, chair seat, chair top, chair wheel, chairs, chaise lounge, chandalier, chandelier, chandelier, chandlier, changing table, channel, char, cheese, chess board, chessboard, chest, chiar, child walking, chimney, china, china cabinet, china hutch, china plate, cigarettes, city, cliff, clip board, clock, clock occluded, closet, closets, cloth, clothes, clothes hamper, cloths, cloud, clouds, coaster, coat, coat rack, coatrack, coffee maker, coffee pot, coffee table, column, column, column occluded, columns, comforter, computer, computer monitor, computer screen, computer tower, computers, conference table, cooler, copier, cords, couch, counter, counter top, counter, cove, covered balcony, covered balcony occluded, cow, cows, cpboard, crane, crane occluded, crib, cross, crosswalk, cubbord, cubby, cubical, cubicle, culumn, cup, cup and saucer, cup holders, cup with flower, cupbard, cupboard, cups, curb, curtain, curtain rod, curtains, cushion, cutting board, cutting board island, cutting board with vegitables, cyclist, dam, decoration, decorations, decorative ball, decorative balls, decorative boat, decorative bowl, decorative box, decorative fish, decorative kimono, decorative mask, decorative mirror frame, decorative molding, decorative object, decorative objects, decorative pillow, decorative plat, decorative plate, decorative pots, decorative tree, decorative urn, decorative wall hanging, decoratvie plate, deoderant, derer ground, desert, desert field, desert ground, desk, desk calendar, desk calender, desk divider, desk lamp, desk mat, desk organizer, desk separator, desks, digital watch, dining table, disc, dish, dish rack, dish towel, dish towels, dishes, dishrack, dishwaher, dishwasher, disks, display case, display stand, dock, dog, dog crop, doily, doll, dolphin, dome, dome crop, dome occluded, door, door crop, door entrance, door frame, door knob, door mat, door occluded, door_, doorframe, doorpart, doors, doorway, double door, double door crop, double door occluded, double window occluded, drape, drapes, drawer, drawer nob, drawers, drawyers, dresser, dressing screen, dressor, dried flowers, dried plant, drinking fountain, driver, dry earase board, dry earaser, dry erase board, dryer, dune, eagle, egg crate, eggplant, eilling lamp, electric mixer, electrical outlet, elevator door, elf, embankment, emplem, enclave, end board, end table, entertainment center, entrance, entrance occluded, entry, entry-phone, entryway, envelope, envelopes, equipement, esplanade, estate, exit sign, external driver, external drivers, eye, falg, fall branch, fan, faucet, faucit, faucst, fax machine, fence, fence crop, fence occluded, fences crops, fern, ferns, field, field desert, field flowers, field grass, figurine, figurines, file, file box, file cabinet, file cabinets, file organizer, files, filing cabinet, fining cabinet_, filing cabinets, fire, fire alarm, fire escape, fire extinguisher, fire hydrant, fire place, fire sprinkler, firehose, fireplace, fireplace screen, firewood, fish tank, flag, fip flop, fllor, flock, floor, floor carpet, floor marble, floor, carpet; floor_, floral centerpiece, flower, flower arrangement, flower in vase, flower pot, flowers, flowers in pot, flowers in vase, flute, flyer, flyers, fog banck, fog bank, folder, folders, food, food on plate, foot board, footboard, footbridge, footrest, forest, forest mountain, fork, forks, fountain, fountain crop, fountain occluded, fp screen, frame, framed mirror, framed picture, frozen over river, fruit, fruit bowl, fruit in bowl, fruit plate, fruit stand, fruits, fruits bowl, fruits plate, frying pan, funicular (railway), garage, garage door, garage door occluded, garages, garden, garland, garlic, garment bag, gas pump, gas station, gate occluded, glass, glass bowl, glass cupboard, glass door, glass doors, glass piece, glass shelf, glass table, glass wall, glasses, globe, goblet, goblets, goose, goose occluded, gooses, grandfather clock, grapes, grass, grass field, green field, green peppers, greenhouse, grill, grille, grille occluded, groound, groun grass, ground grass crop, ground snow-covered, groung grass, hair, hair brush, hall, hand rail, hand towel, hand vaccum, handle, handrail, hanger, hanging bird, hanging fish, hanging hat, hanging instrument, hanging lamp, hanging light, hanging monitor, hanging pan, hanging pans, hanging patches, hanging pitcher, hanging plant, hanging plate, hanging pot, hanging pot rack, hanging rack, hanging rug, hanging toy, hanging utencils, hanging utensils, hanging utentcil rack, hanging wall flower, harbor, hat, hay bale, head board, head stand, headboard, headphones, hearth, heater, hedge, highway, hill, hill urban, hills, hold back, hood, horse, hot pad, house, house occluded, house crop, house in ruins, house occluded, houses, houses crop, houses crops, houses occluded, houses occludeds, human, hung out, hutch, hydrant, ice bucket, idol, ilver ware, image frame, in box, industry, inset ceiling light, inset ceiling lights, instrument, intercom, ipod, iron, island, isle, jar, jars, jet, jetski, jetty, jewelry box, joist, joists, jug, junk, kettle, key board, key board shelf, key pad, keyboard, kiosk, kite, Kleenex, Kleenex box, knife, knife holder, knife holdrer, knife set, knives, knobs, ladder, ladle, lake, lake water, laminating machine, lamp, lamp part, lamp shade, lamps, lampshade, land, lap top, laptop, large bowl, large window, lava, leaf, leaves, leaves tree, ledge, lemon, letter bin, lettuce, lichen, light, light fixture, light switch, lighter, lighthose, lighthouse, lights, lignthouse, line persons, liquor bottle, litter bin, litter bin crop, little bear, lodge, loft, logs, lotion, lots of chairs, lower cabinets, machine, machines, magazine, magazine holder, magazine rack, magazines, magnifier, magnifying glass, mailbox, mailboxes, make up case, man, manhole, mantle, map, markers, massage table, massager, mast, mat, matt, mattress, message board, message board_, metal door, metal door occluded, metal shutters crop, microphone, microwave, might stand table, mill, mini blinds, mirror, mirror extender, misc., mixer, molding, monitor, monitor stand, monitor_display, monitors, monitpr, monkey, monkey occluded, monolith, monolith crop, monument, moon, motorbikde occluded, motorbike, motorbike crop, motorbike occluded, motorbike side, motorbike side crop, motorbike side occluded, motorbikes side, motorclyclist, motorcyclist, motorcyclist crop, mountain, mountain pass, mountains, mountainside, mountan, mouse, mouse pad, mouse stand, moutain, mouth, movie screen, muffin, mug, mugs, napkin, napkin in glass, napkins, neck pillow, news stand, news-stand, newspaper, night stand, night stand cabinet, night stand dresser, night stand part, night stand table, nightstand, notebook, notebooks, nothing, notice, nozzle, nutcracker, obelisk, object, object, objetc, objectds, objects, occluded, occluded sky, ocean, ocean water, oject, onion, orange, ottamon, ottoman, ounter top, outlet, overhead projector, pad, paht, pail, painting, paintings, paitings, pallet, palm tree, palm pilot, palm tree, palm tree crop, palm tree cropped palm tree occlude, palm tree occluded, palm tree trunk, palm trees, palms trees, pan, pane, pane crop, pane occluded, panel, panelling, panels, pants, paper, paper filer, paper roll, paper sheet, paper towel, paper towel holder, paper towels, paper weight, papers, park, parking lot, parking meter, parking place, parquing door, parquing meter, marquing meter occluded, pass window, passway, pasta, path, pbject, pear, pears, pedestal, pedestel, pedestrian street, pen, pen box, pen holde, pen holder, pen set, pencil, pencil cup, pencils, pens, people, people riding, people sitting, people walking, pepper mill, pepper shaker, perkalator, person standing occluded, person, person standing, person walking, person boy, person boy standing, person crop, person cyclist, person in line, person man back crop, person man sitting, person man standing, person man walking, person man walking occluded, person occluded, person riding, person setanding, person sitting, person sitting cropped, person sitting occluded, person skiing, person skiing crop, person skiing occluded, person stading, person stangig, person standing crop, person standing kid, person standing occluded, person sweping, person swiming, person swimming, person waling, person walkin, person walking, person walking crop, person walking occluded, person walking ocluded, person wallking, person wise, person wise back, person wise back occluded, person wise backoccluded, person wise crop, person wise in profile, person wise occluded, person woman, person woman sitting, person woman standing, person woman walking, person working, persons rowing, persons standing, persons walking, peson standing, peson walking, pesrson walking, pestal and mortar, pet bowl, phone, photocopier, piano, picture, picture frame, pictures, pie, pig statue, pillar, pillow, pillows, pipe, pitcher, placard, place mat, place matt, placemat, plain, plams trees, plan pot, plank, plank occluded, plank, plant, plant box, plant box occluded, plant crop, plant fern, plant grapevine, plant in stand, plant in vase, plant occluded, plant pot, plant pot occluded, plant stand, plant tree, plants, plants fern, plants ferns, plaque, plate, plate and bowl, plate of fruit, plates, platform, platter, plug, plumbing, pneumatic tire, podium, pole, pole crop, pole occluded, poles, pond, porch, porch occluded, portfolio, portico, post it note, post it notes, poster, poster board, poster crop, poster occluded, posters pole, pot, pot holder, pot plant, pot plant crop, pot plant occluded, pots, potted flowers, potted plant, power cord, pperson driving occluded, precipice, printer, prjection screen, prjector, projection screen, projector, prson standing, prson walking, puddle, puffy object, pulley, purse, pylon, pylon occluded, qall, quay, quilt, rack, rack of fruit, radiator, radio, radio alarm clock, rail, railing, railings, raill, railroad track, railway, rainbow, range hood, recycling bins, red light, ree, reflection, refridgerator, refrigerator, refuge, remote, remote control, revolving doors, rig, river, river bed, river side, river water, riverside, rives water, road, road highway, road traffic, roads, roamn shade, roasting pan, robe, rock, rock cropped, rocke wall, rocks, rocky ground, rocky hill, rocky mountain, rocky mountains, rocky moutain, rocky plain, roks, rolling pin, roman shade, roof, roof occluded, room label, rooster, rooster figurine, rootes, round table, row of chairs, row of desks, rubber duckie, rug, ruin, runner, s, sack, sailboat, sailing boat, sair railing, salt shaker, sand, sand beach, sands, caofa, satellite dish, sauce pot, saucer and bowl, scaffolding, scaffolding crop, scale, scanner, sconce, screen, scrub brush, scrubland, sculpture, sculpture crop, sculpture occluded, sculpyure, sea, sea water, sea beach, sea water, seagull, seashell, serving tray, sewer, shade, shades, shadow, shampoo, shedders, sheep, shelf, shell-bowl of popuri, shelves, ship, ship occluded, shop window, shop window crop, shop window occluded, shower, shower curtain, shower curtain rod, shower door, shower faucet, shower head, shower nozzle, shrub, shrub cropped, shrubs, shrubs crop, shrubs occluded, shudder, shutter, side, side rail, side tabel, side table, side walk, sideboard, sidetable, sidewalk, sidewalk caf&#x000e9;, sig, sign, sign crop, sign occluded, signes, signs, siky, silver ware, silverware, sing, sink, sink faucet, site office, sk, skier, skier crop, skly, sky, sky light, sky occluded, sky sunset, skyscraper occluded, skyscrape, skyscraper, skyscraper building, askyscraper crop, skyscraper occluded, skyscraperoccluded, skyscrapers, skyscrapers occluded, skyscrapr occluded, skyscrapre occluded, skyscrpaer occluded, skyscrper occluded, skysscraper occluded, slated wooden panel, sled, sleepers, sleeping robe, sliding door, sliding door crop, sliding glass door, slipper, slippers, slope, sloped ceiling, small bowl, small plate, small rug, small table, small table part, small vase, smoke alarm, smoke detector, snow, snow covered, snow covered ground, snow covered mountain pass, snow covered plain, snow covered road, snow covered valley, snow land, snowly mountain, snowy covered field, snowy ground, snowy hill, snowy mountain, snowy mountain pass, snowy plain, snowy road, snowy trees, soap, soap bars, soap bottle, soap box, soap dish, soap dispenser, soap holder, soap on a rope, soaps, socket, sofa, sofa bed, sofa part, soil, soldier, sonwy mountain, sown field, space heater, spatulas, speacker, speaker, spice jar, spice rack, sping, sponge, spoon, spotlight, spray bottle, sprinkler, ssnowy mountain, stacj of papers, stack of books, stack of papers, stack of plates, staicase, stained-glass window, stainless steel back splash, stainless steel splash guard, stainless steel wall, stair board, stair railing, staircase, staircase occluded, stairs, stairway, stake, stand, stand occluded, stands, stapler, starage rack, starfish, station, statue, streetlight, step, steps, stero, stick, sticker, sticks, stone, stone ball, stone ball crop, stone occluded, stone vase with flowers, stones, stones wall, stool, storage box, storage rack, stov, stove, stove gaurd, stove hood, stove nob, stove sheild, stove top, streelight, street, street light, street lighting, street market, streetcar, streetlamp, streetlight, streetlight crop, streetlight occluded, streetlights, streetlilght, string, stump of tree, subway, suitcase, sun, sun occluded, sunflower field, sunflowers field, sunflowers field, sunset, supermarket, supplies, support beam, tabel occluded, table, table cloth, table lamp, table leg, table occluded, table part, table runner, table top, table with tablecloth, tablecloth runner, tableland, tables, tank, tanker occluded, tapestry, tea cup and saucer, tea kettle, tea pot, teapot, teddy bear, telephone, telephone booth, telephone box, telephone box occluded, television, television cabinet, television case, television screen, television stand, tence, terotauuko, terrace, terrace occluded, text, thermos, thermostat, tile, tile ?, tiled wall, tissue, tissue box, tissues, toast, toaster, toaster oven, toilet, toilet brush, toilet brush stand, toilet paper, toilet paper holder, toilet paper roll, toll gate, tomatoes, toolbox, tooth brush holder, tooth brushes, toothbruch holder, toothbrush, toothbrush in cup, toothbrush in jar, toothbrushes, toothpaste, tootlbrush, top bunk, top chair, top of chair, top wall, torch, towel, towel hanger, towel rack, towels, tower, tower crop, tower occluded, town, toy, toys, tp roll, track, track lighting, tractor, traffic lights, traffic light, traffic light frontal, traffic light side, traffic lights, traffic lights crop, traffic sign, trafficlights, trash, trash can, trash compactor, tray, tray table, tray with supplies, trays, tree, tree crop, tree cropped, tree cut down, tree in pot, tree leaves, tree occluded, tree top, tree trunk, tree trunk crop, tree trunk cropped, tree trunk fallen, tree trunk occluded, tree trunks, trees, trees cropped, trees occluded, trees occludeds, trees top, trophy, trres, truck, truck crop, truck frontal, truck frontal occluded, truck occluded, truck rear, truck rear occluded, truck side, truck side occluded, trucks occluded, truk, truk occluded, truk side occluded, trumpet, trunk, trunk tree fall, tub, tube, tuck occluded, tumbledown building, tunnel, tv, tv stand, typewriter, umbrella occluded, umbrellas, undergroth, undergrowth, urban plain, urban valley, urbanization, urinal, urn, utencil, utencil holder, utencil rack, utensil, utensils, valance, valley, van, van crop, van frontal, van occluded, van rear, van rear side, van side, van side crop, van side occluded, vanity, varehouse, vase, vase with flowers, vase with leaves, vase with plant, vases, vault, VCR, vegetable, vegetables, vegetation, vegitable, vegitables, vegitables in bowl, vegitables on a plate, vehicles, vent, vents, verge, vertical blinds, verticle blinds, viewpoint, vineyard, volcano, votive, wakk, wal, wall, wall boarder, wall border, wall clock, wall hang, wall hanging, wall lamp, wall light, wall mount, wall occluded, wall outlet, wall outlets, wall stitch, walls, wardrobe, wardrobe part, wash cloth, washcloths, washing machine, waste basket, water, water bottle, water bottles, water cooler, water fall, water fountain, water ocean, water pond, water river, water sea, waterfall, watering can, watermelon, wathervane, wave splash, waves, web cam, webcam, wheat field, wheelbarrow, wheelchair, wheels, white board, white out, whiteboard, whte board, widnow ledge, wind chime, windex, window, window ceiling, window crop, window frame, window ledge, window occluded, window pane, window seat, window shade, window shades, window shop, window shudders, window shutter, window sill, windows, windwo, wine bottle, wine cupboard, wine glas, wine glass, wine glasses, wine rack, wineglasses, wineglass, wineglasses, wire, wire rack, woman, woman walking, wood, wood beam, wood post, wooden ship, workstation, x, xx, xxx, zebra, zebra crossing, zebras.</p></app><app id="A2"><title>Appendix B: final region labels</title><p>Adding machine, advert, air conditioner, air filter, airplane, alarmclock, alcove, antenna, apple, arcade, arch, armchair, artichokes, ashtray, attic, awning, backpack, bag, balcony, bandstand, barbecue, base board, basket, bath, bath mat, bath plug, bathrobe, beach, beam, bed, bed posts, bedskirt, bedspread, bell, bell pepper, bench, bicycle, bidet, binder, bird, birdcage, blanket, blender, block, board games, boat, book, bookshelf, boot, bottle, bouquet, bowl, box, braided garlic, branch, bread, bridge, briefcase, broom, bucket, buffalo, buffet, buggy, building, bulletin board, buoy, bus, bus stop, bush, business cards, cabin, cabinet, cactus, cake, cake dish, cake stand, calendar, can, canal water, candle, candle holder, canister, cannon, canoe, canopy, car, card, carpet, carrots, case, cassettes, castle, cat, cave, cds, ceiling, ceiling fan, ceiling tile, cell telephone, centerpiece, certificate, chair, chaise lounge, chandelier, changing table, chart, cheese, chessboard, chest, chimney, china, china hutch, cigarettes, city, cliff, clip board, clock, closet, cloth, clothes, clothes hamper, cloud, coaster, coat, coat rack, coffee maker, coffee table, column, computer, computer monitor, cooler, counter, cow, crane, crib, cross, crosswalk, cubicle, cup, curb, curtain, curtain rod, cutting board, dam, decoration, deer, deodorant, desert, desk, desk divider, desk mat, desk organizer, dish, dish towel, dishrack, dishwasher, display case, dock, dog, doily, doll, dolphin, dome, door, door mat, drawer, dresser, dressing screen, drinking fountain, dryer, dune, eagle, egg crate, eggplant, electric mixer, electrical outlet, elevator door, emblem, end table, entertainment center, entrance, envelope, equipment, eraser, esplanade, estate, exit sign, external hard drive, fan, faucet, fax machine, fence, fern, field, file, file organizer, filing cabinet, fire, fire alarm, fire escape, fire extinguisher, fire hydrant, fire sprinkler, firehose, fireplace, fireplace screen, firewood, fish tank, flag, flock, floor, flower pot, flowers, fog, food, footboard, footrest, forest, fork, fountain, frame, fruit, fruit bowl, fruit stand, frying pan, garage, garage door, garden, garden, garland, garlic, garment bag, gas pump, gas station, gate, glass, glasses, globe, goblet, goose, grandfather clock, grapes, grass, greenhouse, grill, ground, hair brush, hall, hanger, harbor, hat, hay bale, headboard, headphones, hearth, heater, hedge, hill, horse, house, instrument, intercom, ipod, iron, island, jar, jetski, jetty, jewelry box, jug, kettle, keyboard, kiosk, kite, knife, knife set, knob, ladder, ladle, lake, laminating machine, lamp, lamp shade, land, laptop, lava, leaves, ledge, lemon, lettuce, lichen, light, light switch, lighter, lighthouse, lodge, loft, lotion, machine, magazine, magazine rack, magnifying glass, mailbox, make up case, manhole, mantle, map, markers, massage table, massager, mast, mat, mattress, median, microphone, microwave, mirror, mirror extender, molding, monitor stand, monkey, monolith, monument, moon, motorcycle, mountain, mountain pass, mouse, mouse pad, muffin, mug, napkin, news stand, newspaper, nightstand, notebook, nozzle, nutcracker, obelisk, object, ocean, onion, orange, ottoman, oven, overhead projector, painting, pallet, palm pilot, pan, panel, pants, paper, paper, roll, paper towels, paper towels holder, paper weight, park, parking lot, parking meter, parking place, pasta, path, pear, pedestal, pen, pen holder, pen set, pencil, pencil cup, pepper shaker, person, pestle and mortar, pet bowl, photocopier, piano, picture, pie, pillow, pipe, pitcher, placard, placemat, plank, plant, plant pot, plaque, plate, plateau, platform, platter, plumbing, podium, pole, pond, porch, portfolio, post it note, poster, pot, pot holder, pot rack, power cord, printer, projection screen, projector, puddle, pulley, purse, pylon, quay, quilt, rack, radiator, radio, railing, railroad track, railway, rainbow, recycling bins, reflection, refrigerator, remote control, river, river bank, road, rock, rolling pin, roof, room label, roots, rubber duckie, rug, ruin, salt shaker, sand satellite dish, scaffolding, scale, scanner, sconce, scrub brush, seagull, sealion, seashell, sewer, shadow, shampoo, sheep, shelf, shelves, shoe, shop window, shower, shower curtain, shower door, shower head, shutters, sideboard, sidewalk, sidewalk caf&#x000e9;, sign, silverware, sink, site office, sky, sky light, skyscraper, sled, sliding door, slippers, slope, smoke detector, snow, soap, soap dish, soap dispenser, soap on a rope, sofa, soil, space heater, spatulas, speaker, splash guard, sponge, spoon, spotlight, spray bottle, sprinkler, stained glass window, staircase, stake, stand, stapler, starfish, station, statue, step, stereo, stick, sticker, stool, stove, stove hood, stove top, street market, streetcar, streetlight, string, stump of tree, subway, suitcase, sun, supermarket, table, table runner, tablecloth, tank, tapestry, teapot, teddy bear, telephone, telephone booth, television, television stand, terrace, thermos, thermostat, tile, tire, tissue, toast, toaster, toilet, toilet brush, toilet brush stand, toilet paper, toilet paper holder, toll gate, tomatoes, toolbox, toothbrush, toothbrush holder, toothpaste, torch, towel, towel rack, tower, toy, tractor, traffic light, trash, trash can, trash compactor, tray, tray table, tree, trophy, truck, trumpet, trunk, tunnel, typewriter, umbrella, urinal, urn, utensils, utensils rack, vacuum, valley, van, vanity, vase, vault, VCR, vegetable, vent, verge, vineyard, volcano, wall, wall hanging, wall mount, wardrobe, warehouse, washing machine, watch, water, water bottle, water buffalo, water cooler, water fountain, waterfall, watering can, water cooler, water fountain, waterfall, watering can, watermelon, wave, weathervane, webcam, wheel, wheelbarrow, wheelchair, white out, whiteboard, wind chime, windex, windmill, window, window frame, window ledge, window seat, window shade, window sill, wine bottle, wine glass, wine rack, wire, wood, wood post, zebra.</p></app><app id="A3"><title>Appendix C: labels that were deleted (SIC)</title><p>Bottom of chair, box contants, chair leg, chair wheel, eye, hold back, junk, lamp part, misc., mouth, nothing, occluded, precipice, side, sping, supplies, table leg, terotauuko, viewpoint, xx, xxx.</p></app><app id="A4"><title>Appendix D: auxiliary data set</title><list list-type="alpha-lower"><list-item><p>Object density</p><p>Unique object density for the auxiliary set was highly correlated with the main data set (<italic>r</italic> = 0.82). While natural landscapes in the auxiliary set had fewer objects when compared to urban scenes [<italic>t</italic><sub>(6)</sub> = 3.46, <italic>p</italic> &#x0003c; 0.05], there was no reliable difference in object density in this database between indoor and outdoor environments [<italic>t</italic><sub>(14)</sub> &#x0003c; 1], as urban scene images in this database were more complex than urban images in the main set.</p></list-item><list-item><p>Classification using ensemble statistics</p><p>For the auxiliary set, images could be classified at the superordinate level with 85% accuracy (AUC = 0.86). Likewise, basic-level categorization could be done on this data set with 33% accuracy (AUC = 0.61). Although the categorization performance is significantly above the chance level of 6.3%, it is lower than the 61% achieved by the main database (<italic>Z</italic> = 13.8, <italic>p</italic> &#x0003c; 0.0001). To what extent is this lower performance due to the smaller size of the database? I sampled 5000 sets of 1220 images from the main database and tested classification performance on these sets, and found a median basic-level classification accuracy of 51% (95% confidence interval: 48&#x02013;54%). Thus, although database size explains some of the performance difference, the higher classification performance on the main dataset may reflect homogeneity in the images that might not be reflected in the real world. This does not diminish from the main point of this analysis: that simple ensemble statistics of objects in scenes provides above-chance information about scene categories.</p></list-item><list-item><p>Ten most frequent objects in auxiliary set</p><p>(In descending order of frequency). Wall, sky, floor, window, tree, road, building, lamp, door, table.</p></list-item><list-item><p>Overlap between frequent objects in main database vs. auxiliary database</p><p>How did the object frequencies measured in the main database compare to the object frequencies in the auxiliary set? For each basic-level scene category, I computed the overlap between the 10 most frequent objects in both databases. Overall, there was 69% overlap, ranging from 50% (<italic>conference room</italic>) to 90% (<italic>bathroom</italic>).</p></list-item><list-item><p>Object diagnosticity</p><p>To compare object diagnosticity between the main and auxiliary datasets, I computed the overlap of the 10 most diagnostic objects in both sets. 34% of the 10 most diagnostic objects in the main set were in the 10 most diagnostic objects in the auxiliary set.</p><p>The 10 most diagnostic objects in each basic-level category: (In descending order of diagnosticity). <italic>Bathroom</italic>: sink, shower, shower door, soap, toilet lid, toilet paper, towel rack, toilet, towel, bath. <italic>Bedroom</italic>: bed, nightstand, dresser, pillow, painting, telephone, closet, fan, clock, picture. <italic>Conference room</italic>: board, object, laptop, bottle, ground, bench, bicycle, speaker, chair, table. <italic>Corridor</italic>: entryway, light, light switch, bench, door handle, door, poster, ceiling, trash can, umbrella. <italic>Dining room</italic>: plate, dish, flower, vase, fireplace, tray, radiator, armchair, knife, bowl. <italic>Kitchen</italic>: cutlery, dish towel, stove hood, oven, coffee maker, stove, dishwasher, refrigerator, microwave, blender. <italic>Living room</italic>: sofa, fireplace, magazine, television stand, television, pillow, carpet, armchair, furniture, vase. <italic>Office</italic>: mousepad, keyboard, computer monitor, computer, desk, mug, mouse, pen, speaker, can. <italic>Tallbuilding</italic>: skyscraper, river, bus, building, statue, roof, fire hydrant, streetlight, pole, chimney. <italic>City</italic>: umbrella, house, balcony, parking meter, street, motorcycle, traffic light, wheel, van, awning. <italic>Street</italic>: hedge, wire, manhole, headlight, shutters, windshield, tail light, truck, van, license plate. <italic>Highway</italic>: bridge, median, tail light, fence, grass, road, cloud, truck, car, skyscraper. <italic>Coast</italic>: beach, ocean, sand, boat, cliff, rock, water, sky, cloud, mountain. <italic>Open country</italic>: field, hill, grass, cloud, bush, bison, house, sky, mountain, snow. <italic>Mountain</italic>: bison, snow, mountain, forest, hill, cloud, water, house, rock, sky. <italic>Forest</italic>: path, tree trunk, forest, foliage, river, snow, ground, tree, rock, grass.</p><p>The smaller degree of overlap for diagnosticity, compared to frequency (section Object Frequency) or mutual information (section Mutual Information) is likely due to the fact that the auxiliary set is much smaller, so there are fewer objects with at least 10 instances in this dataset.</p></list-item><list-item><p>Scene-object specificity</p><p>Forty six percent of objects were found in only one category, and of these, 43% had more than one instance in the database. A total of 19 objects (3.6%) were found in nine or more scene categories. Comparing these 19 objects to the 19 objects from the main database found in nine or more categories, 13 objects were found in both. These are: &#x0201c;box,&#x0201d; &#x0201c;chair,&#x0201d; &#x0201c;clock,&#x0201d; &#x0201c;door,&#x0201d; &#x0201c;lamp,&#x0201d; &#x0201c;light,&#x0201d; &#x0201c;person,&#x0201d; &#x0201c;plant,&#x0201d; &#x0201c;poster,&#x0201d; &#x0201c;table,&#x0201d; &#x0201c;tree,&#x0201d; &#x0201c;wall,&#x0201d; &#x0201c;window.&#x0201d;</p></list-item><list-item><p>Number of unique combinations</p><p>In the 1220 image auxiliary set, 1108 images had unique object combinations (9% redundancy).</p></list-item><list-item><p>Entropy</p><p>The entropy of the auxiliary set was 6.15 bits per object.</p></list-item><list-item><p>Mutual information</p><p>Fifty four percent of the 10 most informative objects for each scene category were shared between the main database and the auxiliary set, and six of the 10 objects with the highest overall mutual information for all scene categories were shared between the two data sets.</p></list-item><list-item><p>Classification using bag of words model</p><p>Ninety seven percent accuracy (AUC = 0.97) at superordinate-level categorization (not significantly different from main set performance: <italic>Z</italic> = 1.2, <italic>p</italic> = 0.23) and 80.2% basic-level categorization accuracy (AUC=0.88). This was significantly lower performance compared to the main dataset (<italic>Z</italic> = 14, <italic>p</italic> &#x0003c; 0.001). As the two data sets contained different objects, it is not possible to train on one and test on the other as in the previous section.</p></list-item><list-item><p>Classification using structural model</p><p>The auxiliary set achieved 77.5% correct at basic-level categorization (significantly lower than main dataset: <italic>Z</italic> = 13, <italic>p</italic> &#x0003c; 0.001) and 97% correct at superordinate-level categorization (similar to performance on main data set: <italic>Z</italic> &#x0003c; 1).</p></list-item></list><fig id="FA1" position="anchor"><label>Figure A1</label><caption><p><bold>Example images from the auxiliary database</bold>.</p></caption><graphic xlink:href="fpsyg-04-00777-a0001"/></fig></app></app-group></back></article>